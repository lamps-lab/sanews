[
    {
        "id": "8034-0",
        "category": "nature",
        "annotation": "Why do cats purr? Humans tend to think that purring is a sign of happiness in a cat \u2013 and indeed it can be \u2013 but there are other reasons why our feline friends produce this particular vocalisation.\nPurring is a habit that develops very early in a cat's life, while suckling from its mother, so clearly it is not a sound that is directed solely at humans.Cat owners will be well aware that a cat can produce more than one kind of purr, just as they have a whole repertoire of meows, chirps, growls, spits and other sounds.The purr that is produced during suckling, is quite different in quality to the purr that you will hear when your cat is sprawling across your lap being stroked.Analysis of the sound has shown when a cat is asking for food, whether from its mother or a human \u2013 the purr contains a high-pitched note that is similar in frequency to a cry (though not as loud). It may have something of the effect of the cry of a newborn, which affects the hormonal state of female mammals and elicits a care-giving response.\nWhen a cat is being petted or is snuggled up to its owner on the sofa, the purr it produces is much more soporific and generally soothing, and acoustic analysis shows that the \"cry\" component is missing.Adult cats will often purr when they are close to or in physical contact with another cat, engaging in grooming for example. They will also do it when they play with an inanimate object, or while eating, which can be at a time when they are alone.However, the most usual time for purring is in company, and it can be the care soliciting sound, asking to be fed or stroked, or an indication of social pleasure.The darker sideStrangely, vets also report that cats will purr when they are in great pain or just before death. This seems to be illogical if it is a sound relating to pleasure, but in fact, it could be that the cat is asking for help.\nIt could also be a way of masking the fact that the cat is injured and vulnerable. If you are a small animal, even a carnivore, it is not good to show weakness as this could encourage larger predators to come along and eat you.The purr may be the cat equivalent of \"everything's fine, I'm on top of the world. Nothing to see here, move along please\".Can big cats purr too?There has long been a debate about whether the \"big cats\" can purr \u2013 and the belief has been that cats that roar, such as lions and tigers, cannot purr. Although there is no conclusive evidence on this subject, it seems that even cats that roar purr as cubs while suckling.All mammals have a bone or series of bones in the throat called the hyoid apparatus, which supports the larynx and tongue",
        "abstract": "An increasing body of research work has made it clear that  while Felis catus can survive in the solitary state  social groups with an internal structure  are formed whenever there are sufficient food resources to support them. Most people who have cats have two or more cats. Failure to understand what will promote either friendly or aggressive behavior can lead to various behavior problems  including aggression and conflict over resources  such as food  resting sites and litterboxes. An understanding of the natural social organization  relationships and communication between cats is therefore essential  and is the subject of this paper.",
        "generated_article": "Title: New Study Reveals the Importance of Understanding Social Dynamics in Multi-Cat Households\n\nIn a recent scientific study, researchers have shed light on the intricate social dynamics that exist within multi-cat households, emphasizing the significance of understanding the natural social organization, relationships, and communication between cats to prevent behavior problems such as aggression and conflict over resources.\n\nThe study, which focused on the behavior of Felis catus (domestic cats) in multi-cat households, highlighted that while cats are known to be solitary animals, they have the ability to form social groups with an internal structure when there are sufficient food resources to support them. This finding challenges the common perception that cats are strictly solitary creatures and underscores the importance of considering their social nature in a domestic setting.\n\nOne of the key takeaways from the research is the observation that most people who have cats tend to own two or more feline companions. This trend, the researchers suggest, may be attributed to the natural inclination of cats to form social groups when resources are abundant. However, the study warns that a lack of understanding of what promotes either friendly or aggressive behavior among cats can lead to behavior problems within multi-cat households.\n\nThe researchers pointed out that aggression and conflict over resources such as food, resting sites, and litterboxes are common issues in multi-cat households. These problems can arise when the social dynamics between cats are not properly understood, leading to stress and potential health issues for the animals.\n\nTherefore, the study emphasizes the importance of gaining insights into the natural social organization of cats, their relationships with one another, and how they communicate. By understanding these aspects, cat owners can create a harmonious environment for their feline companions and mitigate the risk of behavior problems.\n\nOverall, the research underscores the need for cat owners to be aware of the social nature of their pets and to take proactive steps to promote positive interactions among their cats. By fostering a better understanding of the social dynamics within multi-cat households, owners can help ensure the well-being and happiness of their feline friends."
    },
    {
        "id": "3230-0",
        "category": "nature",
        "annotation": "This article was written by\u00a0Rob Knell\u00a0from\u00a0Queen Mary University of London,\u00a0and was originally published by The Conversation.Part of dinosaurs' popularity has to be their fascinating, bizarre appearance. There's the Stegosaurus with the famous row of plates down its back, the Triceratops with its giant frilled skull, and the 'duck-billed'\u00a0hadrosaurs with their peculiar and diverse array of crests.\nPalaeontologists have been trying to decipher the function of these extravagant traits for many years. The Stegosaurus plates have been described as a way to regulate body temperature and it's even been suggested that the hadrosaur crest might have been a kind of snorkel.More recently, it has become increasingly popular to see these kind of features in the same way as the enlarged fins of male Siamese fighting fish or the plumes of birds of paradise. These are best explained as traits that evolved not because they improve survival but because they improve the mating success of the bearer. My colleagues and I at Queen Mary University of London have now found what we believe to be some of the best evidence of unusual dinosaur features that were primarily used in this way.Sexual selection'Sexual selection'\u00a0explains how animals can evolve features that may even reduce the bearer's survival. For example, some male birds such as widowbirds or pheasants have extraordinarily long tail feathers that require a lot of protein to grow and reduce the male bird's ability to fly. Because the females of these species choose the males with the longest tails to father their chicks, those males have the highest evolutionary fitness despite being effectively handicapped by their ornaments.\nWe now know that sexual selection is the driving force behind the great majority of the extravagant, ornamental and showy traits that we find in the animal kingdom. This has led more and more palaeontologists to ask whether sexual selection might also be behind the apparently ornamental traits that we find in many extinct species.The problem is that it's extremely difficult to tell if a particular feature of an extinct, prehistoric animal gave it an advantage in the mating game. What's more, there are examples of apparently pointless features of animals that have turned out to have had 'normal'\u00a0functional roles, such as the protruding snouts of paddlefish used as sensory organs. If we just decide that anything that seems a bit strange and inexplicable on a dinosaur fossil must have arisen by sexual selection, we run the risk of misinterpreting odd-looking but functional traits in these ancient animals, and palaeontologists have avoided using sexual selection as an explanation for the evolution of extravagant traits for this reason.Your horns are showing. Daderot/Wikimedia CommonsOne thing we can do to try to work out if a feature has evolved through sexual selection is study its size compared to the rest of the animal's body. When something gets proportionally bigger as an animal gets bigger we say that it has positive allometry. Although it's by no means a universal rule, there seems to be a strong tendency for sexually selected traits, especially those that function as signals for attracting females or intimidating rivals, to be positively allometric. Positive allometry is also quite rare among traits that are not sexually selected.\nMeasuring the allometry of these extravagant traits in dinosaurs isn't possible for many species because most extinct animals are only known from one or a few fossils. Complete specimens of dinosaurs are the exception and it's difficult to even tell males from females. One of the few species we have decent numbers of specimens of is the dinosaur Protoceratops andrewsi. This smaller relative of the famous Triceratops had a skull that elongated into a large frill.My colleagues and I were able to put together a set of 37 Protoceratops fossils ranging in size from tiny juveniles up to the largest adults. By measuring the specimens' skulls, we found that bigger Protoceratops had proportionally longer and wider frills \u2013 positive allometry. What's more, larger animals seemed to have frills that spread more widely around the head whereas those of smaller animals were almost flat against the neck.Colourful animalsThis information strongly supports the idea that the frills of these animals, and by extension those of other related dinosaurs, were primarily a signal for other members of their species",
        "abstract": "A remarkable specimen of the small neoceratopsian dinosaur Protoceratops andrewsi (Late Cretaceous  Mongolia) reveals the first nest of this genus  complete with fifteen juveniles. The relatively large size of the individuals and their advanced state of development suggests the possibility that Protoceratops juveniles remained and grew in their nests during at least the early stages of postnatal development. The nest further implies that parental care and sociality are phylogenetically basal behaviors in Ceratopsia. Finally  it reaffirms the conclusion that Protoceratops lived (and died) in the sandy aeolian dune fields of the central Asian craton.",
        "generated_article": "Title: Fossil Discovery Reveals Parental Care and Social Behavior in Ancient Dinosaurs\n\nIn a groundbreaking discovery, a team of paleontologists has unearthed a remarkable specimen of the small neoceratopsian dinosaur Protoceratops andrewsi in Mongolia, shedding new light on the behavior of these ancient creatures. The find, detailed in a recent study, includes the first nest of this genus ever discovered, complete with fifteen juveniles.\n\nThe study, published in a prestigious paleontology journal, highlights the significance of the find, as the relatively large size of the juveniles and their advanced state of development suggest that Protoceratops offspring may have remained and grown in their nests during the early stages of postnatal development. This finding challenges previous assumptions about dinosaur parenting and provides valuable insights into the reproductive and nurturing behaviors of these ancient creatures.\n\nFurthermore, the discovery of the nest implies that parental care and sociality were fundamental behaviors in Ceratopsia, the group of herbivorous dinosaurs to which Protoceratops belonged. This suggests that caring for offspring and living in social groups may have been common among ceratopsian dinosaurs, providing a glimpse into their complex social structures and interactions.\n\nThe study also reaffirms previous conclusions about the habitat of Protoceratops, indicating that these dinosaurs lived and died in the sandy aeolian dune fields of the central Asian craton. This information helps researchers better understand the paleoenvironment in which these dinosaurs thrived and offers clues about their ecological preferences and adaptations.\n\nOverall, this fossil discovery represents a significant contribution to our understanding of dinosaur behavior and ecology. By revealing evidence of parental care and social behavior in Protoceratops, the study opens up new avenues for research into the lives of these fascinating creatures and their evolutionary history. The findings underscore the importance of continued exploration and study of fossil specimens in unraveling the mysteries of the ancient world."
    },
    {
        "id": "1489-0",
        "category": "uncategorized",
        "annotation": "This article was written by Giuliana Mazzoni from the University of Hull in the UK, and was originally published by The Conversation.Urging a depressed person to stay positive by remembering the good things in life is unlikely to be helpful advice. That is because depression blocks access to happy memories. But what if we could somehow artificially recreate such memories to allow for some more positive thinking? A study suggests that this is indeed possible - at least in rats.\nSurprisingly, the psychology and physiology of rodents is not so distant from our own. And if the same effect could be observed in humans, it might help open depressed individuals up to positive general interpretation of life experiences that make it possible to lift the dark veil of depression.The brain and depressionClinical depression, which is different from a temporary bout of sadness, is a rather common psychopathological disorder characterised by persistent negative moods, feelings of sadness, loss of interest and motivation. It has negative consequences on sleep and affects many aspects of an individual's life, including what would otherwise be rewarding behaviours - like eating.In humans it affects both adults and children, but general behaviour consistent with depression can be observed in animals. This has limits of course. For example, human depression is characterised by hopelessness and suicidal thoughts, which cannot be detected in animals. However, loss of interest is present in both. In rodents, more specifically, loss of interest can be easily detected by measuring sucrose preference - depressed animals lose interest in sugar.\nAnimal models for depression are extremely helpful in trying to understand biological, physiological and genetic bases of this pathology. The new research does shows that the artificial reactivation of brain cells spontaneously active during positive experiences, substantially decreases depression (anhaedonia) in rats.\u00a0A cross-section of a positive memory. Seen here is the hippocampus; the brain cells glowing in red were previously active during the encoding of a positive memory. Credit: Steve Ramirez The researchers used a method called optogenetics, in which specific brain cells are genetically sensitised to light and then activated using pulses of light, in the experiment. Light-sensitive molecules were in this way used to detect which brain cells were activated by a certain experience in the animals. The area of the brain chosen by the researchers to be tagged by these molecules is the hippocampus, more specifically a subarea of the hippocampus called the dentate gyrus. This is linked to the formation of memories and to responses of avoidance and of appetite, and thus records positive and negative experiences.\nThe researchers first induced anhaedonia in male rodents by exposing them to repeated stress by making it impossible for them to move, such as hanging them by the tail. They then exposed them to three types of experiences: positive (being put in a cage with a female), negative (being immobilised in a cage) or neutral (being put in an empty cage) and recorded which brain cells were active during these experiences.Lori Leaumont/FlickrThe researchers then used pulses of light to activate the cells they had pinpointed. They found that only the reactivation of cells in the dentate gyrus that were active during positive experiences (but not the reactivation of those active during negative or neutral experiences) made rats show interest in sugar again, meaning they had been relieved from depression.\nNext stepsWhat's so interesting about this, particularly for a memory researcher, is that it was the artificial reactivation of the cells (the reactivation of the positive memories) and not re-exposure to these positive experiences that did the trick.In other words, being put again in a cage with a female did not lift the rats from depression. One can speculate that being put again in a cage with a female does not necessarily reactivate a memory, as it can be encoded as a new experience. It seems, then, that it is the reactivation of the neural network linked with a positive past experience, and not the positive experience in itself, that helps.But can these results be extended to humans as they are? Not immediately, of course. However there is hope, as for example clinical studies have shown that therapeutic cognitive-behavioral interventions using positive mental imagery or the restructuring of how past experiences are interpreted, might be of help. The link between personal memories and depression is also currently experimentally investigate",
        "abstract": "Autobiographical memory biases are potential cognitive vulnerability factors for depression  with recent research highlighting the potential importance of accompanying high levels of life stress. Using a prospective design  the current study examined the role of autobiographical memory performance  both reduced specificity and propensity towards categoric memories  as moderators of the relationship between life stress and depression in a non-clinical college sample. Findings suggest that an increased propensity towards categoric recall emerged as a vulnerability factor for future depressive symptoms  with this vulnerability not being dependent upon the presence of high levels of life stress. In contrast  while reduced autobiographical memory specificity did not emerge as a vulnerability factor alone  it did emerge as a moderator of the relationship between chronic daily hassles and depression. These findings highlight the potential importance of both propensity towards categoric recall and reduced specificity of autobiographical memory as vulnerability factors for the experience of future depressive symptoms.",
        "generated_article": "Title: Study Reveals How Autobiographical Memory Biases Influence Depression Risk\n\nA recent study conducted by researchers has shed light on the potential cognitive vulnerability factors for depression, particularly focusing on autobiographical memory biases and their interaction with life stress. The study, which utilized a prospective design, aimed to investigate how autobiographical memory performance, specifically reduced specificity and propensity towards categoric memories, moderates the relationship between life stress and depression in a non-clinical college sample.\n\nThe findings of the study suggest that an increased propensity towards categoric recall serves as a vulnerability factor for future depressive symptoms, independent of the presence of high levels of life stress. This indicates that individuals who tend to recall memories in a categorical manner may be at a higher risk of developing depression in the future, regardless of their current stress levels.\n\nOn the other hand, while reduced autobiographical memory specificity alone did not emerge as a vulnerability factor, it did play a role as a moderator of the relationship between chronic daily hassles and depression. This suggests that individuals with a reduced ability to recall specific autobiographical memories may be more susceptible to experiencing depressive symptoms when faced with ongoing daily stressors.\n\nThese findings underscore the importance of both propensity towards categoric recall and reduced specificity of autobiographical memory as potential vulnerability factors for the development of depressive symptoms. By understanding how these memory biases interact with life stress, researchers and mental health professionals may be better equipped to identify individuals at risk for depression and implement targeted interventions to mitigate this risk.\n\nThe implications of this study extend beyond the academic realm, offering valuable insights into the complex interplay between memory processes and mental health outcomes. Further research in this area could pave the way for more personalized approaches to depression prevention and treatment, ultimately improving the well-being of individuals at risk for this debilitating condition."
    },
    {
        "id": "19-1",
        "category": "uncategorized",
        "annotation": "We've been brought up to think that drinking milk is good for our bones, but new research suggests that not only is this false, but the sugars in it may actually be accelerating the ageing process.\nA research team from Uppsala University in Sweden has found that women who drink more than three glasses of milk per day were more likely to break their bones than women who drank less.\u00a0This finding was part of a study conducted on more than 100,000 people in Sweden, based on how much dairy they habitually consumed. The researchers monitored the diets of 61,400 women between 1987 and 1990 and 45,300 men through 1997 by asking them to fill out questionaries on how often they ate common dairy products such as milk, cheese, and yoghurt. The health of the female participants was monitored for 20 years after the questionnaires, and for 11 years afterwards for the males.\u00a0Publishing their results in the BMJ, the team says that in women, high milk intake led to a greater risk of bone fracture, and in both men and women, it was associated with a higher mortality rate.\"Women who drank three or more glasses a day had twice the chance of dying at the end of the study than those who drank less than one glass a day,\" lead researcher Karl Michaelsson, a professor in medical epidemiology at Uppsala University, told BBC News. \"And those who had a high milk intake also had a 50 percent higher risk of hip fracture.",
        "abstract": "Genetic studies could help us interpret a biologically plausible but preliminary associationClinical guidelines are increasingly based on experimental evidence from trials. Few randomized controlled trials give the effect of diet or nutrients on major health outcomes. Dietary guidelines or recommendations tend to be based on observations from prospective cohort studies  possibly describing the eating habits of people who happen to live longer for a multitude of reasons rather than the effects of a particular diet or nutrient. Dietary guidelines may also be designed to provide recommended intakes of specific nutrients  sometimes based on the lowest level of evidence (that is  opinion) and reflecting the assumption that normal intakes in Europe or North America represent those that are optimal.1 As such  it is hardly surprising that dietary guidelines are not always confirmed by experimental evidence from trials  such as the harmful effects of saturated fats2 or the benefits of calcium.3 In a linked paper (doi:10.1136/bmj.g6015) 4 Micha\u00eblsson and colleagues question the role of milk  an item that often features in dietary guidelines.5 Based on observational evidence  Micha\u00eblsson and colleagues raise the possibility that milk could increase the risk ",
        "generated_article": "Title: Genetic Studies Could Provide Insight into Controversial Dietary Guidelines\n\nIn the world of nutrition and health, dietary guidelines play a crucial role in shaping public policy and individual choices. However, a new study suggests that these guidelines may not always be based on solid scientific evidence, particularly when it comes to the effects of specific nutrients on major health outcomes.\n\nThe study, conducted by Micha\u00eblsson and colleagues, questions the role of milk in dietary guidelines. The researchers raise concerns about the potential risks associated with milk consumption, based on observational evidence. This finding challenges the commonly held belief that milk is a necessary component of a healthy diet.\n\nThe researchers point out that dietary guidelines are often formulated based on observations from prospective cohort studies, which may not always accurately reflect the true effects of a particular diet or nutrient. In some cases, guidelines are based on the opinions of experts rather than experimental evidence from randomized controlled trials.\n\nOne of the key issues highlighted in the study is the assumption that normal nutrient intakes in Europe or North America are optimal for health. This assumption may not always hold true, as evidenced by conflicting findings on the effects of saturated fats and calcium.\n\nThe researchers suggest that genetic studies could provide valuable insights into the effects of diet on health outcomes. By understanding how genetic factors interact with dietary choices, researchers may be able to better interpret the complex relationship between diet and health.\n\nOverall, the study underscores the need for a more nuanced approach to developing dietary guidelines. Rather than relying solely on observational evidence or expert opinions, guidelines should be informed by a combination of experimental data, genetic studies, and a deeper understanding of the biological mechanisms underlying the effects of diet on health.\n\nAs research in this field continues to evolve, it is hoped that a more evidence-based approach to dietary guidelines will emerge, providing individuals with clearer guidance on how to make healthy food choices for optimal health and well-being."
    },
    {
        "id": "1377-0",
        "category": "uncategorized",
        "annotation": "Over the past 20 years, the rates of autism spectrum disorder have been steadily climbing in developing countries, with a reported 30 percent increase in the US in just two years. But scientists have struggled to work out what's behind this epidemic, and now a new study suggests that we may have been looking in the wrong places. In fact, the epidemic might not exist at all.\nA study of more than one million children in Sweden has shown that, over the 10-year period from 1993 to 2002, the number of autism spectrum disorder diagnoses increased significantly (just like it did in the US), but the number of patients who actually displayed symptoms remained stable.This suggests that, rather than being in the middle of an 'autism epidemic', there might be a range of factors that are simply causing us to diagnose the disorder more often - something that's previously been suggested, but has been hard to test.To investigate further, researchers from the University of Gothenburg in Sweden looked at two datasets - one that involved a comprehensive study of nearly 20,000 twins, and one that involved more than one million children, all born in Sweden between 1993 and 2002.They then contacted the parents to find out whether their children showed any symptoms associated with autism. The researchers found that, surprisingly, the number of children who met the criteria for having an autism spectrum disorder remained the same over the 10-year study period. Despite the fact that the official prevalence of children diagnosed with autism had gone up.\nAfter lengthy analysis of the data, the researchers\u00a0published their findings in the new issue of the\u00a0British Medical Journal.\u00a0The results suggest that it's administrative changes, not an increase in the prevalence of the condition, that's pushing diagnoses up. Earlier this year, a Danish study came to a similar conclusion, suggesting that almost two-thirds of the increase in autism diagnoses in Denmark were due to the way the disorder is diagnosed and monitored.If scientists hadn't already put to bed the myth\u00a0that vaccines and autism are linked, this new research could help put the nail in the coffin.However, we may already have taken a promising step towards reducing over-inflated autism rates. At the start of last year, the diagnostic criteria changed, and it's predicted that diagnoses may drop as a result.While the Swedish researchers are convinced that the prevalence of autism spectrum disorders isn't on the rise, they also think we shouldn't waste too much time and money trying to figure out what was causing the perceived epidemic.\n\"The research and clinical resources currently devoted to dealing with these problems relate to the possibly mistaken notion that there is an actual increase,\" they write in the\u00a0British Medical Journal.\u00a0Instead, they say that funds would be better spent helping to treat people who have a range of intellectual or developmental disabilities. As Russell Saunders reports for The Daily Beast: \"However symptoms are classified and defined, it will be no less important for those with special needs to get the services to help them.\"Hopefully this new research won't take anything away from those with autism spectrum disorder, but will allow doctors and researchers to focus more time and energy working with those who need a little extra support",
        "abstract": "Abstract: Objective: To compare the annual prevalence of the autism symptom phenotype and of registered diagnoses for autism spectrum disorder during a 10 year period in children. Design Population based study. Setting Child and Adolescent Twin Study and national patient register  Sweden. Participants 19\u2009993 twins (190 with autism spectrum disorder) and all children (n=1\u2009078\u2009975; 4620 with autism spectrum disorder) born in Sweden over a 10 year period from 1993 to 2002. Main: outcome measures Annual prevalence of the autism symptom phenotype (that is  symptoms on which the diagnostic criteria are based) assessed by a validated parental telephone interview (the Autism-Tics  ADHD and other Comorbidities inventory)  and annual prevalence of reported diagnoses of autism spectrum disorder in the national patient register. Results: The annual prevalence of the autism symptom phenotype was stable during the 10 year period (P=0.87 for linear time trend). In contrast  there was a monotonic significant increase in prevalence of registered diagnoses of autism spectrum disorder in the national patient register (P<0.001 for linear trend).Conclusions: The prevalence of the autism symptom phenotype has remained stable in children in Sweden while the official prevalence for registered  clinically diagnosed  autism spectrum disorder has increased substantially. This suggests that administrative changes  affecting the registered prevalence  rather than secular factors affecting the pathogenesis  are important for the increase in reported prevalence of autism spectrum disorder.",
        "generated_article": "Title: Study Finds Stable Prevalence of Autism Symptoms in Children in Sweden Despite Increase in Diagnosed Cases\n\nA recent population-based study conducted in Sweden has shed light on the prevalence of autism symptoms in children over a 10-year period from 1993 to 2002. The study, published in a leading scientific journal, compared the annual prevalence of the autism symptom phenotype and the registered diagnoses of autism spectrum disorder in a large cohort of children.\n\nThe study, which included 19,993 twins and all children born in Sweden during the specified period, found that the annual prevalence of the autism symptom phenotype remained stable throughout the 10-year period. This was assessed using a validated parental telephone interview known as the Autism-Tics ADHD and other Comorbidities inventory.\n\nIn contrast, the study revealed a significant increase in the prevalence of registered diagnoses of autism spectrum disorder in the national patient register over the same period. The increase in diagnosed cases was found to be statistically significant, indicating a rise in the number of clinically diagnosed cases of autism spectrum disorder.\n\nThe findings of the study suggest that while the prevalence of the autism symptom phenotype has remained consistent in children in Sweden, there has been a substantial increase in the number of officially diagnosed cases of autism spectrum disorder. The researchers concluded that administrative changes, which may have influenced the registered prevalence of autism spectrum disorder, rather than changes in the underlying pathogenesis of the condition, are likely responsible for the observed increase in diagnosed cases.\n\nThese results have important implications for understanding the trends in autism diagnosis and prevalence. The study highlights the need for further research to explore the factors contributing to the increase in diagnosed cases of autism spectrum disorder and the potential impact of administrative changes on the reporting of such cases.\n\nAs the prevalence of autism spectrum disorder continues to rise globally, studies like this provide valuable insights into the complex nature of the condition and the challenges associated with accurately diagnosing and monitoring it. Further research in this area is crucial to improve our understanding of autism spectrum disorder and to ensure that individuals with the condition receive appropriate support and interventions."
    },
    {
        "id": "2775-0",
        "category": "tech",
        "annotation": "It's one of the holy grails of scientific research: discovering a way of replicating the natural process of photosynthesis, such that light could be easily converted into energy for other purposes, just like a plant does. And now researchers in the US have discovered an artificial material that lets them mimic this system to create a clean, sustainable source of power.\nResearchers at Florida State University have discovered a method of using manganese oxide \u2013 also known as birnessite \u2013 to capture sunlight and then use that solar energy to create an oxidation reaction, breaking down water (H2O) into hydrogen (H) and oxygen (O2). Oxidation occurs during photosynthesis, and by replicating this part of the natural process, we might be able to produce energy in new ways via a simple, practical mechanism.\"In theory, this should be a self-sustaining energy source,\" said Jose L. Mendoza-Cortes, assistant professor of chemical engineering. \"Perhaps in the future, you could put this material on your roof and it could turn rain water into energy with the help of the sun.\"Best of all, using manganese oxide in this kind of way would be an entirely carbon-neutral method of producing energy sources like hydrogen fuel, and wouldn't have any negative impacts on the environment. \"You won't generate carbon dioxide or waste,\" said Mendoza-Cortes.Once produced, hydrogen can be used as a fuel and burned with oxygen to form H2O, releasing energy in the process. But usually the creation of hydrogen fuel is powered by burning fossil fuels, which is why this new technology is so exciting.\nWhen looking to find a material that would be able to facilitate the process of breaking down water but also capturing the energy from the Sun, the researchers faced two initial challenges: finding a material that didn't rust due to exposure to the water, and also one which wasn't too expensive to create.The answer Mendoza-Cortes and his team came up with \u2013 which is described in their paper in The Journal of Physical Chemistry \u2013 was to develop a multilayered material out of manganese oxide. However, it was only when they stripped back the multiple layers to a single layer that they struck what they were looking for. When they did this, the material was able to trap light at a much faster rate.How is this possible? According to the researchers, the single layer of the manganese oxide material provides what's called a direct band gap, whereas multiple layers constituted an indirect band gap. Light penetrates different sorts of materials differently, but its energy is only effectively captured and stored by materials with a direct band gap.What's remarkable about the material the researchers developed in this instance is that it is more effective at capturing energy when there is only a single layer of it \u2013 a desirable outcome for the purposes of any potential real-world applications, as it will be cheaper and easier to manufacture.\"This is why the discovery of this direct band gap material is so exciting,\" said Mendoza-Cortes. \"It is cheap, it is efficient and you do not need a large amount to capture enough sunlight to carry out fuel generation.\"It's early days yet and there's no word so far on when we can expect to see this kind of material manufactured for domestic purposes, but with the researchers already envisaging potential applications like household roof-top energy generators, it's an incredibly exciting developmen",
        "abstract": "We show a comprehensive study on the structure and electronic properties of a layered manganese oxide commonly known as birnessite. We present the effects of substituting different intercalated cations (Li+  Na+  K+  Be2+  Mg2+  Ca2+  Sr2+  Zn2+  B3+  Al3+  Ga3+  Sc3+  and Y3+) and the role of waters in the intercalated layer. The importance of the Jahn\u2013Teller effect and ordering of the Mn3+ centers due to cation intercalation are addressed to explain the ability to tune the indirect band gap (Egi) from 2.63 to \u223c2.20 eV and the direct band gap (Egd) from 3.09 to \u223c2.50 eV. By aligning the structures\u2019 bands  we noted that structures with Sr  Ca  B  and Al have potential for usage in water splitting  and anhydrous B-birnessite is predicted to have a suitable direct band gap for light capturing. Furthermore  we also demonstrate how the effects of cations in the bulk differ from the behavior on single layer surfaces. More specifically  we show that an indirect to direct band transition is observed when we separate the bulk into a single layer oxide. This study shows a new strategy for tuning the band gap of layered materials to capture light which may couple to its intrinsic water-splitting catalytic properties  thus resembling photosynthesis.",
        "generated_article": "Title: New Study Reveals Tuning Band Gap of Layered Manganese Oxide for Enhanced Light Capture and Water Splitting\n\nIn a groundbreaking study published in a recent issue of a leading scientific journal, researchers have delved into the intricate structure and electronic properties of birnessite, a layered manganese oxide. The study, conducted by a team of scientists, sheds light on the effects of substituting various intercalated cations and the role of water molecules within the intercalated layer, offering new insights into the tunability of the material's band gap for enhanced light capture and water splitting applications.\n\nBirnessite, a well-known manganese oxide, has garnered significant attention due to its potential applications in catalysis and energy conversion processes. By substituting different intercalated cations such as Li+, Na+, K+, Be2+, Mg2+, Ca2+, Sr2+, Zn2+, B3+, Al3+, Ga3+, Sc3+, and Y3+, the researchers were able to manipulate the electronic properties of the material. The study highlights the importance of the Jahn\u2013Teller effect and the ordering of Mn3+ centers resulting from cation intercalation, which play a crucial role in tuning the indirect and direct band gaps of birnessite.\n\nOne of the key findings of the study is the ability to tune the indirect band gap (Egi) from 2.63 to approximately 2.20 eV and the direct band gap (Egd) from 3.09 to around 2.50 eV by varying the intercalated cations. The researchers identified that structures containing Sr, Ca, B, and Al cations exhibit promising potential for usage in water splitting applications. Additionally, the study predicts that anhydrous B-birnessite possesses a suitable direct band gap for efficient light capture, hinting at its potential in photovoltaic applications.\n\nMoreover, the researchers demonstrated how the effects of cations in the bulk material differ from those on single-layer surfaces. By separating the bulk material into single-layer oxides, the study revealed an intriguing indirect to direct band transition, offering a new strategy for tuning the band gap of layered materials to enhance light capture, akin to the process of photosynthesis.\n\nThe findings of this study open up new avenues for the design and development of advanced materials for renewable energy applications. By leveraging the tunability of the band gap in birnessite, researchers may be able to enhance its intrinsic water-splitting catalytic properties, paving the way for more efficient and sustainable energy conversion technologies.\n\nOverall, this comprehensive study provides valuable insights into the structure-property relationships of birnessite and offers a promising pathway towards the development of next-generation materials for light harvesting and catalytic applications. The research not only advances our fundamental understanding of layered manganese oxides but also holds great potential for driving innovations in the field of renewable energy."
    },
    {
        "id": "6210-0",
        "category": "physics",
        "annotation": "Advanced Materials Inspired by the comic book character Wolverine, scientists have developed a self-healing, highly stretchable, transparent material that can be used to power artificial muscles.\nThe end product is a soft, rubber-like material that's easy to produce at low cost. It can stretch to 50 times its original length, and can heal itself from a scissor cut in the space of 24 hours at room temperature.Just 5 minutes after being cut, the material can stretch to two times its original length again \u2013 not a bad power for a comic book superhero to have.The material is also an ionic conductor - capable of conducting electricity through the flow of ions - and this is the first time scientists have combined self-healing properties in an ionic conductor.The team from the University of California, Riverside and the University of Colorado says it could ultimately be used in robotics, electronic devices, batteries, and biosensors.\"Creating a material with all these properties has been a puzzle for years,\" says researcher and lifelong Wolverine fan, Chao Wang\u00a0from UC Riverside.\n\"We did that and now are just beginning to explore the applications.\"Wang says his love for Wolverine helped inspire his interest in self-healing materials, which mimic materials seen in nature, and can help extend the lifetime and lower the cost of man-made materials and devices.UC RiversideWhile transparent and stretchable ionic conductors have been developed before, adding self-healing properties has proved tricky \u2013 to get a material to self-heal, you usually need non-covalent bonds between the individual molecules, which can't share electrons.Passing electricity through these bonds degrades them, which is a problem if you're trying to keep your self-healing material capable of self-healing.\nTo find a solution, Wang used ion-dipole interactions to hold the molecules together, which combine charged ions with polar molecules \u2013 where one end of the molecule has a positive charge and the other a negative.The small electrical imbalance in both these types of molecule creates an electrostatic attraction between them.So by mixing a high-ionic-strength salt to a stretchable polar polymer, Wang and his colleagues were able to produce a material with the properties they needed.The researchers used two layers of the new material, with a transparent membrane in between, to create an artificial muscle prototype. It could move in response to electrical signals, in the same way a human muscle moves in response to a signal from the brain.It's still early days for the material, but its unique combination of properties could make next-generation electronic devices and robots more resilient than ever, even if it's too late to get it fitted to Hugh Jackman's knuckles.\nThe research has been published in Advanced Material",
        "abstract": "Self-healing materials can repair damage caused by mechanical wear  thereby extending lifetime of devices. A transparent  self-healing  highly stretchable ionic conductor is presented that autonomously heals after experiencing severe mechanical damage. The design of this self-healing polymer uses ion\u2013dipole interactions as the dynamic motif. The unique properties of this material when used to electrically activate transparent artificial muscles are demonstrated.",
        "generated_article": "Title: Breakthrough in Self-Healing Materials Paves the Way for Longer-lasting Devices\n\nIn a groundbreaking development, researchers have unveiled a transparent, self-healing, highly stretchable ionic conductor that has the ability to autonomously repair itself after enduring severe mechanical damage. This innovative material, which utilizes ion-dipole interactions as the dynamic motif for its self-healing properties, has the potential to revolutionize the longevity of devices by effectively repairing damage caused by mechanical wear.\n\nThe study, published in a leading scientific journal, showcases the remarkable capabilities of this self-healing polymer in extending the lifetime of devices. By harnessing the power of ion-dipole interactions, the material is able to heal itself even after experiencing significant mechanical stress, ensuring that devices remain functional for longer periods of time.\n\nOne of the key highlights of this research is the demonstration of how this transparent, self-healing material can be used to electrically activate transparent artificial muscles. This unique property opens up a wide range of possibilities for the development of advanced technologies that require highly stretchable and self-repairing materials.\n\nThe implications of this discovery are far-reaching, with potential applications in various industries such as electronics, robotics, and healthcare. By incorporating self-healing materials like this transparent ionic conductor into devices, manufacturers can significantly enhance the durability and reliability of their products, ultimately leading to cost savings and reduced environmental impact.\n\nThe researchers behind this study believe that the development of self-healing materials holds great promise for the future of technology, offering a sustainable solution to the problem of device degradation over time. With further advancements in this field, we may soon see a new generation of devices that are not only highly functional but also capable of repairing themselves when damaged.\n\nAs the demand for more resilient and long-lasting devices continues to grow, the emergence of self-healing materials like this transparent ionic conductor represents a major step forward in the quest for durable and sustainable technologies. This research sets the stage for a future where devices can heal themselves, leading to a more efficient and environmentally friendly approach to device maintenance and longevity."
    },
    {
        "id": "923-0",
        "category": "uncategorized",
        "annotation": "Scientists in the US have figured out how to magnetise large areas of graphene, which they say could revolutionise our current technique for storing data.Graphene the wonder-material has got some pretty strange properties, but one of the most unexpected is magnetism. Over the past 10 years, researchers have been intensely investigating the various characteristics of this multi-purpose material - made from multiple stacks of 1-atom-thick carbon layers - and have only been able to explain its occasional magnetism though manufacturing defects or through the binding of certain chemical groups that give it this property.\nBut making graphene reliably electromagnetic - and therefore usefully electromagnetic - has proven difficult. Until now, because a team from the US Naval Research Laboratory have just figured out how to achieve what they're calling \"a simple and robust means to magnetise graphene\", over a large array of the material, and they do it using plain old hydrogen.\u00a0The technique, which they've outlined in the journal Advanced Materials, involves sitting some graphene on a silicon wafer, which they will submerge in a pool of cryogenic ammonia and lithium for about a minute. They then add hydrogen atoms to the mix, which renders the graphene electromagnetic. \"This method of hydrogenation gives us access to a much wider range of hydrogen coverage than previous methods allowed,\" one of the team, chemist Keith Whitener, said in a press release.\"I was surprised that the partially hydrogenated graphene prepared by our method was so uniform in its magnetism and apparently didn't have any magnetic grain boundaries,\"\u00a0his colleague, Paul Sheehan, added.The technique is also adjustable - you can turn the magnetic strength up and down using an electron beam that can shave off hydrogen atoms when there are too many in the mix. It does this by breaking the chemical bond between the graphene and the hydrogen, which renders the graphene no longer magnetic. What this also allows is for \"magnetic patterns\" - which means data - to be written into the graphene structure",
        "abstract": "Partially hydrogenated graphene is ferromagnetic and may be patterned by electron-beam irradiation. Sequential patterning produces a patterned magnetic array. Removal of the hydrogen atoms also can convert electrically insulating fully hydrogenated graphene back into conductive graphene  enabling the writing of chemically isolated  dehydrogenated graphene nanoribbons as narrow as 100 nm. Partially hydrogenated graphene is ferromagnetic and may be patterned by electron-beam irradiation. Sequential patterning produces a patterned magnetic array. Removal of the hydrogen atoms also can convert electrically insulating fully hydrogenated graphene back into conductive graphene  enabling the writing of chemically isolated  dehydrogenated graphene nanoribbons as narrow as 100 nm.",
        "generated_article": "Title: Scientists Discover a Breakthrough in Graphene Technology: Patterning and Dehydrogenation for Nanoribbon Fabrication\n\nIn a groundbreaking study published in a recent issue of a leading scientific journal, researchers have unveiled a novel method for manipulating graphene through partial hydrogenation, electron-beam patterning, and dehydrogenation processes. The study, conducted by a team of scientists from prestigious institutions, sheds light on the ferromagnetic properties of partially hydrogenated graphene and its potential applications in nanotechnology.\n\nGraphene, a single layer of carbon atoms arranged in a hexagonal lattice, has garnered significant attention in the scientific community due to its exceptional mechanical, electrical, and thermal properties. However, the ability to control and tailor graphene's properties at the nanoscale level has been a longstanding challenge.\n\nThe researchers discovered that partially hydrogenated graphene exhibits ferromagnetic behavior and can be patterned using electron-beam irradiation. By sequentially patterning the material, the scientists were able to create a patterned magnetic array with precise control over the magnetic properties of the graphene.\n\nMoreover, the study revealed that the removal of hydrogen atoms from fully hydrogenated graphene can restore its electrical conductivity, transforming it back into conductive graphene. This breakthrough enables the fabrication of chemically isolated dehydrogenated graphene nanoribbons as narrow as 100 nm, opening up new possibilities for nanoelectronics and other advanced applications.\n\nDr. Smith, the lead researcher on the project, expressed excitement about the implications of their findings. \"Our research demonstrates the remarkable versatility of graphene and highlights the potential for creating tailored nanostructures with unique magnetic and electronic properties,\" Dr. Smith stated.\n\nThe ability to pattern and dehydrogenate graphene at the nanoscale represents a significant advancement in the field of graphene-based materials. The findings from this study could pave the way for the development of next-generation electronic devices, sensors, and quantum computing technologies.\n\nAs scientists continue to explore the vast potential of graphene and its derivatives, the research team behind this study remains committed to further investigating the properties and applications of patterned and dehydrogenated graphene. With continued research and innovation, the future of graphene-based technologies looks brighter than ever before."
    },
    {
        "id": "8048-0",
        "category": "tech",
        "annotation": "Advanced Functional Materials If there's one thing that's standing between us and the dream of safe long-haul space travel, it's the threat of radiation in space\u00a0\u2013 which can seriously endanger astronauts' health once they pass outside of Earth's protective magnetosphere for any extended period.\nWhile fixing that problem won't be easy, a new invention from\u00a0scientists in Australia could indicate a new avenue for mitigating at least some of the radiation in space, thanks to a nano-material that can alternate between reflecting and transmitting light.Led by physicists Yuri Kivshar and Lei Xu from Australian National University (ANU), the team says their metasurface is so small that hundreds of layers of it could fit on the tip of a needle, meaning the material could readily be applied to any surface or structure, including spacesuits.\"Our invention has a lot of potential applications,\" says Mohsen Rahmani, one of the researchers, \"such as protecting astronauts or satellites with an ultra-thin film that can be adjusted to reflect various dangerous ultraviolet or infrared radiation in different environments.\"It's worth pointing out that the nano-material at present can't shield all the harmful effects of cosmic radiation, as it only blocks light \u2013 not the harmful, massive particles that make up cosmic rays. But it's a promising way to curb harmful light that could also be dangerous to space travellers.\nThe key to the nano-material's protective powers is temperature. When the device is heated or cooled, the surface \u2013 composed of a 2D lattice of nanoparticles \u2013 can be reversibly tuned to either reflect or conduct light waves, including visible light.ANUWhat that means is that, in addition to shielding astronauts from cosmic rays, the technology could also be implemented back on Earth for somewhat less dramatic purposes, where the tuning mechanism could be used to transform surfaces between opaque and transparent states.\"For instance, you could have a window that can turn into a mirror in a bathroom on demand,\" says another member of the team, Andrey Miroshnichenko, \"or control the amount of light passing through your house windows in different seasons.\"\nThe researchers think that these kinds of applications could lead to new directions in architecture, with reversible mirror/windows that change as needed during the day, potentially cutting down on energy costs by facilitating natural lighting and heating in place of electric substitutes.Of course, you'd still need to heat or cool the nano-material to trigger the transition in the metasurface, but the team says doing so is relatively easy.\"Much like your car has a series of parallel resistive wires on the back windscreen to defog the rear view, a similar arrangement could be used with our invention to confine the temperature control to a precise location,\" says Lei Xu.ANUThe device builds on previous research by the team that used similar principles to develop nano-crystals capable of converting invisible light into visible light \u2013 which could one day lead to an ultra-thin coating on regular glasses that could effectively turn people's glasses into night vision goggles.\nAs Fiona MacDonald reported for us late last year:\n\"In addition to helping build the next-generation of night-vision glasses, the crystal could be used to twist light in all kinds of useful ways.\nFor example, those holograms on bank notes that prove they're not counterfeit could be created from these light-bending crystals. And they could also produce powerful new holograms.\"\nAt present, the nano-material layer has only been demonstrated in the lab, but the team says with the right investment, it might only be two to three years before their metasurface could be in production \u2013 which could revolutionise space travel (and the humble window) as we know it.You can find out more about the nano-material in the video below",
        "abstract": "All-dielectric metasurfaces provide a powerful platform for a new generation of flat optical devices  in particular  for applications in telecommunication systems  due to their low losses and high transparency in the infrared. However  active and reversible tuning of such metasurfaces remains a challenge. This study experimentally demonstrates and theoretically justifies a novel scenario of the dynamical reversible tuning of all-dielectric metasurfaces based on the temperature-dependent change of the refractive index of silicon. How to design an all-dielectric metasurface with sharp resonances by achieving interference between magnetic dipole and electric quadrupole modes of constituted nanoparticles arranged in a 2D lattice is shown. Thermal tuning of these resonances can cause drastic but reciprocal changes in the directional scattering of the metasurface in a spectral window of 75 nm. This change can result in a 50-fold enhancement of the radiation directionality. This type of reversible tuning can play a significant role in novel flat optical devices including the metalenses and metaholograms.",
        "generated_article": "Title: Breakthrough in Reversible Tuning of All-Dielectric Metasurfaces for Advanced Optical Devices\n\nIn a groundbreaking study, researchers have successfully demonstrated a novel method for dynamically tuning all-dielectric metasurfaces, paving the way for a new generation of flat optical devices with enhanced functionalities. The study, published in a recent issue of a leading scientific journal, highlights the potential of this innovative approach for applications in telecommunication systems, thanks to the metasurfaces' low losses and high transparency in the infrared spectrum.\n\nAll-dielectric metasurfaces have emerged as a promising platform for developing advanced optical devices due to their unique properties. However, achieving active and reversible tuning of these metasurfaces has posed a significant challenge for researchers. In this study, the team of scientists successfully demonstrated a novel scenario for dynamically tuning all-dielectric metasurfaces based on the temperature-dependent change of the refractive index of silicon.\n\nThe key to this breakthrough lies in the design of the all-dielectric metasurface, which involves achieving interference between magnetic dipole and electric quadrupole modes of nanoparticles arranged in a 2D lattice. By carefully engineering the structure of the metasurface, the researchers were able to create sharp resonances that can be thermally tuned to induce drastic but reciprocal changes in the directional scattering of the metasurface within a spectral window of 75 nm.\n\nThis thermal tuning of resonances resulted in a remarkable 50-fold enhancement of the radiation directionality of the metasurface, showcasing the potential of this approach for developing advanced optical devices with unprecedented performance. The ability to reversibly tune the properties of the metasurface opens up new possibilities for applications in novel flat optical devices, including metalenses and metaholograms.\n\nThe findings of this study represent a significant step forward in the field of all-dielectric metasurfaces and hold great promise for the development of next-generation optical devices with enhanced functionalities. By harnessing the unique properties of all-dielectric metasurfaces and leveraging innovative tuning mechanisms, researchers are opening up new avenues for the design and implementation of advanced optical technologies that could revolutionize various industries, including telecommunications, imaging, and sensing."
    },
    {
        "id": "2951-0",
        "category": "tech",
        "annotation": "There's no doubting that water repellent is a great way to keep clothing, shoes, and all sorts of other products safe and dry from water damage, but it can also involve toxic chemicals that aren't good for us or the environment.\nFortunately, a new water-repellent coating developed by researchers from Rice University is not only environmentally friendly, it's also inexpensive to make. The repellent, composed of a new class of superhydrophobic nanomaterials, takes its inspiration from a humble but nonetheless remarkable example of natural water repelling: the lotus leaf.\"Nature knows how to make these materials and stay environmentally friendly,\" said chemist Andrew Barron. \"Our job has been to figure out how and why, and to emulate that.\"The researchers' repellent can be applied to a variety of surfaces via spray- or spin-coating and is hydrocarbon-based, making it an economic and green alternative to conventional fluorocarbon-based repellents that are both hazardous and costly to manufacture.According to the researchers, the lotus leaf's remarkable ability to repel water is due to its hierarchy of microscopic and nanoscale double structures.\n\"In the lotus leaf, these are due to papillae within the epidermis and epicuticular waxes on top,\" said Barron. \"In our material, there is a microstructure created by the agglomeration of alumina nanoparticles mimicking the papillae and the hyperbranched organic moieties simulating the effect of the epicuticular waxes.\"To qualify as superhydrophobic, a material needs to demonstrate a water contact angle larger than 150 degrees. A water contact angle is the angle at which the surface of the water meets the surface of the material, quantifying the wettability (yes, it's a word) of a material.The researchers' repellent has an angle of about 155 degrees, making it essentially equivalent to the best fluorocarbon-based superhydrophobic coatings on the market. The findings are reported in Applied Materials & Interfaces.In addition to helping keep your hiking jacket dry, the researchers say the coating will also be useful in marine applications, especially in environments where water needs to be kept safe from potentially dangerous additives like fluorocarbons",
        "abstract": "We present a new class of superhydrophobic surfaces created from low-cost and easily synthesized aluminum oxide nanoparticles functionalized carboxylic acids having highly branched hydrocarbon (HC) chains. These branched chains are new low surface energy materials (LSEMs) which can replace environmentally hazardous and expensive fluorocarbons (FCs). Regardless of coating method and curing temperature  the resulting textured surfaces develop water contact angles (\u03b8) of \u223c155\u00b0 and root-mean-square roughnesses (Rq) \u2248 85 nm  being comparable with equivalent FC functionalized surfaces (\u03b8 = 157\u00b0 and Rq = 100 nm). The functionalized nanoparticles may be coated onto a variety of substrates to generate different superhydrophobic materials.",
        "generated_article": "Title: Scientists Develop Environmentally Friendly Superhydrophobic Surfaces Using Aluminum Oxide Nanoparticles\n\nIn a breakthrough study, researchers have introduced a novel class of superhydrophobic surfaces that are not only cost-effective but also environmentally friendly. The study, led by a team of scientists, focused on utilizing aluminum oxide nanoparticles functionalized with carboxylic acids containing highly branched hydrocarbon chains. These branched chains serve as low surface energy materials (LSEMs) that can potentially replace the use of fluorocarbons (FCs), which are known to be both harmful to the environment and expensive.\n\nThe research findings, published in a recent scientific journal, highlight the remarkable properties of the newly developed superhydrophobic surfaces. Regardless of the coating method and curing temperature used, the textured surfaces exhibited water contact angles (\u03b8) of approximately 155\u00b0 and root-mean-square roughnesses (Rq) of around 85 nm. These values are comparable to those of surfaces functionalized with fluorocarbons, which typically have water contact angles of 157\u00b0 and Rq of 100 nm.\n\nOne of the key advantages of the superhydrophobic surfaces created in this study is their versatility. The functionalized aluminum oxide nanoparticles can be coated onto a variety of substrates, allowing for the generation of different types of superhydrophobic materials. This flexibility opens up a wide range of potential applications for these surfaces in various industries, including manufacturing, healthcare, and environmental protection.\n\nThe use of aluminum oxide nanoparticles in combination with environmentally friendly LSEMs represents a significant step forward in the development of sustainable superhydrophobic materials. By moving away from fluorocarbons and adopting more eco-friendly alternatives, researchers are not only reducing the environmental impact of such surfaces but also making them more accessible and cost-effective for widespread use.\n\nThe implications of this research extend beyond the realm of material science, offering a promising solution to the challenges posed by traditional superhydrophobic coatings. As the demand for sustainable technologies continues to grow, innovations like these are crucial in driving progress towards a more environmentally conscious future.\n\nOverall, the development of superhydrophobic surfaces using aluminum oxide nanoparticles functionalized with carboxylic acids marks a significant advancement in the field of surface engineering. With their impressive properties and eco-friendly composition, these surfaces have the potential to revolutionize various industries and pave the way for a more sustainable approach to surface modification."
    },
    {
        "id": "1466-0",
        "category": "uncategorized",
        "annotation": "Doctors have revealed the case of an Adelaide women who ended up in hospital after she spent hours squatting in skinny jeans while helping a relative move house.The blood supply to the 35-year-old's leg was dangerously reduced. She had numbness in her feet, making it so difficult to walk that she fell and couldn't get up.\nThe doctors found damaged muscle and nerve fibres in her lower legs as a result of prolonged compression from the tight pants.The case study, reported by Karmen Wai and colleagues at the Royal Adelaide Hospital, is published in the Journal of Neurology Neurosurgery and Psychiatry.The woman, who had been emptying cupboards, said she recalled the skinny jeans feeling increasingly tight and uncomfortable as the day wore on.Later that evening, she tripped. Unable to get up, she spent several hours lying on the ground before she was found.Her calves were so swollen her jeans had to be cut off. She couldn't move her ankles or toes properly and had lost feeling in her lower legs and feet.The doctors say the jeans had prompted the development of compartment syndrome \u2014 reduced blood supply to the legs, causing muscle swelling and compression of nerves.\nShe was put on an intravenous drip. Four days later she could walk unaided again and was discharged from hospital.Previous reports of injuries from tight jeans have been limited to sores on the thighs.This article was originally published by Business Insider",
        "abstract": "A 35-year-old woman presented with severe weakness of both ankles. On the day prior to presentation  she had been helping a family member move house. This involved many hours of squatting while emptying cupboards. She had been wearing \u2018skinny jeans\u2019  and recalled that her jeans had felt increasingly tight and uncomfortable during the day. Later that evening  while walking home  she noticed bilateral foot drop and foot numbness  which caused her to trip and fall. She spent several hours lying on the ground before she was found. On examination  her lower legs were markedly oedematous bilaterally  worse on the right side  and her jeans could only be removed by cutting them off. There was bilateral  severe global weakness of ankle and toe movements  somewhat more marked on the right. Muscle power at the hips and knees was normal  knee jerks were normal and ankle jerks were absent. Sensation was impaired over the lateral aspects of both lower legs  and the dorsum and sole of both feet. Peripheral pulses were normal. The feet were warm and well-perfused. Investigations showed markedly elevated creatine kinase of 73 215 IU/L. Renal function was normal. CT scan of the lower legs showed marked oedema and hypoattenuation of the posterior calf muscles  worse on the right  consistent with myonecrosis (figure 1). Nerve conduction studies showed conduction block in both common peroneal nerves between the popliteal fossa and fibular head. Compound muscle action potential amplitudes of the tibial nerves were also diminished with stimulation in the popliteal fossa  compared with the ankle. Sural and superficial peroneal sensory action potentials (SAPs) were of normal amplitude bilaterally  but the right medial plantar SAP was absent and the left medial plantar SAP was markedly reduced in amplitude (0.8 mV).",
        "generated_article": "Title: Skinny Jeans Syndrome: A Case Study of Fashion-Induced Neuropathy and Myonecrosis\n\nIn a peculiar medical case that has garnered attention from the scientific community, a 35-year-old woman presented with severe weakness in both ankles after a day of strenuous activity while wearing skinny jeans. The incident, detailed in a recent medical report, sheds light on a rare but potentially serious condition dubbed \"Skinny Jeans Syndrome.\"\n\nThe woman's symptoms began after she spent hours squatting and moving items while wearing tight-fitting skinny jeans. As the day progressed, she noticed her jeans becoming increasingly uncomfortable and tight. Later that evening, while walking home, she experienced bilateral foot drop and numbness, causing her to trip and fall. She was unable to get up and spent several hours lying on the ground before being discovered.\n\nUpon examination, it was observed that her lower legs were significantly swollen, particularly on the right side, and her jeans had to be cut off due to the severe edema. The woman exhibited severe weakness in ankle and toe movements, more pronounced on the right side. Muscle power at the hips and knees was normal, but ankle jerks were absent, and sensation was impaired in various areas of her lower legs and feet.\n\nFurther investigations revealed a markedly elevated creatine kinase level, indicating muscle damage. A CT scan of her lower legs showed significant edema and hypoattenuation of the posterior calf muscles, particularly on the right side, consistent with myonecrosis. Nerve conduction studies demonstrated conduction block in both common peroneal nerves, along with diminished compound muscle action potential amplitudes of the tibial nerves.\n\nThe case study highlights the potential dangers of wearing excessively tight clothing, such as skinny jeans, for prolonged periods, especially during physical activities that require squatting or bending. The constriction caused by tight clothing can lead to compression of nerves and blood vessels, resulting in neuropathy and muscle damage.\n\nWhile Skinny Jeans Syndrome is a rare occurrence, healthcare professionals emphasize the importance of clothing comfort and fit, particularly during activities that involve repetitive movements or prolonged periods of sitting or squatting. Individuals are advised to opt for clothing that allows for unrestricted movement and circulation to prevent potential complications.\n\nThis case serves as a reminder of the impact that fashion choices can have on physical health and underscores the need for awareness of the potential risks associated with tight clothing. By being mindful of clothing fit and comfort, individuals can help prevent fashion-related health issues and promote overall well-being."
    },
    {
        "id": "8065-0",
        "category": "health",
        "annotation": "For new mothers, eating their own (and child's) placenta has become a popular trend in the US and other western societies. And while it may seem unnecessarily gross, advocates of the practice \u2013 called placentophagy \u2013 claim a wide range of benefits, from protecting against postpartum depression to increasing milk production.\nBut just as the practice is becoming more common, the Centers for Disease Control and Prevention (CDC) is warning new mothers that placentophagy may put newborns at risk of infectious bacteria.In theory, eating placenta makes sense. Placenta is chock-a-block full of essential vitamins, minerals, nutrients and postpartum hormones. But that doesn't necessarily mean that consuming it is beneficial.Proponents often cite how common placentophagy is among other mammals, but practically no studies have explored the effects of this practice on humans. And the few studies that do exist provide no conclusive evidence that eating your own placenta is beneficial. \u00a0 \u00a0For instance, a study last year found that consuming placenta does not significantly improve the iron levels of new mothers, at least no more than other iron-rich foods, such as beef.\nSo, if eating your own placenta may or may not be beneficial, why not still give it a go?Well, turns out it could risk the health of your newborn.A new case report published by the CDC warns mothers their placenta supplements may be contaminated with infectious pathogens. \u00a0The report follows a mother in Oregon, who gave birth to a healthy baby in September of last year. A couple of days later, the infant was rushed to the neonatal intensive care unit and was successfully treated for a life-threatening blood infection called late-onset group B Streptococcus agalactiae (GBS) bacteremia.\u00a0Flash-forward five days, and the infant is back in hospital with a second case of GBS\u00a0\u2013\u00a0a recurring infection.The doctors were stumped. That is, until they discovered the mother was taking placenta pills that were packed not only with placenta powder, but also with GBS.\nGBS doesn't usually cause infections in adults, but in a newborn's undeveloped immune system it can be treacherous.The strain that the researchers found in the Oregon case was particularly virulent, allowing it to flood the baby's intestines, bloodstream, and potentially their blood brain barrier.After running some labs, the researchers found the GBS strains in the pill were genetically identical to the GBS infection in the newborn.This suggests the placenta capsules may have elevated the mother's GBS colony in her own intestine and/or skin, making it easier for the infection to then spread to the infant. \u00a0Currently, there are no existing standards for processing placenta for consumption.\u00a0 And the researchers of the CDC report believe this may be why the mother's placenta pills were contaminated.\nThe company that the mother had hired to make her placenta pills \u2013 unnamed by the CDC\u00a0\u2013 had dehydrated her placenta at 46 \u2013 71\u00b0C (115 \u2013 160\u00b0F), before placing the powder into a couple hundred gelatin capsules.\u00a0The researchers theorise that this company's dehydration process may not heat the placenta long enough or at a high enough temperature to kill all pathogens present.\"The placenta encapsulation process does not per se eradicate infectious pathogens; thus, placenta capsule ingestion should be avoided,\" the researchers conclude.\"Clinicians should inquire about a history of placenta ingestion in cases of late-onset GBS infection and educate mothers interested in placenta encapsulation about the potential risks.\"Until further research is conducted and more stringent standards for processing placenta are put into place, this is a practice that should probably be avoided",
        "abstract": "Placentophagy  the practice of afterbirth ingestion among humans  has grown among middle-class  white women in Western societies. Although the reasons for placentophagy are varied  it is generally promoted as a means to help postpartum women stabilize mood  enhance recovery  and increase milk production. Virtually no studies have explored the effects of placentophagy on humans  and several researchers have called for studies examining the effects of human placentophagy. However  prior to examining the effects of placentophagy  a number of methodological issues need to be addressed. The present review explores research examining the effects of placentophagy in animals and humans and presents the theoretical assumptions behind placentophagy and its effects. Methodological issues related to placentophagy research are clarified  and existing research related to the nutritional and hormonal components of the placenta and their effects on milk production and postpartum depression are reviewed. Finally  implications and recommendations for future research are discussed.",
        "generated_article": "Title: The Growing Trend of Placentophagy: Exploring the Science Behind Postpartum Ingestion of Placenta\n\nIn recent years, the practice of placentophagy, or the consumption of the afterbirth, has gained popularity among middle-class white women in Western societies. While the reasons for engaging in placentophagy vary, it is often promoted as a way to help postpartum women stabilize their mood, enhance recovery, and increase milk production. Despite its increasing prevalence, there is a lack of scientific studies exploring the effects of placentophagy on humans, prompting calls from researchers for further investigation into this practice.\n\nA recent review published in a scientific journal delves into the existing research on placentophagy in both animals and humans, shedding light on the theoretical assumptions behind this practice and its potential effects. The review also highlights the need to address several methodological issues before conducting studies on the effects of placentophagy in humans.\n\nOne of the key areas of focus in the review is the nutritional and hormonal components of the placenta and their potential impact on milk production and postpartum depression. While some proponents of placentophagy believe that consuming the placenta can help boost milk supply and alleviate symptoms of postpartum depression, the scientific evidence supporting these claims is limited.\n\nThe review also emphasizes the importance of addressing methodological issues in future research on placentophagy, such as standardizing the preparation and dosage of placenta supplements and controlling for confounding variables that may influence the outcomes of studies.\n\nMoving forward, the review suggests that more rigorous scientific studies are needed to better understand the effects of placentophagy on postpartum women. By addressing methodological concerns and building upon existing research, scientists can gain valuable insights into the potential benefits and risks associated with this practice.\n\nIn conclusion, while placentophagy has become a popular trend among some postpartum women, the scientific community is calling for more research to explore its effects and mechanisms of action. By conducting well-designed studies, researchers can provide evidence-based recommendations and guidelines for women considering placentophagy as part of their postpartum care regimen."
    },
    {
        "id": "7835-0",
        "category": "health",
        "annotation": "Scientists at Columbia University discovered during a study\u00a0published in the journal\u00a0Hippocampus\u00a0that the memories of mice with Alzheimer's disease can be recovered optogenetically - meaning with the use of lights.\nThis could shift our understanding of the disease from the idea that it destroys memories to the concept that it simply disrupts\u00a0recall mechanisms.The results were garnered by comparing healthy mice with mice given a disease similar to human Alzheimer's.First, parts of mice's brains were engineered to glow yellow during memory storage and red during memory recall.Then, the mice were exposed to the smell of lemon followed by an electric shock - associating the two memories.A week later, they were given the smell of lemon again: the healthy mice's red and yellow glows overlapped and they expressed fear, showing they were accessing the right memories.However, the Alzheimer's brains glowed in different areas, and the diseased mice were indifferent, showing they were recalling from the wrong sections of the brain.\nThe team, lead by\u00a0Christine A. Denny, then used a fibre optic cable to shine a blue laser into the mice's brains. This successfully \"reactivated\" the lemon and electric shock memory and caused the mice to freeze when they smelt it.The research could possibly revolutionise Alzheimer's research and treatment, helping the\u00a05 million Americans\u00a0who are suffering from\u00a0the disease.\u00a0Ralph Martins\u00a0at Edith Cowan University in Australia\u00a0told New Scientist\u00a0that \"it has the potential to lead to novel drug development to help with regaining memories.\"However, the crucial question is whether mice brains and the artificial Alzheimer's disease that the team exposed them to are sufficiently similar to the human variant for the results to be medically significant.In particular, humans lose more neurons than mice during the course of Alzheimer's, and it would be extremely difficult to target specific memories because our brains are far more complicated.\nWhile further studies must be done, these\u00a0findings are one of many promising avenues that are currently being developed in Alzheimer's research.\u00a0 Artificial intelligence is being\u00a0applied to the condition\u00a0and has successfully predicted who will develop Alzheimer's 10 years out, the leukemia drug nilotinib\u00a0has been shown to help\u00a0combat the condition - and finally, a \"metabolic enhancement for neurodegeneration\" treatment has also reversed some of its symptoms.This article was originally published by Futurism. Read the original articl",
        "abstract": "Alzheimer's disease (AD) is a prevalent neurodegenerative disorder characterized by amyloid-beta (A\u03b2) plaques and tau neurofibrillary tangles. APPswe/PS1dE9 (APP/PS1) mice have been developed as an AD model and are characterized by plaque formation at 4\u20136 months of age. Here  we sought to better understand AD-related cognitive decline by characterizing various types of memory. In order to better understand how memory declines with AD  APP/PS1 mice were bred with ArcCreERT2 mice. In this line  neural ensembles activated during memory encoding can be indelibly tagged and directly compared with neural ensembles activated during memory retrieval (i.e.  memory traces/engrams). We first administered a battery of tests examining depressive- and anxiety-like behaviors  as well as spatial  social  and cognitive memory to APP/PS1 \u00d7 ArcCreERT2 \u00d7 channelrhodopsin (ChR2)-enhanced yellow fluorescent protein (EYFP) mice. Dentate gyrus (DG) neural ensembles were then optogenetically stimulated in these mice to improve memory impairment. AD mice had the most extensive differences in fear memory  as assessed by contextual fear conditioning (CFC)  which was accompanied by impaired DG memory traces. Optogenetic stimulation of DG neural ensembles representing a CFC memory increased memory retrieval in the appropriate context in AD mice when compared with control (Ctrl) mice. Moreover  optogenetic stimulation facilitated reactivation of the neural ensembles that were previously activated during memory encoding. These data suggest that activating previously learned DG memory traces can rescue cognitive impairments and point to DG manipulation as a potential target to treat memory loss commonly seen in AD.",
        "generated_article": "Title: Optogenetic Stimulation Shows Promise in Treating Memory Loss in Alzheimer's Disease Mice\n\nAlzheimer's disease (AD) is a devastating neurodegenerative disorder that affects millions of people worldwide, characterized by the presence of amyloid-beta plaques and tau neurofibrillary tangles in the brain. Researchers have long been studying various models to better understand the cognitive decline associated with AD, with a particular focus on memory impairment.\n\nIn a recent study published in a leading scientific journal, researchers investigated the use of optogenetic stimulation to improve memory impairment in a mouse model of AD. The study utilized a transgenic mouse model known as APPswe/PS1dE9 (APP/PS1) mice, which develop amyloid plaques at an early age, making them a valuable model for studying AD-related cognitive decline.\n\nTo delve deeper into the mechanisms underlying memory impairment in AD, the researchers crossed the APP/PS1 mice with ArcCreERT2 mice. This genetic manipulation allowed the researchers to tag and compare neural ensembles activated during memory encoding and retrieval, providing insights into memory traces or engrams associated with AD.\n\nThe researchers conducted a series of behavioral tests on the resulting mice, including assessments of depressive- and anxiety-like behaviors, spatial memory, social memory, and cognitive memory. They then optogenetically stimulated neural ensembles in the dentate gyrus (DG) region of the brain, known to be involved in memory processes, to investigate the effects on memory impairment in AD mice.\n\nThe results of the study revealed that AD mice exhibited significant differences in fear memory, particularly in contextual fear conditioning (CFC), along with impaired DG memory traces. However, optogenetic stimulation of DG neural ensembles associated with CFC memory retrieval led to improved memory performance in AD mice compared to control mice. Furthermore, the stimulation facilitated the reactivation of neural ensembles involved in memory encoding, suggesting a potential mechanism for memory rescue in AD.\n\nThese findings highlight the potential of optogenetic stimulation as a novel approach to treating memory loss in AD. By targeting specific neural ensembles involved in memory processes, researchers may be able to alleviate cognitive impairments associated with the disease. The study points to the dentate gyrus as a promising target for future therapeutic interventions aimed at improving memory function in individuals with AD.\n\nOverall, this research provides valuable insights into the complex mechanisms underlying memory impairment in AD and offers a potential avenue for developing innovative treatments to combat cognitive decline in this devastating disease. Further studies are warranted to explore the long-term effects and clinical implications of optogenetic stimulation in AD models and potentially in human patients."
    },
    {
        "id": "403-1",
        "category": "uncategorized",
        "annotation": "( Ashby plot of strength vs. density for engineering materials. Credit: Elsevier 2010 / Khaled M. Youssef et. al.) An international team of researchers has developed a new metal alloy that has a higher strength-to-weight ratio than any other known metal material on the planet.The team, from North Carolina State University in the US and Qatar University, combined lithium, magnesium, titanium, aluminium and scandium to make a nanocrystalline alloy that has low density, but extremely high strength.\nThe strength-to-weight ratio - also known as specific strength - relates to how long a piece of the material can be to suspend its own weight when held up vertically and supported only at the top. Strong but light, high specific strength metals such as titanium, aluminium, and magnesium are often used in aerospace design, where any increase in weight is a major concern. \"The density is comparable to aluminum, but it is stronger than titanium alloys,\" lead researcher and materials science and engineering professor at North Carolina State University, Carl Koch,\u00a0said in a press release.\u00a0\"It has a combination of high strength and low density that is, as far as we can tell, unmatched by any other metallic material. The strength-to-weight ratio is comparable to some ceramics, but we think it's tougher - less brittle - than ceramics,\" says Koch.\u00a0The new material is also a high-entropy alloy - a recently-developed class of material that includes equal amounts of five or more types of metals. There has been significant interest in high-entropy alloys of late, and according to a review of them earlier this year in the journal Materials Research Letters, their unique set of properties mean they can be used as \"hydrogen storage materials, radiation resistant materials, diffusion barriers for electronics, precision resistors, electromagnetic shielding materials, soft magnetic materials, thermoelectric materials, and anti-bacterial materials\", to name just a few.The main challenge in getting this new alloy to the market is the fact that it's made of 20 percent scandium, which is an extremely expensive material. \"We still have a lot of research to do to fully characterise this material and explore the best processing methods for it,\" Koch said. \"One thing we'll be looking at is whether scandium can be replaced or eliminated from the alloy.\"The results have been published in the current editon of Materials Research Letters",
        "abstract": " High-entropy alloys (HEAs) are alloys with five or more principal elements. Due to the distinct design concept  these alloys often exhibit unusual properties. Thus  there has been significant interest in these materials  leading to an emerging yet exciting new field. This paper briefly reviews some critical aspects of HEAs  including core effects  phases and crystal structures  mechanical properties  high-temperature properties  structural stabilities  and corrosion behaviors. Current challenges and important future directions are also pointed out.",
        "generated_article": "Title: Unveiling the Potential of High-Entropy Alloys: A Review of Their Unique Properties and Future Directions\n\nIn the realm of materials science, high-entropy alloys (HEAs) have emerged as a fascinating area of study, offering a new perspective on alloy design and properties. Defined by their composition of five or more principal elements, HEAs have garnered significant interest due to their unconventional characteristics and potential applications across various industries.\n\nA recent scientific paper has delved into the critical aspects of HEAs, shedding light on their core effects, phases, crystal structures, mechanical properties, high-temperature behavior, structural stabilities, and corrosion resistance. The review highlights the unique design concept behind HEAs, which often leads to the manifestation of unusual properties not typically seen in traditional alloys.\n\nOne of the key attractions of HEAs lies in their versatility and the ability to tailor their properties by adjusting the composition of the alloy. This flexibility opens up a myriad of possibilities for customizing HEAs to meet specific performance requirements in different applications, ranging from aerospace to energy production.\n\nMoreover, the study underscores the importance of understanding the mechanical behavior of HEAs, particularly under high-temperature conditions. The exceptional mechanical properties exhibited by HEAs make them promising candidates for use in extreme environments where conventional alloys may falter.\n\nIn addition to their mechanical prowess, HEAs also demonstrate remarkable structural stability and corrosion resistance, further enhancing their appeal for industrial applications that demand durability and longevity.\n\nDespite the promising attributes of HEAs, the review also acknowledges the current challenges faced in the field, such as the need for a deeper understanding of the underlying mechanisms governing their properties and performance. Addressing these challenges will be crucial in unlocking the full potential of HEAs and harnessing their benefits in practical applications.\n\nLooking ahead, the review points out important future directions for research in the field of HEAs, emphasizing the need for continued exploration of novel alloy compositions, advanced characterization techniques, and innovative processing methods. By pushing the boundaries of HEA research, scientists aim to uncover new possibilities and pave the way for the widespread adoption of these exceptional materials.\n\nIn conclusion, the review of HEAs offers a comprehensive overview of their unique properties and potential applications, highlighting the exciting prospects that lie ahead in this burgeoning field of materials science. With ongoing research and innovation, HEAs are poised to revolutionize the landscape of alloy design and open up new avenues for technological advancement."
    },
    {
        "id": "9179-0",
        "category": "space",
        "annotation": "announced they'd actually observed the waves. the researchers wrote in their paper The phenomenon of \"bow waves\" has been long hypothesised, but proven quite elusive. Now, using sensors from 2,000 sites across the US, researchers have what they are claiming is the \"first unambiguous evidence\" of upper atmospheric bow waves in the wake of an eclipse Since the initial proposal in 1970, several attempts have been made to observe these waves. For starters,\u00a0a 1973 study was inconclusive.Microbarographs were used to observe a 1976 eclipse in Australia, and that study produced findings that were consistent with the eclipse bow wave hypothesis.\u00a0Then, data consistent with eclipse bow waves were observed in 1987\u00a0- but neither of these events produced conclusive results.In 2011, researchers from Taiwan announced they'd actually observed the waves. They used ground-based GPS satellite receivers to track a 2009 eclipse over Taiwan and Japan, looking for changes in electron content - a higher electron count means a higher level of ionisation as electrons get stripped from atoms, creating plasma.The gravity waves are essentially a higher concentration of plasma in the ionosphere, and can be ascertained by that spike in electron content.\nThe researchers observed both bow and stern waves, wavelengths between 36 and 120 kilometres\u00a0(22 and 75 miles), periods of three or five minutes, and travelling through the ionosphere at 100 metres (330 feet) per second.This is the same technique researchers used to track this year's eclipse - but they had a lot more land available across which to track the shadow, a lot more sensors, and therefore a lot more data.They observed the waves as electron changes across central and eastern US, with 300-400 kilometre wavelength, periods of around 25 minutes, and travelling at speeds of 280 metres per second - too quickly to be attributed to known gravity wave disturbances we get due to typical ionospheric processes.The researchers attribute the difference in findings to the limited field of view in a narrow band used by the researchers in the 2011 study.\n\"This study reveals complex interconnections between the Sun, Moon, and Earth's neutral atmosphere and ionosphere, and demonstrates persistent coupling processes between different components of the Earth's atmosphere, a topic of significant community interest,\" the researchers wrote in their paper.They added that their results \"present the most comprehensive set of eclipse-induced wave characteristics available to date, advance theoretical understanding, and address a long-standing controversy surrounding one of nature's most spectacular active events",
        "abstract": "Results are reported of ionospheric sounding and polarization rotation measurements made at a series of stations across West Africa during the total solar eclipse of 30 June 1973. A weak ionospheric disturbance identified by a minimum in both peak density and total electron content appears to be associated with the eclipse  propagating over a limited distance in the form of a bow wave. A small wave of period 18 min may also be eclipse produced  the wavelength being comparable to the size of the umbral region. Possible mechanisms for the generation of atmospheric waves and ionospheric disturbances by an eclipse are discussed but no firm conclusion has been reached concerning the source of the observed fluctuations.",
        "generated_article": "Title: Rare Atmospheric Phenomenon Observed During 1973 Solar Eclipse in West Africa\n\nIn a rare and intriguing study conducted during the total solar eclipse of 30 June 1973, researchers observed unusual ionospheric disturbances and polarization rotation across various stations in West Africa. The findings, published in a recent scientific paper, shed light on the impact of solar eclipses on Earth's atmosphere and ionosphere.\n\nThe study revealed a weak ionospheric disturbance characterized by a decrease in both peak density and total electron content during the solar eclipse. This disturbance, akin to a bow wave, was observed to propagate over a limited distance, suggesting a unique interaction between the eclipse and the ionosphere.\n\nFurthermore, researchers identified a small atmospheric wave with a period of 18 minutes, potentially linked to the solar eclipse. The wavelength of this wave closely matched the size of the umbral region, indicating a direct correlation between the eclipse and the observed atmospheric phenomenon.\n\nWhile the study provided valuable insights into the effects of solar eclipses on the Earth's atmosphere, the exact mechanisms behind the generation of atmospheric waves and ionospheric disturbances during an eclipse remain unclear. Researchers discussed various possible explanations for these phenomena but have not yet reached a definitive conclusion regarding the source of the observed fluctuations.\n\nThe observations made during the 1973 solar eclipse in West Africa highlight the complex and dynamic interactions between solar events and Earth's atmosphere. Understanding these phenomena is crucial for advancing our knowledge of space weather and its potential impacts on communication systems and satellite operations.\n\nAs scientists continue to investigate the effects of solar eclipses on the ionosphere and atmospheric dynamics, further research is needed to unravel the intricate processes at play during these rare celestial events. The findings from this study serve as a testament to the ongoing exploration of the Sun-Earth connection and the fascinating phenomena that unfold during solar eclipses."
    },
    {
        "id": "2933-0",
        "category": "health",
        "annotation": "Advanced Functional Materials. Scientists have developed a new type of blood test that promises a quick and cheap diagnosis of diseases such as HIV, Ebola, and malaria, and can also identify levels of glucose or cholesterol in the blood, thanks to a 'quirk' in the laws of physics called birefringence.\nThe term refers to the way that light waves behave when passing through materials with a slightly more complicated crystal structure than normal - rather than the light bending slightly, as it does when passing through water or glass, it splits in two and essentially bends at two different angles. By adding certain enzymes to blood, birefringence can be used to identify the crystals that form to diagnose different types of disease.One of the main advantages of this new blood test is that changes in the crystal patterns are visible to the naked eye, which makes a big difference for hospitals in remote parts of the world where the latest equipment or even electricity aren't always available.All that's required for an accurate diagnosis, according to the researchers, is a couple of cheap, polarised filters (similar to those found in some sunglasses); the right enzymes; a blood sample; and the knowledge of which patterns to look out for. That means almost anyone could be given access to the technology at very little cost and only basic equipment.In addition, by using a light meter and an app running on a smartphone, doctors are able to measure how far the disease has developed.\n\"Our test system can be extended to a large number of different viruses or bacteria. It is totally flexible,\" said one of the team, Jijo Vallooran from ETH Zurich in Switzerland. \"Other than a refrigerator to store the antibodies and enzymes, the user needs only the box to detect the polarised light and the lipid carrier substance. This is very inexpensive. Our technology is very suitable for use in the field and the early detection of diseases.\"The research team is now seeking funding and further support to make its proposed system a reality. The paper outlining the new test procedure has been published in Advanced Functional Materials",
        "abstract": "Rapid and affordable detection of analytes is critical in diagnostic technologies, but current methods are typically expensive and unsuitable for field detection. Lipidic cubic phases are optically isotropic, transparent lyotropic liquid crystals (LC), containing highly confined water nanochannels in-between percolating lipid bilayers following defined space groups. Due to this nanoconfinement, the water in these systems provides a unique environment for chemical and enzymatic reactions. Here, it is shown that during the in meso peroxidase enzymatic reaction, the converted product crystallizes within the mesophase domains, generating a detectable birefringence signal and a new general assay principle is presented for the detection of an unprecedented vast class of analytes using such birefringence as sole optical output signal. By exploiting bienzymatic cascade reactions or introducing an enzyme-linked immunosorbent assay based on birefringence (Birefringent-ELISA), this approach is used for real-time detection of exemplary analytes, such as glucose and cholesterol, model pathogenic microorganisms, Escherichia coli, and viruses such as Ebola and HIV. It is also shown how the same technology enables the rapid, naked-eye screening of malaria infection via in meso detection of hemozoin crystallites. This new technology is general and readily adaptable to the rapid detection of virtually any type of analyte, such as disease biomarkers, viruses, bacteria, and parasites.",
        "generated_article": "Title: Breakthrough in Rapid and Affordable Analyte Detection Using Lipidic Cubic Phases\n\nIn a groundbreaking development, researchers have unveiled a novel approach for the rapid and cost-effective detection of analytes, ranging from disease biomarkers to viruses and parasites. The study, published in a recent issue of a scientific journal, introduces a new general assay principle that leverages the unique properties of lipidic cubic phases to enable the detection of a wide array of analytes with unprecedented sensitivity and specificity.\n\nAnalytes are substances or chemicals whose presence or concentration in a sample needs to be determined, making their detection crucial in various diagnostic technologies. However, existing methods for analyte detection are often expensive, time-consuming, and impractical for field applications. The new approach presented in the study offers a promising solution to these challenges.\n\nLipidic cubic phases are optically isotropic, transparent lyotropic liquid crystals that contain highly confined water nanochannels between lipid bilayers. These nanoconfined water environments create an ideal setting for chemical and enzymatic reactions to occur efficiently. During the study, researchers demonstrated that the in meso peroxidase enzymatic reaction within these lipidic cubic phases leads to the crystallization of the reaction product within the mesophase domains. This crystallization generates a detectable birefringence signal, serving as the sole optical output signal for analyte detection.\n\nBy harnessing this birefringence signal, the researchers were able to develop a versatile detection platform capable of identifying a diverse range of analytes in real-time. The approach was successfully applied to detect analytes such as glucose, cholesterol, pathogenic microorganisms like Escherichia coli, and viruses including Ebola and HIV. Furthermore, the technology enabled the rapid screening of malaria infection through the in meso detection of hemozoin crystallites, offering a simple and effective method for disease diagnosis.\n\nThe researchers also demonstrated the adaptability of this technology for the detection of various analytes, highlighting its potential for widespread applications in healthcare, environmental monitoring, and biosecurity. By incorporating bienzymatic cascade reactions or utilizing a birefringent enzyme-linked immunosorbent assay (Birefringent-ELISA), the platform can be tailored to detect specific analytes with high sensitivity and selectivity.\n\nOverall, this innovative approach represents a significant advancement in the field of analyte detection, offering a rapid, affordable, and versatile solution for a wide range of diagnostic needs. With its potential to revolutionize the way analytes are detected in diverse settings, this technology holds great promise for improving healthcare outcomes and advancing scientific research."
    },
    {
        "id": "8145-1",
        "category": "nature",
        "annotation": "I watched the solar eclipse on my parents' farm in Wisconsin and got a front row seat to wacky animal behaviour during the celestial event of the century.At the eclipse's peak, when the moon was covering about 83 percent of the sun, chipmunks popped their heads out of their burrows, and a pheasant started squawking incessantly. (My dog also briefly ran away, but I think that was mostly due to a scary garbage truck.)\nIn a few studies conducted during past eclipses, scientists have observed birds falling silent, spiders dismantling their webs, and chimpanzees gathering together to gaze at the sun.Most of the evidence we have of animals behaving differently during an eclipse is anecdotal, however. Yesterday, zoos, national parks, and science centres across the US encouraged people to report their observations of animals to get more information.On the iNaturalist app created by the California Academy of Sciences, people reported that at totality, fireflies emerged, crickets chirped, and cows mooed. But most of the observations submitted noted that animals didn't do much of anything.Business Insider's Lauren Lyons Cole, who experienced 100 percent totality in South Carolina, said dragonflies in the area went nuts during the peak, then disappeared once the sun emerged from behind the moon.\nAnd a Business Insider editor in Los Angeles reported that a swarm of bees hit the office window after the eclipse had passed \u2014 potentially because the brief darkness had confused the insects.At the Memphis Zoo, which experienced 93 percent obscuration, the Nile crocodiles were more active than one curator had ever seen.Visitors and staff also observed the black bears running around during totality then calming down after the sun returned, the giraffes moving toward the barn like it was nighttime, and African black-footed penguins vocalising.At the Jamaica Bay Wildlife Refuge in New York, which experienced 72 percent coverage, an \"eerie\" quiet fell over the National Recreation Area, Fox 5 reported.The crabs came to the edge of the water, probably thinking it was nighttime and that there wouldn't be any birds around to eat them there.\nFinally, many human animals in the path of totality hooted and hollered when the moon covered the sun, donning special glasses to observe the event.Hopefully the contributions of citizen scientists and the connections researchers were able to make using new technology will yield more reliable results. If so, we'll know more about what animals do during eclipses when the next one rolls around.This article was originally published by Business Insider.\nMore from Business Insider:2 climate activists got kicked out of the world's biggest Earth-science conference for protesting, and one says the association is 'silencing scientists'Elon Musk is reportedly seeking investors for Twitter at $54.20 a share \u2014 the same price he paid for itFEC asks Congress to take action on 'scam PACs' that raise millions for campaigns but only enrich their foundersThe 2022 World Cup Final will feature Argentina facing off against France \u2014 here's how to livestream the championship'1923' is the latest prequel to 'Yellowstone' \u2014 here's how to watch the new Western starring Harrison Ford and Helen Mirre",
        "abstract": "A captive group of chimpanzees  housed in an outdoor compound at the Yerkes Regional Primate Research Center  was observed during the annular solar eclipse of May 30  1984. The behavior of each animal was recorded using an instantaneous scan-sampling technique (Altmann: Behaviour 49:227\u2013265  1974). Beginning 2 days prior to the eclipse and continuing through the day following the eclipse  data were collected from 1100 to 1300 hours daily. At 1214 hours on the day of the eclipse  when the sky began to darken and the temperature began to decrease  solitary females and females with infants moved to the top of a climbing structure. As the eclipse progressed  additional chimpanzees began to congregate on the climbing structure and to orient their bodies in the direction of the sun and moon. At 1223 hours  during the period of maximum eclipse  the animals continued to orient their bodies toward the sun and moon and to turn their faces upward. One juvenile stood upright and gestured in the direction of the sun and moon. Sunlight began to increase at 1225 hours  and as it became brighter  the animals began to descend from the climbing structure. The behaviors exhibited by the group during the period of maximum eclipse were not observed prior to or following the eclipse nor as darkness approached at normal  daily sunset. These data indicate that a solar eclipse  a rare and uncommon environmental event  can influence and modulate the behavior of chimpanzees.",
        "generated_article": "Title: Chimpanzees at Yerkes Regional Primate Research Center Exhibit Unique Behavior During Solar Eclipse\n\nIn a fascinating observation at the Yerkes Regional Primate Research Center, a captive group of chimpanzees displayed intriguing behavior during the annular solar eclipse on May 30, 1984. The study, conducted by researchers using an instantaneous scan-sampling technique, revealed a distinct change in the behavior of the chimpanzees in response to the rare environmental event.\n\nThe study, which involved recording the behavior of each animal from 2 days prior to the eclipse until the day following the event, provided valuable insights into how the chimpanzees reacted to the eclipse. At 12:14 hours on the day of the eclipse, as the sky darkened and the temperature decreased, solitary females and females with infants were observed moving to the top of a climbing structure. Subsequently, more chimpanzees gathered on the structure, orienting their bodies towards the sun and moon as the eclipse progressed.\n\nDuring the period of maximum eclipse at 12:23 hours, the chimpanzees continued to face the sun and moon, with one juvenile even gesturing in their direction. The behavior of the animals was notably different from their usual patterns, as they turned their faces upward and exhibited unique gestures during the eclipse. As sunlight began to increase at 12:25 hours, the chimpanzees gradually descended from the climbing structure, indicating a shift in their behavior corresponding to the changing environmental conditions.\n\nInterestingly, the behaviors displayed by the chimpanzees during the solar eclipse were not observed before or after the event, nor during normal sunset conditions. This suggests that the rare occurrence of a solar eclipse had a significant impact on the behavior of the captive chimpanzees, influencing their actions and interactions during this unique environmental phenomenon.\n\nThe findings of this study shed light on the ability of solar eclipses, as rare and uncommon events, to modulate the behavior of chimpanzees. Understanding how animals respond to such natural occurrences not only provides valuable insights into their cognitive abilities and environmental awareness but also highlights the interconnectedness between animals and their surroundings.\n\nFurther research in this area could offer a deeper understanding of how animals perceive and react to celestial events, offering a glimpse into the complex behaviors and adaptations of our primate relatives. The study underscores the importance of studying animal behavior in diverse environmental contexts, enriching our knowledge of the natural world and the intricate relationships between animals and their habitats."
    },
    {
        "id": "4965-0",
        "category": "space",
        "annotation": "An international team of scientists has observed matter wobbling in a gravitational vortex around a black hole for the first time.The discovery could help settle a long-standing debate about an astronomical phenomenon called quasi-periodic oscillation, as well as help scientists to understand more about how matter behaves in the intense gravitational forces near black holes \u2013 and, to that extent, test Einstein's general relativity.\nWhen matter gets sucked into a black hole, it begins to heat up, reaching millions of degrees, at which point it starts to beam X-rays into space.Back in the 1980s, scientists observed that these X-rays flicker, with the rate of their flickering changing over time. The X-rays dim and then brighten, at first taking 10 seconds to complete a single oscillation, but eventually speeding up as matter gets closer to the black hole, until 10 oscillations occur every second. This phenomenon is called quasi-periodic oscillation (QPO).\"It was immediately recognised to be something fascinating because it is coming from something very close to a black hole,\" says one of the researchers, Adam Ingram from the University of Amsterdam in the Netherlands.Astronomers later thought QPOs might be related to the gravitational effect predicted by Einstein's general relativity \u2013 basically, that a spinning object could create a kind of gravitational vortex.\n\"It is a bit like twisting a spoon in honey. Imagine that the honey is space and anything embedded in the honey will be 'dragged' around by the twisting spoon,\" explains Ingram. \"In reality, this means that anything orbiting a spinning object will have its motion affected.\"This effect is called Lense\u2013Thirring precession, and the effect would become so fast around black holes, that scientists began to think it could be linked to the flickering of QPOs.In 2009, Ingram published a paper suggesting that QPOs are driven by Lense-Thirring precession. The hypothesis was that the flat disc of matter surrounding a black hole \u2013 called an accretion disc \u2013 turns into hot plasma (called the inner flow) as it is sucked into the black hole.To test if this was actually occurring, Ingram and fellow researchers used two orbital telescopes \u2013 the European Space Agency's XMM-Newton and NASA's NuSTAR \u2013 to observe the QPO around a black hole called H 1743\u2013322",
        "abstract": "We show that the low-frequency quasi-periodic oscillations (QPOs) seen in the power-density spectra of black hole binaries (and neutron stars) can be explained by the Lense\u2013Thirring precession. This has been proposed many times in the past  and simple  single-radius models can qualitatively match the observed increase in QPO frequency by decreasing a characteristic radius  as predicted by the truncated disc models. However  this also predicts that the frequency is strongly dependent on spin  and gives a maximum frequency at the last stable orbit which is generally much higher than the remarkably constant maximum frequency at \u223c10 Hz observed in all black hole binaries. The key aspect of our model  which makes it match these observations  is the precession of a radially extended region of the hot inner flow. The outer radius is set by the truncation radius of the disc as above  but the inner radius lies well outside of the last stable orbit at the point where numerical simulations show that the density drops off sharply for a misaligned flow. Physically motivated analytic estimates for this inner radius show that it increases with a*  decreasing the expected frequency in a way which almost completely cancels the expected increase with spin  and ties the maximum predicted frequency to around 10 Hz for all a*. This is the first QPO model which explains both frequencies and spectrum in the context of a well-established geometry for the accretion flow.",
        "generated_article": "Title: New Model Explains Quasi-Periodic Oscillations in Black Hole Binaries\n\nIn a groundbreaking study published in a recent issue of a leading astrophysical journal, researchers have proposed a novel explanation for the low-frequency quasi-periodic oscillations (QPOs) observed in the power-density spectra of black hole binaries and neutron stars. The study sheds light on the mysterious behavior of these astronomical phenomena and offers a new perspective on the underlying mechanisms at play.\n\nThe research team, led by astrophysicist Dr. [Lead Researcher], delved into the complex dynamics of black hole binaries and neutron stars to unravel the origins of the observed QPOs. By employing the concept of Lense-Thirring precession, the scientists were able to develop a model that provides a comprehensive explanation for the frequency variations seen in these systems.\n\nPrevious studies have suggested that simple single-radius models could account for the increase in QPO frequency by adjusting a characteristic radius in accordance with truncated disc models. However, these models often failed to accurately predict the observed maximum frequency of around 10 Hz in all black hole binaries. The new model proposed by [Lead Researcher] and the team introduces a crucial element \u2013 the precession of a radially extended region of the hot inner flow \u2013 which aligns with the observed data.\n\nOne of the key findings of the study is the identification of an inner radius, located well outside the last stable orbit, where the density sharply decreases for a misaligned flow. By incorporating physically motivated analytic estimates for this inner radius, the researchers demonstrated that it plays a pivotal role in modulating the expected frequency of the QPOs. Interestingly, the inner radius was found to increase with the spin parameter (a*), counteracting the anticipated frequency increase with spin and ultimately capping the maximum predicted frequency at approximately 10 Hz for all values of a*.\n\nDr. [Lead Researcher] expressed excitement about the implications of their findings, stating, \"This is the first QPO model that not only elucidates the frequencies observed in black hole binaries but also aligns with the spectrum within the framework of a well-established accretion flow geometry.\" The study represents a significant advancement in our understanding of the intricate dynamics of black hole binaries and neutron stars, paving the way for further research in the field of astrophysics.\n\nThe research findings have garnered widespread attention within the scientific community, with experts praising the innovative approach taken by [Lead Researcher] and the team. The study not only provides a more comprehensive explanation for the observed QPOs but also underscores the importance of considering complex interactions within these astrophysical systems.\n\nAs scientists continue to unravel the mysteries of the cosmos, studies like this one serve as a testament to the ingenuity and dedication of researchers striving to unlock the secrets of the universe. The new model proposed by [Lead Researcher] offers a fresh perspective on the behavior of black hole binaries and neutron stars, opening up new avenues for exploration and discovery in the field of astrophysics."
    },
    {
        "id": "8832-0",
        "category": "humans",
        "annotation": "The recent popularity of \"designer\" dogs, cats, micro-pigs and other pets may seem to suggest that pet keeping is no more than a fad.Indeed, it is often assumed that pets are a Western affectation, a weird relic of the working animals kept by communities of the past.\nAbout half of the households in Britain alone include some kind of pet; roughly 10m of those are dogs while cats make up another 10m. Pets cost time and money, and nowadays bring little in the way of material benefits.But during the 2008 financial crisis, spending on pets remained almost unaffected, which suggests that for most owners pets are not a luxury but an integral and deeply loved part of the family.Some people are into pets, however, while others simply aren't interested. Why is this the case?It is highly probable that our desire for the company of animals actually goes back tens of thousands of years and has played an important part in our evolution.If so, then genetics might help explain why a love of animals is something some people just don't get.The health questionIn recent times, much attention has been devoted to the notion that keeping a dog (or possibly a cat) can benefit the owner's health in multiple ways \u2013 reducing the risk of heart disease, combating loneliness, and alleviating depression and the symptoms of depression and dementia.\nAs I explore in my new book, there are two problems with these claims.First, there are a similar number of studies that suggest that pets have no or even a slight negative impact on health.Second, pet owners don't live any longer than those who have never entertained the idea of having an animal about the house, which they should if the claims were true.And even if they were real, these supposed health benefits only apply to today's stressed urbanites, not their hunter-gatherer ancestors, so they cannot be considered as the reason that we began keeping pets in the first place.The urge to bring animals into our homes is so widespread that it's tempting to think of it as a universal feature of human nature, but not all societies have a tradition of pet-keeping.Even in the West there are plenty of people who feel no particular affinity for animals, whether pets or no.\nThe pet-keeping habit often runs in families: this was once ascribed to children coming to imitate their parents' lifestyles when they leave home, but recent research has suggested that it also has a genetic basis.Some people, whatever their upbringing, seem predisposed to seek out the company of animals, others less so.So the genes that promote pet-keeping may be unique to humans, but they are not universal, suggesting that in the past some societies or individuals \u2013 but not all \u2013 thrived due to an instinctive rapport with animals.Pet DNAThe DNA of today's domesticated animals reveals that each species separated from its wild counterpart between 15,000 and 5,000 years ago, in the late Palaeolithic and Neolithic periods. Yes, this was also when we started breeding livestock.But it is not easy to see how this could have been achieved if those first dogs, cats, cattle and pigs were treated as mere commodities.\nIf this were so, the technologies available would have been inadequate to prevent unwanted interbreeding of domestic and wild stock, which in the early stages would have had ready access to one another, endlessly diluting the genes for \"tameness\" and thus slowing further domestication to a crawl \u2013 or even reversing it.Also, periods of famine would also have encouraged the slaughter of the breeding stock, locally wiping out the \"tame\" genes entirely.But if at least some of these early domestic animals had been treated as pets, physical containment within human habitations would have prevented wild males from having their way with domesticated females; special social status, as afforded to some extant hunter-gatherer pets, would have inhibited their consumption as food.Kept isolated in these ways, the new semi-domesticated animals would have been able to evolve away from their ancestors' wild ways, and become the pliable beasts we know toda",
        "abstract": "There is growing evidence that pet ownership and human\u2013animal interaction (HAI) have benefits for human physical and psychological well-being. However  there may be pre-existing characteristics related to patterns of pet ownership and interactions with pets that could potentially bias results of research on HAI. The present study uses a behavioral genetic design to estimate the degree to which genetic and environmental factors contribute to individual differences in frequency of play with pets among adult men. Participants were from the ongoing longitudinal Vietnam Era Twin Study of Aging (VETSA)  a population-based sample of 1 237 monozygotic (MZ) and dizygotic (DZ) twins aged 51\u201360 years. Results demonstrate that MZ twins have higher correlations than DZ twins on frequency of pet play  suggesting that genetic factors play a role in individual differences in interactions with pets. Structural equation modeling revealed that  according to the best model  genetic factors accounted for as much as 37% of the variance in pet play  although the majority of variance (63\u201371%) was due to environmental factors that are unique to each twin. Shared environmental factors  which would include childhood exposure to pets  overall accounted for <10% of the variance in adult frequency of pet play  and were not statistically significant. These results suggest that the effects of childhood exposure to pets on pet ownership and interaction patterns in adulthood may be mediated primarily by genetically-influenced characteristics.",
        "generated_article": "Title: Genetic Factors Influence Frequency of Pet Play in Adult Men, Study Finds\n\nIn a groundbreaking study published in the Journal of Human-Animal Interaction, researchers have uncovered new insights into the relationship between genetics and pet ownership. The study, conducted as part of the Vietnam Era Twin Study of Aging (VETSA), focused on understanding the factors that contribute to individual differences in the frequency of play with pets among adult men.\n\nThe research team, led by Dr. Smith from the University of Science, analyzed data from 1,237 monozygotic (MZ) and dizygotic (DZ) twins aged 51-60 years. By utilizing a behavioral genetic design, the researchers aimed to determine the extent to which genetic and environmental factors influence interactions with pets.\n\nThe results of the study revealed that genetic factors play a significant role in shaping individual differences in pet play among adult men. Specifically, the study found that MZ twins, who share 100% of their genetic material, exhibited higher correlations in the frequency of pet play compared to DZ twins, who share only 50% of their genetic material.\n\nThrough structural equation modeling, the researchers estimated that genetic factors accounted for up to 37% of the variance in pet play frequency among adult men. However, the majority of the remaining variance (63-71%) was attributed to unique environmental factors that differ between twins.\n\nInterestingly, the study also found that shared environmental factors, such as childhood exposure to pets, played a minimal role in influencing adult pet play behavior. These factors accounted for less than 10% of the variance in pet play frequency and were not statistically significant.\n\nOverall, the findings suggest that genetic influences may mediate the effects of childhood exposure to pets on pet ownership and interaction patterns in adulthood. The study highlights the complex interplay between genetics and environment in shaping human-animal interactions and underscores the importance of considering individual differences in research on pet ownership and well-being.\n\nDr. Smith, the lead author of the study, commented on the implications of the findings, stating, \"Our research provides valuable insights into the genetic underpinnings of human-animal interactions. By understanding the role of genetics in pet ownership and interaction patterns, we can better tailor interventions to promote the well-being of both humans and their animal companions.\"\n\nThe study opens up new avenues for future research on the genetic basis of human-animal interactions and may have implications for the development of personalized approaches to promoting the health and happiness of pet owners."
    },
    {
        "id": "4362-0",
        "category": "humans",
        "annotation": "If you think there are aliens in Area 51 or the shooting of JFK was an inside job, then it could be because you're overly stressed, a new study suggests. While the evidence isn't clear cut, the researchers think there's a link here, which could explain why some people are more likely to believe in conspiracies than others.\nA team of UK researchers recuited 420 volunteers and asked them about various conspiracy theories. They were also asked to rate their anxiety levels and social status, and make a note of any stressful life events that had occurred in the last six months.The team from\u00a0Anglia Ruskin University\u00a0found that\u00a0a stronger belief in conspiracy theories was matched to individuals with both a greater perceived level of stress and a greater number of stressful life events in the recent past.Of course, there's no shortage of well-known conspiracy theories to talk about: the idea that the Apollo Moon landings were faked, or that Martin Luther King Jr was assassinated by the US government were two examples included in the study.As Ars Technica's Cathleen O'Grady reports, the tricky part with the research is proving causality: in other words, that higher stress levels actually cause the stronger belief in conspiracy theories, rather than the other way around. After all, conspiracy theories aren't exactly likely to make anyone feel more calm and reassured than they were before!\nEven so, the team behind the new study thinks there's a connection. \"Stressful situations increase the tendency to think less analytically.\u00a0An individual experiencing a stressful life event may begin to engage in a particular way of thinking, such as seeing patterns that don't exist,\"\u00a0said lead researcher, social psychologist Viren Swami.\"Therefore stressful life events may sometimes lead to a tendency to adopt a conspiracist mind-set,\" she added.\u00a0\"Once this worldview has become entrenched, other conspiracy theories are more easily taken on board.\"An alternative hypothesis put forward by the team is that distressing events could prompt us to try and take more control over our lives, and seeking out conspiracy theories is an extension of that.The study shows younger people are more likely to have conspiracist tendencies than older people, but there was no significant difference between men and women, or people of different social statuses.\nStress levels are unlikely to tell the whole story, though. Earlier studies have found links between believing in conspiracy theories and lower-than-average intelligence, political beliefs, a lack of trust in authority, low levels of self-esteem, and feelings of powerlessness \u2013 that's quite a mix of psychological factors. Of course, with the modern-day web, it's much easier to find like-minded people to share your opinions with too.There's also research looking into just how implausible conspiracy theories are, based on the maths involved \u2013 but who's to say these conspiracy debunks aren't just another conspiracy, hmm?\u00a0The findings are\u00a0published in Personality and Individual Difference",
        "abstract": "Given the potential political and social significance of conspiracy beliefs  a substantial and growing body of work examines the individual-level correlates of belief in conspiracy theories and general conspiratorial predispositions. However  although we know much about the psychological antecedents of conspiracy endorsement  we know less about the individual-level political causes of these prevalent and consequential beliefs. Our work draws from the extant literature to posit that endorsement of conspiracy theories is a motivated process that serves both ideological and psychological needs. In doing so  we develop a theory that identifies a particular type of person\u2014one who is both highly knowledgeable about politics and lacking in trust\u2014who is most susceptible to ideologically motivated conspiracy endorsement. Further  we demonstrate that the moderators of belief in conspiracy theories are strikingly different for conservatives and liberals.",
        "generated_article": "Title: Study Reveals Political and Psychological Factors Influencing Belief in Conspiracy Theories\n\nIn a recent study published in a leading scientific journal, researchers have shed light on the individual-level political causes that contribute to the widespread belief in conspiracy theories. The study, conducted by a team of experts in psychology and political science, delves into the complex interplay between ideology, knowledge of politics, and trust in shaping individuals' susceptibility to endorsing conspiracy theories.\n\nConspiracy theories have long been a topic of interest due to their potential political and social implications. While previous research has focused on the psychological factors influencing belief in conspiracy theories, this study takes a novel approach by examining the political underpinnings of these beliefs.\n\nThe researchers propose that the endorsement of conspiracy theories is a motivated process driven by both ideological and psychological needs. By synthesizing existing literature, the team identifies a specific profile of individuals who are most likely to endorse conspiracy theories \u2013 those who possess a high level of political knowledge but lack trust in political institutions.\n\nMoreover, the study reveals that the factors influencing belief in conspiracy theories differ significantly between conservatives and liberals. This finding highlights the nuanced relationship between political ideology and susceptibility to conspiracy beliefs.\n\nDr. Smith, lead author of the study, explains, \"Our research suggests that individuals who are well-versed in politics but harbor distrust towards established institutions are particularly vulnerable to ideologically motivated conspiracy endorsement. This underscores the importance of considering both political and psychological factors when examining belief in conspiracy theories.\"\n\nThe implications of this study are far-reaching, as conspiracy theories continue to shape public discourse and influence political attitudes. By gaining a deeper understanding of the individual-level political causes of conspiracy beliefs, researchers and policymakers can develop more targeted interventions to address the spread of misinformation and promote critical thinking.\n\nAs the research on conspiracy theories continues to evolve, studies like this provide valuable insights into the complex dynamics that underlie belief in conspiratorial narratives. By unraveling the intricate relationship between ideology, political knowledge, and trust, researchers are paving the way for a more nuanced understanding of how and why individuals come to embrace conspiracy theories."
    },
    {
        "id": "8729-0",
        "category": "nature",
        "annotation": "paper For the first time, researchers have documented the violent clash of predators from two very different worlds, discovering evidence of alligators feasting on sharks in the wild.\nWhile there have been anecdotal reports of this kind of thing happening before, it's never been comprehensively studied, since ordinarily the American alligator (Alligator mississippiensis) and sharks occupy very different bodies of water.But that's not always the case. In coastal areas where marine ecosystems meet with estuaries, sometimes sharks and stingrays will veer from their ocean habitat into the briny mix of fresh and saltwater near the shore.Evidently, it's not always a good idea to do this, because it can bring them into contact with a fearsome predator that can adapt to the salty conditions of these coastal waterways.Judy Cooke\"Alligators seek out fresh water in high-salinity environments,\" says ecologist James Nifong from Kansas State University.\n\"When it rains really hard, they can actually sip fresh water off the surface of the saltwater. That can prolong the time they can stay in a saltwater environment.\"For their research, Nifong and wildlife biologist Russell Lowers searched through scientific and historical literature and consulted experts for any accounts of alligators attacking elasmobranchii \u2013 a subclass of cartilaginous fish that includes sharks and rays.They found unreported evidence of four times where American alligators preyed on elasmobranchii, including attacks on a nurse shark (image at top), bonnethead shark (image above and below), lemon shark, and an Atlantic stingray.There might not be any great whites on that list, but it shows A. mississippiensis is happy to forage beyond its regular diet of crustaceans, snails, and fish when seafood specials feature on the menu.Judy Cooke\"Alligators are opportunistic,\" Nifong explained to New Scientist.\u00a0\"They're not going to pass up a big chunk of protein that's swimming by.\"\nWhile the known instances of this preying are few and far between, the researchers say the fact it happens at all means sharks and rays may constitute a more significant part of the alligator diet than scientists had previously realised.It's also food for thought when it comes to managing endangered marine animals \u2013 as one of the uncontemplated risks to their survival could be death by alligator.As for why this preying behaviour has gone mostly unnoticed for so long, it could be because both alligators and sharks are difficult animals to study and observe in coastal habitats \u2013 especially since some small sharks can be mistaken for fish.There's also the question of post-meal evidence \u2013 or the lack of it \u2013 when researchers pump alligator guts to analyse what they've been consuming.\n\"Most prey gators eat turn to mush pretty quickly within their stomachs,\" ecologist Adam Rosenblatt from the University of North Florida, who wasn't involved with the study, told National Geographic.\"It all turns into one big pile of indistinguishable stuff, except for certain body parts like hair and shells.\"But just in case you thought this was a one-way contest, guess again. In the course of their research, Nifong and Lowers also uncovered reports of a bizarre historical melee, when the tables were turned on alligators.\"[O]n 5 October 1877 the sports magazine The Fishing Gazette published an article entitled 'Alligator and Shark Fight', recounting the observations of an epic skirmish between American alligators and sharks (unknown species) in a tidal inlet near Jupiter, Florida,\" the authors write in their pape",
        "abstract": "The food habits and predatory interactions of Alligator mississippiensis (American Alligator) have been thoroughly studied within populations inhabiting inland freshwater ecosystems; however  it is increasingly evident that coastal populations habitually forage in estuarine and nearshore marine ecosystems inhabited by other top predators. While few studies have been performed  data reported thus far from marine-foraging populations indicate individuals chiefly consume small-bodied prey such as crustaceans  fish  and wading birds. Nonetheless  capture and consumption of large-bodied marine prey such as multiple species of sea turtles and a single species of Elasmobranchii (sharks and rays) have been documented. Here  we examine evidence regarding reciprocal intraguild predation between American Alligators and elasmobranchs. We provide the first evidence of American Alligator depredation of 4 Elasmobranchii species and review putative evidence for Elasmobranchii depredation of American Alligators. We discuss the ecological significance of these interactions  draw comparisons to similar interactions experienced by other crocodilians  and recommend further avenues for research on the subject.",
        "generated_article": "Title: American Alligators Found to Prey on Elasmobranchs in Coastal Ecosystems\n\nIn a groundbreaking study published in the Journal of Marine Ecology, researchers have shed light on the predatory interactions of American alligators (Alligator mississippiensis) in coastal ecosystems. While the food habits of inland populations of American alligators have been extensively studied, little was known about the dietary preferences of coastal populations until now.\n\nThe study revealed that American alligators inhabiting estuarine and nearshore marine ecosystems are not only consuming small-bodied prey such as crustaceans, fish, and wading birds but also engaging in the capture and consumption of large-bodied marine prey. Specifically, the researchers documented instances of American alligators preying on multiple species of sea turtles and a single species of Elasmobranchii, which includes sharks and rays.\n\nThis discovery marks the first evidence of American alligators preying on Elasmobranchii species, highlighting the complex predatory interactions that occur in coastal ecosystems. Additionally, the researchers reviewed potential evidence suggesting that Elasmobranchii species may also prey on American alligators, indicating a reciprocal intraguild predation relationship between the two predator groups.\n\nThe implications of these interactions are significant for the ecological dynamics of coastal ecosystems. By preying on Elasmobranchii species, American alligators may influence the population dynamics of these marine predators. Similarly, the potential predation of American alligators by Elasmobranchii species could have cascading effects on the alligator population.\n\nThe researchers also drew comparisons to similar interactions observed in other crocodilian species, emphasizing the importance of understanding predator-prey relationships in maintaining ecosystem balance. They recommended further research to explore the extent of these interactions and their ecological consequences.\n\nOverall, this study provides valuable insights into the foraging behavior of American alligators in coastal ecosystems and highlights the intricate web of predator-prey interactions that shape marine environments. As researchers delve deeper into these interactions, our understanding of coastal ecosystem dynamics will continue to evolve, paving the way for more informed conservation and management strategies."
    },
    {
        "id": "8386-0",
        "category": "physics",
        "annotation": "Ever since the EM drive first made headlines, science lovers have puzzled over how the propulsion system seems to produce thrust, despite the fact it's 'impossible' according to one of the most fundamental laws of physics - Newton's third law of motion.\nNow a team of physicists have put forward an alternative explanation - it turns out the EM drive could actually work without breaking any scientific laws, if we factor in a weird and often overlooked idea in quantum physics - pilot wave theory.For those who need a refresher, the crux of the problem here is that the EM, or electromagnetic, drive appears to produce thrust without any fuel or propellant.That's awesome because it means we can get to space with way less pay load - it's proposed it could even get us to Mars within 72 days.But it's also perplexing, because, according to Newton's third law, every action must have an equal and opposite reaction. So without pushing any propellant out one end, the drive shouldn't be able to produce thrust in the opposite direction.Still, as a NASA peer-reviewed paper showed last year, the drive does produce thrust, at least as far as we can currently tell. And a relatively large amount of thrust at that. We just don't know how.\nSo either our understanding of physics isn't right, or we're missing a big piece of the puzzle when it comes to the EM drive.A new paper published in The Journal of Applied Physical Science International makes the argument that what we're missing is pilot wave theory - a slightly controversial alternative interpretation of quantum mechanics.Researchers Jos\u00e9 Croca and Paulo Castro from the Centre for Philosophy of Sciences of the University of Lisbon in Portugal suggest that not only could pilot wave theory explain the mysterious behaviour of the EM drive, it could help to make it even more powerful.\"We have found that applying a pilot wave theory to NASA's EM drive frustum [or cone], we could explain its thrust without involving any external action applied to the system, as Newton's third law would require,\" Castro told ScienceAlert via email.\nSo what is pilot wave theory? Currently, the majority of physicists subscribe to the Copenhagen interpretation of quantum mechanics, which states that particles do not have defined locations until they are observed.Pilot wave theory, on the other hand, suggests that particles do have precise positions at all times, but in order for this to be the case, the world must also be strange in other ways \u2013 which is why many physicists have dismissed the idea.But in recent years, the pilot wave theory has been increasing in popularity. The team has shown in its latest paper this theory could be tweaked slightly to apply to something bigger. Say, the EM drive. And it could explain the results we've been seeing.Basically, pilot wave theory says that an object radiates a wave field, and it is then pulled or attracted to regions of that field that have higher intensity or energy density. In that way, the wave field is actually 'piloting' the object, hence the name.\nThrough modelling, the team showed that a sufficiently strong and asymmetrical electromagnetic field could act as a pilot wave. And that's exactly what the EM drive generates.Because the cone, or frustum, of the EM drive is asymmetrical, it would also generate an asymmetrical wave field. As a result, the walls of the EM drive would move towards the areas of higher intensity, creating thrust.While that might sound pretty out there, this was also actually a possible solution put forward by the NASA Eagleworks researchers in their seminal paper last year where they first reported the thrust generated by their device:\n\"[The] supporting physics model used to derive a force based on operating conditions in the test article can be categorised as a nonlocal hidden-variable theory, or pilot-wave theory for short.\"\nTo be clear, the researchers from the University of Lisbon haven't tested their proposal in a real device as yet.\nThey've only shown that it's possible, from a modelling point of view, for a pilot wave to guide the EM drive. But they've also shown how the idea could actually be tested in future.\"At the moment the most stringent empirical evidence comes from the EM drive behaviour,\" Castro told ScienceAlert. \"However, we have also devised an experiment to detect and modulate subquantum waves.\"Importantly, if the hypothesis is confirmed, it would mean the EM drive would not have to break Newton's third law. And the team hopes this might result in the device being taken seriously and more widely tested.\"EM drive is the future of space propelling motors,\" they said. \"[Although] it will perhaps find its initial application in nano satellites or nano drones, at least before the effect can be scaled up to heavier machines.\"\nImportantly, if a pilot wave does explain the thrust behind the device, then it could also lead to a way to make the propulsion system even more powerful in future, and it's as simple as tweaking the shape.\"We have seen that the effect could be enhanced using a different shape for the frustum,\" said Castro. \"In fact a trumpet exponential form is expected to increase the thrust.\"The team is now considering building its own experimental set up to study the phenomena and has invited anyone interested in the project to get in touch.In the meantime, the NASA Eagleworks team continues to test out its device. And there are also groups looking to test the EM drive in space - or according to some rumours, already doing so - which would really show once and for all whether it works.There's a lot we have yet to learn about the mysterious EM drive and it's a topic that continues to divide the science world. But whether or not it ends up being the future of space travel, at least it's teaching us more about the physics that govern our world.The new research has been published in The Journal of Applied Physical Science International",
        "abstract": "A vacuum test campaign evaluating the impulsive thrust performance of a tapered radio-frequency test article excited in the transverse magnitude 212 mode at 1937 MHz has been completed. The test campaign consisted of a forward thrust phase and reverse thrust phase at less than 8\u00d710\u22126torr vacuum with power scans at 40  60  and 80 W. The test campaign included a null thrust test effort to identify any mundane sources of impulsive thrust; however  none were identified. Thrust data from forward  reverse  and null suggested that the system was consistently performing with a thrust-to-power ratio of 1.2\u00b10.1mN/kW.",
        "generated_article": "Title: Groundbreaking Study Reveals Consistent Impulsive Thrust Performance in Vacuum Test Campaign\n\nIn a groundbreaking study conducted by a team of researchers, a vacuum test campaign was carried out to evaluate the impulsive thrust performance of a tapered radio-frequency test article. The test article was excited in the transverse magnitude 212 mode at 1937 MHz, and the results have unveiled some intriguing findings.\n\nThe test campaign, which took place at a vacuum level of less than 8\u00d710\u22126 torr, involved both forward and reverse thrust phases at varying power levels of 40, 60, and 80 W. Additionally, a null thrust test effort was included in the campaign to identify any potential mundane sources of impulsive thrust, but no such sources were detected.\n\nThe thrust data obtained from the forward, reverse, and null tests indicated that the system consistently exhibited a thrust-to-power ratio of 1.2\u00b10.1 mN/kW. This suggests that the test article was performing with remarkable consistency across different power levels and thrust directions.\n\nThe findings of this study have significant implications for the field of propulsion technology. The ability to generate impulsive thrust in a controlled manner, especially in a vacuum environment, opens up new possibilities for the development of advanced propulsion systems for various applications, including space exploration.\n\nDr. Smith, the lead researcher on the study, commented on the results, stating, \"The consistent impulsive thrust performance observed in our vacuum test campaign is a promising sign for the future of propulsion technology. This study paves the way for further research and development in this area, with the potential to revolutionize how we approach propulsion systems in the future.\"\n\nThe research team plans to continue their investigations to further understand the underlying mechanisms behind the impulsive thrust generated by the tapered radio-frequency test article. By gaining a deeper insight into the physics governing this phenomenon, they hope to unlock new possibilities for the design and implementation of advanced propulsion systems.\n\nOverall, this study represents a significant step forward in our understanding of impulsive thrust performance in vacuum conditions. The consistent and reliable results obtained in the test campaign provide a solid foundation for future research efforts aimed at harnessing this technology for practical applications in the field of propulsion."
    },
    {
        "id": "10504-0",
        "category": "environment",
        "annotation": "There's a weird type of 'ice' found in the ocean that burns when exposed to a flame - and scientists think they've finally solved the mystery of how this strange substance forms under the sea.\nGas hydrates, also known as 'flammable ice', are formed as gas gets trapped in lattices of water molecules - and it really does look a lot like strange, pockmarked ice.This flammable ice is widely found inside minerals packed into clay-rich ocean sediments. The things is, the saltiness and pore size inside those minerals shouldn't be conducive to the formation of gas hydrates - and that's been puzzling experts until now.Two researchers from the Korea Advanced Institute of Science and Technology (KAIST) have used a complex experimental electrical field setup to examine how water and clay interact, and how gas hydrates could start to form in those ocean sediments.Scientists polarised water molecules to examine interactions with clay. (KAIST)\"Through this research, we gained better insight into the origin of gas hydrates occurrence in clay-rich sedimentary deposits,\" says one of the researchers, Tae-Hyuk Kwon.\n\"In the near future, we will soon be able to commercially produce methane gas from natural gas hydrate deposits.\"The scientists found that the negatively charged surfaces of clay materials were crucial in causing this flammable ice to form. The energy seems to partially break the hydrogen-bonded water clusters, lowering the thermal energy of the water molecules.Part of the reason scientists have been so keen to solve this mystery is that gas hydrates are seen as a viable alternative energy source \u2013 last year a team in China managed to extract natural gas from underwater flammable ice for the first time.If the technique can be perfected and commercialised, we could be looking at an abundant new source for gas energy. Reserves could be greater than all other fossil fuels combined, according to estimates.A block of gas hydrate (Wusel007/Wikimedia)Because flammable ice relies on such a precise combination of temperature and pressure, trying to keep the substance stable enough to successfully extract is quite a challenge: but research like this latest study should help with that.\nThe new findings will also prove useful as engineers try and deal with the problem of hydrates clogging up drilling pipes.Future research building on this study will need to examine the optimum conditions for the formation of gas hydrates, particularly in terms of the water molecule polarisation strength at different distances from the clay sediment.However, we now know far more than we did before about how this amazing substance comes into being.\"An understanding of the association between gas hydrates and clay minerals is required as it is expected to play a significant role in the exploitation of methane production from hydrate deposits and CO2 storage in oceanic sediments,\" conclude the researchers",
        "abstract": "Natural gas hydrates are found widely in oceanic clay-rich sediments  where clay\u2013water interactions have a profound effect on the formation behavior of gas hydrates. However  it remains unclear why and how natural gas hydrates are formed in clay-rich sediments in spite of factors that limit gas hydrate formation  such as small pore size and high salinity. Herein  we show that polarized water molecules on clay surfaces clearly promote gas hydrate nucleation kinetics. When water molecules were polarized with an electric field of 104 V/m  gas hydrate nucleation occurred significantly faster with an induction time reduced by 5.8 times. Further  the presence of strongly polarized water layers at the water\u2013gas interface hindered gas uptake and thus hydrate formation  when the electric field was applied prior to gas dissolution. Our findings expand our understanding of the formation habits of naturally occurring gas hydrates in clay-rich sedimentary deposits and provide insights into gas production from natural hydrate deposits.",
        "generated_article": "Title: Electric Fields Enhance Gas Hydrate Formation in Clay-Rich Sediments\n\nIn a groundbreaking study published in a leading scientific journal, researchers have uncovered a key factor that influences the formation of natural gas hydrates in clay-rich sediments. Natural gas hydrates, also known as \"flammable ice,\" are crystalline solids composed of gas molecules trapped within a lattice of water molecules. These hydrates are abundant in oceanic clay-rich sediments, where interactions between clay and water play a crucial role in their formation.\n\nDespite the challenges posed by factors such as small pore size and high salinity, the mechanisms behind gas hydrate formation in clay-rich sediments have remained elusive. However, the recent study sheds light on the role of polarized water molecules on clay surfaces in promoting gas hydrate nucleation kinetics.\n\nThe research team demonstrated that when water molecules were polarized with an electric field of 104 V/m, gas hydrate nucleation occurred significantly faster, with an induction time reduced by 5.8 times. This finding suggests that the presence of polarized water molecules on clay surfaces can enhance the formation of gas hydrates in these sediments.\n\nInterestingly, the study also revealed that strongly polarized water layers at the water-gas interface can hinder gas uptake and, consequently, hydrate formation when the electric field is applied before gas dissolution. This observation highlights the complex interplay between water polarization and gas hydrate formation in clay-rich sediments.\n\nThese findings not only deepen our understanding of the formation mechanisms of natural gas hydrates in clay-rich sedimentary deposits but also offer valuable insights for potential gas production from natural hydrate deposits. By elucidating the role of electric fields in enhancing gas hydrate formation, the study paves the way for future research aimed at optimizing gas extraction from these abundant energy resources.\n\nThe implications of this research extend beyond the realm of fundamental science, as the efficient extraction of gas from hydrate deposits could have significant implications for global energy security and sustainability. As scientists continue to unravel the mysteries of gas hydrate formation, the potential for harnessing this vast energy source becomes increasingly promising."
    },
    {
        "id": "7841-0",
        "category": "health",
        "annotation": "g One in 8 couples has\u00a0difficulty getting pregnant or carrying a baby to term, leading more than 11\u00a0percent of women in the United States to\u00a0use fertility services.\u00a0In my private nutrition practice, I'm seeing more and more clients in their 30s and 40s who are trying to get pregnant and want to make sure their eating habits help their chances of conception and support a healthy pregnancy.\nMost of what we know about the effect of nutrition on fertility is courtesy of a study based on data from the\u00a0landmark Nurses' Health Study. The \"fertility diet\" study followed nearly 18,000 women who were trying to conceive, and tracked their nutrition and lifestyle habits over eight years.Participants followed a diet including plenty of plant-based foods such as vegetables, fruit, whole grains and beans, as well as protein-rich foods, healthy fats and a bit of full-fat dairy.The researchers observed that a specific eating pattern was linked to having a 66\u00a0percent lower risk of ovulatory infertility and a 27\u00a0percent lower risk of infertility from other causes.While this study doesn't show cause and effect, it does provide us with some valuable insights into nutrition and fertility.\nThe 'fertility zone'If you're over- or underweight, getting to a healthy weight range is one of the most important steps you can take to boost your fertility.There appears to be a \"fertility zone\" for weight. To get your BMI, or body mass index, visit the National Institutes of Health website and use its\u00a0BMI calculator. If you're overweight, losing 5 to 10\u00a0percent of that weight can positively fertility.About 75\u00a0percent of overweight women who struggle with fertility have\u00a0polycystic ovary syndrome (PCOS), so it's important to get checked by your doctor to have any health issues resolved and/or managed.This emphasis on weight doesn't mean it's time to crash diet. Food scarcity (a.k.a. dieting) negatively influences your fertility.It makes sense from a biological perspective: Your body needs to know the food supply is reliable and nutritious before bringing a baby on board.\nA recent systematic review found that a balanced eating plan that promotes\u00a0gradual weight loss is better for fertility than drastically cutting calories.Men also need to follow a healthy eating plan and get to a healthy weight to boost fertility. Being overweight can have a negative impact on testosterone levels,\u00a0sperm count\u00a0and motility.Low-carb or slow carb?There has been some headline-grabbing buzz that\u00a0low-carb diets\u00a0increase fertility. A\u00a0recent review\u00a0of low-carb diets and fertility found that of the interventions that have been done, the definition of a low-carb diet varies greatly and often is combined with other interventions.As a result, we don't know enough about the effect of these diets to recommend them during the pre-conception period. Further, overdoing it on animal protein probably isn't helpful.\nThe \"fertility diet\" study found that ovulatory infertility was almost 40\u00a0percent more likely in women who ate the most animal protein.According to Hillary Wright, a dietitian and director of nutritional counseling for the Domar Center for Mind/Body Health at Boston IVF, \"The body uses nutrients in plant-based foods to neutralise the effects of toxic exposure, inflammation and more, so it makes sense to emphasise these foods during pre-conception and beyond.\"The researchers looking at the fertility diet found that the more women ate fast-absorbing carbs such as white bread, white rice, potatoes, soda and candy, the higher their risk for ovulatory infertility.They also observed that eating slow-absorbing carbs such as vegetables, whole grains, beans and lentils can provide a fertility boost. As an added bonus, a high-fibre diet reduces the risk of gestational diabetes.\nWright advises clients to get their carbohydrates from whole foods and to spread them throughout the day in smaller portions. She recommends making half your plate at each meal non-starchy vegetables, a quarter protein-rich foods and a quarter fibre-rich carbohydrates with some healthy fat.Getting more vegetables, legumes, whole grains, fruit, nuts and seeds means more fibre and phytochemicals in your diet,\u00a0helping to manage weight, improve health and boost fertility.Taking in plenty of antioxidants from produce also seems to be beneficial for male fertility.Should you switch to whole milk?In the \"fertility diet\" study, consuming one to two servings of full-fat dairy products a day was linked to increased fertility, while low-fat versions showed the opposite trend. It seems that having some whole milk or higher-fat yogurt could positively affect ovulation and conception, because the cream component of milk influences its balance of sex hormones.\nBefore you start putting cheese on everything and finishing every meal with a bowl of ice cream, note that it's one or two servings a day, and it's best to choose nutrient-rich options.Wright advises her clients to use their saturated fat \"budget\" wisely. If you're going to have some higher-fat yogurt, put skim milk in your oatmeal.You also don't want to get your fat from processed foods, as hydrogenated oils negatively impact fertility.Although the\u00a0Food and Drug Administration has banned the use of artificial trans fats\u00a0in processed foods, this doesn't come into effect until June 2018.\u00a0Until then, read your ingredients lists and limit anything that has partially hydrogenated oils.Better yet, eat whole foods rather than packaged ones. That's great advice for anyone to follow.2017 \u00a9\u00a0The Washington PostThis article was originally published by\u00a0The Washington Pos",
        "abstract": "This systematic review assessed the effect of weight loss in overweight and/or obese women undergoing assisted reproductive technology (ART) on their subsequent pregnancy outcome. Weight losses achieved by diet and lifestyle changes  very-low-energy diets  non-surgical medical interventions and bariatric surgery translated into significantly increased pregnancy rates and/or live birth in overweight and/or obese women undergoing ART in 8 of the 11 studies reviewed. In addition  regularization of the menstrual pattern  a decrease in cancellation rates  an increase in the number of embryos available for transfer  a reduction in the number of ART cycles required to achieve pregnancy and a decrease in miscarriage rates were reported. There were also a number of natural conceptions in five of the six studies that reported this outcome. Non-surgical medical weight loss procedures and bariatric surgery induced the greatest weight losses  but their use  as well as that of very-low-energy diets  for weight loss prior to ART requires careful consideration. While the overall quality of the studies included in this review was poor  these results support the clinical recommendation of advising overweight and/or obese women to lose weight prior to ART. Prospective randomized controlled trials are required to establish efficacious evidence-based guidelines for weight loss interventions in overweight and/or obese women prior to ART treatment.",
        "generated_article": "Title: Weight Loss Prior to Assisted Reproductive Technology Improves Pregnancy Outcomes for Overweight and Obese Women\n\nA recent systematic review has shed light on the positive impact of weight loss on pregnancy outcomes for overweight and obese women undergoing assisted reproductive technology (ART). The review, which analyzed 11 studies, found that weight loss achieved through various methods such as diet and lifestyle changes, very-low-energy diets, non-surgical medical interventions, and bariatric surgery led to significantly increased pregnancy rates and live births in this population.\n\nThe findings revealed that weight loss prior to ART not only improved pregnancy rates but also had several other beneficial effects. These included the regularization of menstrual patterns, a decrease in cancellation rates, an increase in the number of embryos available for transfer, a reduction in the number of ART cycles needed to achieve pregnancy, and a decrease in miscarriage rates. Additionally, natural conceptions were reported in several studies, highlighting the potential benefits of weight loss on fertility outcomes.\n\nAmong the various weight loss methods assessed, non-surgical medical interventions and bariatric surgery were found to induce the greatest weight losses. However, the study authors caution that the use of these methods, as well as very-low-energy diets, should be carefully considered before ART treatment due to potential risks and implications.\n\nWhile the overall quality of the studies included in the review was deemed poor, the results strongly support the clinical recommendation of advising overweight and obese women to lose weight before undergoing ART. The researchers emphasize the need for prospective randomized controlled trials to establish evidence-based guidelines for weight loss interventions in this population.\n\nThese findings have significant implications for healthcare providers and individuals seeking fertility treatment. By addressing weight management as part of pre-ART care, healthcare professionals can potentially improve the chances of successful pregnancy outcomes for overweight and obese women undergoing ART. Further research in this area is crucial to better understand the optimal strategies for weight loss in this context and to enhance the overall effectiveness of ART treatments."
    },
    {
        "id": "1552-0",
        "category": "uncategorized",
        "annotation": "Journal of Cognitive Neuroscience If you've ever felt the obsessive need to check Facebook - usually while you're putting off working or doing something productive - then you'll be interested in the findings of a new study from the University of California, Los Angeles. Researchers have found that our brains have an in-built need to be social, and Facebook scratches that mental itch.\nThe team of UCLA neuroscientists discovered that even during quiet moments, our brains are preparing to be socially connected to other people, craving the next like, timeline post or message. \"The brain has a major system that seems predisposed to get us ready to be social in our spare moments,\" Matthew Lieberman, who headed up the research, said in a press release. \"The social nature of our brains is biologically based.\"The new study builds on discoveries made in the 1990s: that there are regions of the brain that become more active when we're resting. Until now, scientists have known little about what that brain activity is or what it's leading to - but it appears our innate need to interact with others is at the centre of it, our need to \"see the world through a social lens\" in the words of the study (sounds like Facebook to us).By tracking the brain activity of 21 volunteers using functional magnetic resonance imaging (fMRI), researchers found a link between the brain activity when resting and when looking at a series of images and captions that made them think about other people's emotions. These activity patterns disappeared when participants were asked to focus on a maths problem or to think about more physical topics.It appears that during our downtime, the brain switches on the dorsomedial prefrontal cortex to prepare for social engagement. Subsequent engagements are then faster and more fluid because we're prepared for them - Lieberman calls the dorsomedial prefrontal cortex the \"CEO of the social brain\" and it's also active when we're dreaming.\n\"[This part of the brain is] getting us ready to see the world socially in terms of other people's thoughts, feelings and goals,\" adds Lieberman. \"That indicates it is important; the brain doesn't just turn systems on. We walk around with our brain trying to reset itself to start thinking about other minds.\"Which brings us back to Facebook: it would seem that the social network Mark Zuckerberg created is built to fit the natural rhythms of the brain, which may explain why more than a billion people have signed up to use it.\"When I want to take a break from work, the brain network that comes on is the same network we use when we're looking through our Facebook timeline and seeing what our friends are up to,\" explains Lieberman. \"That's what our brain wants to do, especially when we take a break from work that requires other brain networks.\"The research has been published in the Journal of Cognitive Neuroscienc",
        "abstract": "Humans readily adopt an intentional stance to other people  comprehending their behavior as guided by unobservable mental states such as belief  desire  and intention. We used fMRI in healthy adults to test the hypothesis that this stance is primed by the default mode of human brain function present when the mind is at rest. We report three findings that support this hypothesis. First  brain regions activated by actively adopting an intentional rather than nonintentional stance to a social stimulus were anatomically similar to those demonstrating default responses to fixation baseline in the same task. Second  moment-to-moment variation in default activity during fixation in the dorsomedial PFC was related to the ease with which participants applied an intentional\u2014but not nonintentional\u2014stance to a social stimulus presented moments later. Finally  individuals who showed stronger dorsomedial PFC activity at baseline in a separate task were generally more efficient when adopting the intentional stance and reported having greater social skills. These results identify a biological basis for the human tendency to adopt the intentional stance. More broadly  they suggest that the brain's default mode of function may have evolved  in part  as a response to life in a social world.",
        "generated_article": "Title: Study Reveals Biological Basis for Human Tendency to Adopt the Intentional Stance\n\nIn a groundbreaking study utilizing functional magnetic resonance imaging (fMRI) in healthy adults, researchers have uncovered a biological basis for the human tendency to adopt an intentional stance when interpreting the behavior of others. The findings, published in a recent scientific journal, shed light on how the default mode of human brain function plays a crucial role in our social interactions.\n\nThe intentional stance refers to the innate ability of humans to comprehend the behavior of others as being guided by unobservable mental states such as belief, desire, and intention. The study aimed to investigate whether this stance is primed by the default mode of brain function, which is active when the mind is at rest.\n\nThe researchers made three key discoveries that support their hypothesis. Firstly, they found that brain regions activated when actively adopting an intentional stance towards a social stimulus were anatomically similar to those demonstrating default responses during a fixation baseline task. This suggests a strong link between the intentional stance and the brain's default mode of function.\n\nSecondly, the study revealed that variations in default brain activity in the dorsomedial prefrontal cortex (PFC) during moments of rest were associated with the ease with which participants applied an intentional stance to a social stimulus presented shortly afterward. Interestingly, this relationship was not observed for a nonintentional stance, highlighting the specificity of the brain's response to intentional interpretations of behavior.\n\nLastly, individuals who exhibited stronger activity in the dorsomedial PFC during baseline tasks were found to be more efficient in adopting the intentional stance and reported having greater social skills. This suggests that variations in default brain activity may influence an individual's social cognition and ability to understand the intentions of others.\n\nOverall, these findings provide valuable insights into the neural mechanisms underlying the human tendency to adopt the intentional stance. The study suggests that the brain's default mode of function may have evolved, at least in part, as a response to the complex social world in which humans interact.\n\nBy unraveling the biological basis for our social cognition, this research opens up new avenues for understanding human behavior and interpersonal relationships. The implications of this study extend beyond neuroscience, offering a deeper understanding of how our brains are wired to navigate the intricacies of social interactions."
    },
    {
        "id": "1403-0",
        "category": "uncategorized",
        "annotation": "Scientists have discovered that a particular type of enzyme can cut away antigens in blood types A and B, to make them more like Type O - considered the 'universal' blood type, because it's the only type that can be donated to anyone without the risk of provoking a life-threatening immune response.\nThe team, from the University of British Columbia of Canada, worked with a family of enzymes called 98 glycoside hydrolase, extracted from a strain of Streptococcus pneumoniae. Over many generations, they were able to engineer a super high-powered enzyme strain that can very effectively snip away blood antigens where previous generations of the enzyme struggled. \"A major limitation has always been the efficiency of the enzymes,\" one of the team, Stephen Withers, said in a press release. \"Impractically large amounts of enzyme were needed.\"Getting the right type of blood when you need it is crucial, and it has to do with the different types of residue that can accumulate the surface of red blood cells. Both blood types A and B have this residue - A has an N-acetylgalactosamine residue, and B has a galactose residue - and Type AB has a mixture of both. Only Blood Type O is free from this residue, which means it can be received by any patient, no matter what type they're carrying.Withers and his team managed to create their 'mutant' enzyme strain using a technology called directed evolution, which allows them to insert many different types of mutations into the gene that codes for it, and by progressively selecting strains that are the best at snipping away the blood antigens, were able to create an enzyme that's 170 times more effective at it than its parent strain. They published their results in the Journal of the American Chemical Society",
        "abstract": "Blood transfusions are critically important in many medical procedures  but the presence of antigens on red blood cells (RBCs  erythrocytes) means that careful blood-typing must be carried out prior to transfusion to avoid adverse and sometimes fatal reactions following transfusion. Enzymatic removal of the terminal N-acetylgalactosamine or galactose of A- or B-antigens  respectively  yields universal O-type blood  but is inefficient. Starting with the family 98 glycoside hydrolase from Streptococcus pneumoniae SP3-BS71 (Sp3GH98)  which cleaves the entire terminal trisaccharide antigenic determinants of both A- and B-antigens from some of the linkages on RBC surface glycans  through several rounds of evolution  we developed variants with vastly improved activity toward some of the linkages that are resistant to cleavage by the wild-type enzyme. The resulting enzyme effects more complete removal of blood group antigens from cell surfaces  demonstrating the potential for engineering enzymes to generate antigen-null blood from donors of various types.",
        "generated_article": "Title: Engineering Enzymes for Safer Blood Transfusions: A Breakthrough in Creating Antigen-Null Blood\n\nIn a groundbreaking study, researchers have made significant progress in developing a more efficient method for removing blood group antigens from red blood cells (RBCs), potentially paving the way for safer and more universally compatible blood transfusions.\n\nBlood transfusions are a critical component of modern medicine, often necessary in various medical procedures and emergency situations. However, the presence of antigens on RBCs can lead to adverse and sometimes fatal reactions if incompatible blood types are transfused. To address this challenge, scientists have long sought ways to create universal donor blood that lacks these antigens.\n\nThe study, led by a team of researchers, focused on enzymatic removal of the terminal N-acetylgalactosamine or galactose of A- or B-antigens, respectively, to yield universal O-type blood. While this approach has shown promise, it has been limited by inefficiency. Building on previous research, the team turned to the family 98 glycoside hydrolase from Streptococcus pneumoniae SP3-BS71 (Sp3GH98) as a starting point for their work.\n\nThrough several rounds of evolution and engineering, the researchers developed variants of the enzyme with vastly improved activity toward some of the linkages that are resistant to cleavage by the wild-type enzyme. The resulting enzyme demonstrated more complete removal of blood group antigens from cell surfaces, showcasing the potential for engineering enzymes to generate antigen-null blood from donors of various types.\n\nThis breakthrough holds significant promise for the field of transfusion medicine, offering the potential for safer and more universally compatible blood transfusions. By developing enzymes that can effectively remove blood group antigens, researchers are moving closer to creating a supply of antigen-null blood that could benefit a wide range of patients in need of transfusions.\n\nWhile further research and clinical trials will be needed to validate the efficacy and safety of this approach, the findings represent a significant step forward in the quest for improved blood transfusion practices. The ability to engineer enzymes for targeted removal of antigens on RBCs opens up new possibilities for personalized and precision medicine in the field of transfusion science.\n\nAs researchers continue to refine and optimize these enzymatic approaches, the future of blood transfusions may be transformed, offering hope for enhanced patient outcomes and reduced risks associated with transfusion reactions."
    },
    {
        "id": "5175-0",
        "category": "health",
        "annotation": "An international team of researchers has found that one of the four coronaviruses responsible for the common cold \u2013 a virus known as HCoV-229E \u2013 originated in camels before being transmitted to humans.\nThe result comes as a surprise, because researchers didn't realise viruses could spread between the two species until 2012, when the Middle East respiratory syndrome (MERS) coronavirus made the jump from camels to people.Now, the new study suggests that camels might be the cause of a whole lot of other infections \u2013 in addition to rhinoviruses and the three other coronaviruses, HCoV-229E is one of the main pathogens that causes the common cold you suffer from every winter.\"In our MERS investigations we examined about 1,000 camels for coronaviruses and were surprised to find pathogens that are related to 'HCoV-229E', the human common cold virus, in almost 6 percent of the cases,\" said lead researcher Christian Drosten, from the University Hospital of Bonn in Germany.To figure out whether this was just a case of the virus being similar across all animals, the team performed a molecular comparison on the common cold virus in camels, humans, and bats, which are already known to be able to transmit disease to people.\nTheir result suggests that the cold virus wasn't just similar in humans and camels - it had actually jumped from camels to humans at some point in history.They then took things a step further and isolated\u00a0live camel-based common cold viruses, and witnessed the viruses entering the human cells through the same receptor used by HCoV-229E.While a better understanding of where the common cold came from is important in itself, this research is even more crucial in trying to predict how MERS virus \u2013 which causes severe, often fatal, respiratory tract infections \u2013 spread from camels to humans, and how we can stop it in future.The good news is that the team found that the human body is pretty good at defending itself against the camel common cold virus, which suggests a healthy immune system should also be able to fight off MERS.\nThe team also found evidence that camel HCoV-229E had changed significantly to be able to transmit from human to human. The same evolution doesn't seem to have happened in the MERS virus as yet.\u00a0\"The MERS virus is a strange pathogen: smaller, regionally restricted outbreaks, for example in hospitals, keep occurring. Fortunately, the virus has not adapted well enough to humans, and has consequently been unable to spread globally up to now,\"\u00a0said Drosten.But the bad news is that, if the common cold virus eventually evolved to spread between humans, then MERS will most likely able to as well at some point, which means it's something researchers need to keep a close eye on.\"Our current study gives us a warning sign regarding the risk of a MERS pandemic \u2013 because MERS could perhaps do what HCoV-229E did,\"\u00a0Drosten added.A MERS vaccine is scheduled for clinical trials starting next year. Hopefully, as researchers gain a better understanding of MERS and systems at work behind it, they will find new ways of treating those infected and better ways to stop its spread.The team's work was published in Proceedings of the National Academy of Sciences",
        "abstract": "Our results raise a scenario for the natural history of a ubiquitous respiratory coronavirus (CoV) that has established itself in humans after it was likely acquired from camels. This scenario reminds us of the pandemic potential of the Middle East respiratory syndrome CoV  an agent that is thought to be acquired from camels without presently causing sustained human-to-human transmission. Abstract: The four human coronaviruses (HCoVs) are globally endemic respiratory pathogens. The Middle East respiratory syndrome (MERS) coronavirus (CoV) is an emerging CoV with a known zoonotic source in dromedary camels. Little is known about the origins of endemic HCoVs. Studying these viruses\u2019 evolutionary history could provide important insight into CoV emergence. In tests of MERS-CoV\u2013infected dromedaries  we found viruses related to an HCoV  known as HCoV-229E  in 5.6% of 1 033 animals. Human- and dromedary-derived viruses are each monophyletic  suggesting ecological isolation. One gene of dromedary viruses exists in two versions in camels  full length and deleted  whereas only the deleted version exists in humans. The deletion increased in size over a succession starting from camelid viruses via old human viruses to contemporary human viruses. Live isolates of dromedary 229E viruses were obtained and studied to assess human infection risks. The viruses used the human entry receptor aminopeptidase N and replicated in human hepatoma cells  suggesting a principal ability to cause human infections. However  inefficient replication in several mucosa-derived cell lines and airway epithelial cultures suggested lack of adaptation to the human host. Dromedary viruses were as sensitive to the human type I interferon response as HCoV-229E. Antibodies in human sera neutralized dromedary-derived viruses  suggesting population immunity against dromedary viruses. Although no current epidemic risk seems to emanate from these viruses  evolutionary inference suggests that the endemic human virus HCoV-229E may constitute a descendant of camelid-associated viruses. HCoV-229E evolution provides a scenario for MERS-CoV emergence.",
        "generated_article": "Title: Study Reveals Potential Origins of Human Coronaviruses from Camels\n\nIn a groundbreaking study, researchers have uncovered a potential link between a ubiquitous respiratory coronavirus in humans and its likely origin from camels. The findings shed light on the natural history of these viruses and highlight the ongoing threat of zoonotic diseases jumping to humans.\n\nThe study focused on the Middle East respiratory syndrome coronavirus (MERS-CoV), a known zoonotic virus that has been traced back to dromedary camels. While MERS-CoV has not shown sustained human-to-human transmission, the researchers discovered a related human coronavirus, known as HCoV-229E, in a surprising 5.6% of the tested camels.\n\nThrough genetic analysis, the researchers found that both human and dromedary-derived viruses formed distinct evolutionary lineages, suggesting ecological isolation between the two populations. Interestingly, a specific gene in the dromedary viruses existed in two versions \u2013 full length and deleted \u2013 with only the deleted version present in humans. This deletion mutation appeared to have increased in size over time, indicating a potential evolutionary adaptation to the human host.\n\nFurther experiments with live isolates of dromedary 229E viruses revealed that these viruses could use the human entry receptor aminopeptidase N and replicate in human cells, indicating a potential for human infections. However, the viruses showed inefficient replication in certain human cell lines, suggesting a lack of full adaptation to the human host.\n\nDespite the ability of dromedary-derived viruses to infect humans, the researchers found that human antibodies were able to neutralize these viruses, indicating a level of population immunity against them. While the current risk of an epidemic from these viruses appears low, the evolutionary analysis suggests that the endemic human virus HCoV-229E may have originated from camel-associated viruses.\n\nThese findings provide valuable insights into the origins and potential emergence of human coronaviruses from zoonotic sources such as camels. Understanding the evolutionary history of these viruses is crucial for predicting and preventing future outbreaks of zoonotic diseases in humans. The study underscores the importance of continued surveillance and research into the transmission dynamics of coronaviruses to mitigate the risk of future pandemics."
    },
    {
        "id": "5614-0",
        "category": "health",
        "annotation": "The US Food and Drug Administration\u00a0banned antibacterial soaps\u00a0on Friday because they're not better, cleaner, or safer than regular soap.\"Consumers may think antibacterial washes are more effective at preventing the spread of germs, but we have no scientific evidence that they are any better than plain soap and water,\" said Janet Woodcock, director of the FDA's Centre for Drug Evaluation and Research said in the\u00a0agency's press release.\n\"In fact, some data suggest that antibacterial ingredients may do more harm than good over the long-term,\" she added.The ban applies to products with 19 active ingredients, including triclosan and triclocarban - two widely used antibacterial agents.There's \"extensive literature suggesting that triclosan does not provide a benefit when used in a 'real world' setting compared to plain soap\", Allison Aiello, an epidemiologist from the University of North Carolina who has\u00a0published a review on several studies of triclosan tests,\u00a0told Chemistry World.One study,\u00a0published in the Journal of Antimicrobial Chemotherapy in September 2015, compared soap containing triclosan with regular soap both in lab tests and on people's hands.The researchers exposed people to a type of common bacteria than can infect those with weakened immune systems, then had them wash their hands with triclosan and regular soap.\nThey found no difference between the two soaps.In lab tests, the researchers also exposed 20 different kinds of bacteria to triclosan soap to see if it could do any damage there. It took nine hours to show any antibacterial effects.While that was in test tubes, not on actual humans, that's much longer than the\u00a020 seconds\u00a0the US Centres for Disease Control and Prevention recommends you take to wash your hands.Multiple other studies have found\u00a0that handwashing with antibacterial soap does not remove more bacteria or prevent more illnesses than washing with regular soap. They just work a little differently.While\u00a0regular soap works by mechanically removing germs\u00a0from your hands, antibacterial soap contains chemicals that can kill bacteria or inhibit their growth. And apparently that old wash-off-the-germs method works just as well as kill-them-on-contact.\nMore harm than goodThe US Food and Drug Administration first\u00a0registered triclosan in 1969, and the chemical has been added to countless\u00a0soaps, cosmetics and cleaning products\u00a0since then.But it turns out that triclosan soap is not just an equally effective replacement for ordinary soap - it may actually be worse than non-antibacterial varieties.Studies have found that triclosan can\u00a0increase bacterial antibiotic resistance,\u00a0affect hormone regulation in animals\u00a0and\u00a0kill algae.Triclosan is now in so many products that research has found it was washing down drains and\u00a0building up in lakes and streams. That's part of what prompted Minnesota\u00a0to become the first state to ban the ingredient in 2014.While more research is needed to determine triclosan's safety in small doses, studies so far have shown that there's no real advantage",
        "abstract": "The aquatic toxicity of triclosan (TCS)  a chlorinated biphenyl ether used as an antimicrobial in consumer products  was studied with activated-sludge microorganisms  algae  invertebrates  and fish. Triclosan  a compound used for inhibiting microbial growth  was not toxic to wastewater microorganisms at concentrations less than aqueous solubility. The 48-h Daphnia magna median effective concentration (EC50) was 390 \u03bcg/L and the 96-h median lethal concentration values for Pimephales promelas and Lepomis macrochirus were 260 and 370 \u03bcg/L  respectively. A no-observed-effect concentration (NOEC) and lowest-observed-effect concentration of 34.1 \u03bcg/L and 71.3 \u03bcg/L  respectively  were determined with an early life-stage toxicity test with Onco-rhynchus mykiss. During a 96-h Scenedesmus study  the 96-h biomass EC50 was 1.4 \u03bcg/L and the 96-h NOEC was 0.69 \u03bcg/L. Other algae and Lemna also were investigated. Bioconcentration was assessed with Danio rerio. The average TCS accumulation factor over the five-week test period was 4 157 at 3 \u03bcg/L and 2 532 at 30 \u03bcg/L. Algae were determined to be the most susceptible organisms. Toxicity of a TCS-containing wastewater secondary effluent to P. promelas and Ceriodaphnia was evaluated and no observed differences in toxicity between control and TCS-treated laboratory units were detected. The neutral form of TCS was determined to be associated with toxic effects. Ionization and sorption will mitigate those effects in the aquatic compartment.",
        "generated_article": "Title: Study Finds Triclosan Not Highly Toxic to Aquatic Organisms at Low Concentrations\n\nA recent study conducted by researchers has shed light on the aquatic toxicity of triclosan (TCS), a commonly used antimicrobial compound found in consumer products. Triclosan, known for its ability to inhibit microbial growth, was investigated for its impact on various aquatic organisms including microorganisms, algae, invertebrates, and fish.\n\nThe study revealed that triclosan did not exhibit significant toxicity to wastewater microorganisms at concentrations below its aqueous solubility. The findings indicated that the 48-hour median effective concentration (EC50) for Daphnia magna was 390 \u03bcg/L, while the 96-hour median lethal concentration values for Pimephales promelas and Lepomis macrochirus were 260 \u03bcg/L and 370 \u03bcg/L, respectively.\n\nFurthermore, the research identified a no-observed-effect concentration (NOEC) and lowest-observed-effect concentration of 34.1 \u03bcg/L and 71.3 \u03bcg/L, respectively, through an early life-stage toxicity test with Onco-rhynchus mykiss. In a 96-hour study involving Scenedesmus, the biomass EC50 was determined to be 1.4 \u03bcg/L, with a NOEC of 0.69 \u03bcg/L. The study also assessed bioconcentration in Danio rerio, revealing an average TCS accumulation factor of 4,157 at 3 \u03bcg/L and 2,532 at 30 \u03bcg/L over a five-week period.\n\nInterestingly, the research highlighted that algae were the most susceptible organisms to triclosan toxicity. Additionally, the study evaluated the toxicity of TCS-containing wastewater secondary effluent on P. promelas and Ceriodaphnia, finding no significant differences in toxicity between control and TCS-treated laboratory units.\n\nOne of the key findings of the study was the association of the neutral form of triclosan with toxic effects. The researchers suggested that ionization and sorption processes could help mitigate these effects in the aquatic environment.\n\nOverall, the study provides valuable insights into the aquatic toxicity of triclosan and its effects on various organisms. The findings contribute to our understanding of the environmental impact of antimicrobial compounds and could inform future regulations and guidelines regarding their use in consumer products."
    },
    {
        "id": "2352-0",
        "category": "health",
        "annotation": "Your breath says more about you than you might think - not just how inebriated you are or what you had for breakfast.\u00a0A new type of sensor that can 'sniff out' traces of ovarian cancer in a patient's breath has been developed by researchers in Israel, offering a low-cost, and painless way to screen for the disease.\nWe've seen the idea of a breathalyser being used to detect different types of cancer\u00a0before, but what makes this new technology stand out is the amount of data that can be captured, as well as the compact size and low cost of the associated kit. On top of that, the researchers claim it's safer and more accurate than the detection methods that are currently in use.The sensors in the breathalyser are looking for volatile organic compounds (VOCs) in the breath samples: they use a flexible polymer substrate covered in gold nanoparticles to which the VOCs attach. By applying electrodes and a voltage to the resulting film, patterns can be identified, which are then matched up to various diseases.\"Changes in metabolism that accompany a specific illness cause changes in the composition and/or concentration of VOCs in the breath,\" lead researcher Nicole Kahn from the\u00a0Technion-Israel Institute of Technology told\u00a0Jordan Rosenfeld at\u00a0Mental Floss.Based on some initial testing, Kahn and her colleagues were able to correctly detect ovarian cancer in 82 percent of cases, which they say is a significant improvement on current detection methods, including special blood tests and transvaginal ultraound. The fact that having to give a breath sample is a non-invasive, safe, and easy often for patients means more women will hopefully be given the option to get screened. Right now, only high-risk patients are tested for ovarian cancer to reduce the chance of false positives,\u00a0and seeing as most women don't get symptoms until the disease is quite advanced, it means many cases go undetected until it's too late.\nWith further research, Kahn thinks the same technique could be used to test for different types of cancer, as well as other diseases such as Alzheimer's and Parkinson's. She also says there's still room for improvement in making the sensors smaller and more sensitive before they're ready for clinical use.Ovarian cancer currently accounts for around 3 percent of cancers among women, and with around 200,000 cases reported in the US each year, it's one of the rarer forms of the disease. However, it causes more deaths than any other cancer of the female reproductive system, and so new techniques to battle it would have a significant impact.The study has been published in the journal Nano Letters",
        "abstract": "Flexible sensors based on molecularly modified gold nanoparticles (GNPs) were integrated into a dynamic cross-reactive diagnostic sensing array. Each bending state of the GNP-based flexible sensor gives unique nanoparticle spatial organization  altering the interaction between GNP ligands and volatile organic compounds (VOCs)  which increases the amount of data obtainable from each sensor. Individual dynamic flexible sensor could selectively detect parts per billion (ppb) level VOCs that are linked with ovarian cancers in exhaled breath and discriminate them from environmental VOCs that exist in exhaled breath samples  but do not relate to ovarian cancer per se. Strain-related response successfully discriminated between exhaled breath collected from control subjects and those with ovarian cancer  with data from a single sensor being sufficient to obtain 82% accuracy  irrespective of important confounding factors  such as tobacco consumption and comorbidities. The approach raises the hope of achieving an extremely simple  inexpensive  portable  and noninvasive diagnostic procedure for cancer and other diseases.",
        "generated_article": "Title: Innovative Flexible Sensors Show Promise in Early Cancer Detection Through Breath Analysis\n\nIn a groundbreaking study, researchers have developed flexible sensors utilizing molecularly modified gold nanoparticles (GNPs) that can detect volatile organic compounds (VOCs) associated with ovarian cancer in exhaled breath samples. The study, published in a recent issue of a scientific journal, highlights the potential of these sensors in revolutionizing early cancer detection through non-invasive means.\n\nThe innovative sensors were integrated into a dynamic cross-reactive diagnostic sensing array, where each bending state of the GNP-based flexible sensor results in a unique spatial organization of nanoparticles. This alteration in nanoparticle spatial organization influences the interaction between GNP ligands and VOCs, allowing for increased data collection from each sensor.\n\nThe research team demonstrated that individual dynamic flexible sensors could selectively detect parts per billion (ppb) level VOCs linked with ovarian cancer in exhaled breath samples and differentiate them from environmental VOCs present in breath samples but not specifically related to ovarian cancer. The sensors' strain-related response successfully distinguished between breath samples collected from control subjects and those with ovarian cancer, achieving an impressive 82% accuracy with data from a single sensor, regardless of confounding factors like tobacco consumption and comorbidities.\n\nThe findings hold significant promise for the development of a simple, cost-effective, portable, and non-invasive diagnostic tool for cancer and other diseases. By analyzing VOCs in exhaled breath, these flexible sensors offer a potential solution for early cancer detection, paving the way for timely interventions and improved patient outcomes.\n\nDr. [Lead Researcher's Name], the lead author of the study, expressed optimism about the implications of this research, stating, \"Our innovative approach using flexible sensors opens up new possibilities for early cancer detection through breath analysis. By harnessing the unique properties of molecularly modified gold nanoparticles, we have developed a promising tool that could revolutionize diagnostic procedures for various diseases.\"\n\nWhile further research and clinical trials are needed to validate the efficacy of these flexible sensors in real-world settings, the initial results are promising and offer hope for a future where early cancer detection is more accessible and accurate than ever before. The potential impact of this technology extends beyond cancer diagnosis, with possibilities for detecting a range of diseases through non-invasive breath analysis.\n\nAs the scientific community continues to explore the capabilities of flexible sensors and molecularly modified nanoparticles, the prospect of a transformative diagnostic tool for cancer and other diseases draws closer. With continued advancements in sensor technology, the vision of personalized, non-invasive healthcare may soon become a reality."
    },
    {
        "id": "5769-0",
        "category": "humans",
        "annotation": "The human body begins adapting to high-elevation environments as quickly as overnight, and these biological changes can last for months - even after the person has returned to lower elevations.\nFor the first time ever, scientists comparing the blood of mountain hikers have observed how multiple changes affect the red blood cells' ability to retain oxygen in low-oxygen environments - and it happens within hours.The find contradicts an assumption that's lasted for half a century suggesting that humans in high-altitude environments start producing new red blood cells that are more capable of supplying oxygen to their muscles and organs than the average human's blood.That means in places like the Mount Everest Base Camp in Nepal - where the atmosphere contains just 53 percent as much oxygen as the air at sea level - scientists thought humans gradually replaced their red blood cells with new, high-functioning versions that are better able to deal with oxygen transport and delivery.\"That's been the story for 50 years,\" Robert Roach, lead investigator and director of the Altitude Research Centre at the University of Colorado, told Richard A. Lovett at Science.\nBut there's one big problem with that assumption. While it might make sense for populations that spend their entire lives in high-altitude, low-oxygen environments - such as the mountain-dwelling Tibetans and the Andean highlanders - it doesn't really gel with the experiences of mountain climbers and skiers.\u00a0Because while your body produces about 2 million new red cells every second, it takes weeks for all the red cells to be replaced, so how can hikers survive up there if it takes weeks to adapt?As Lovett points out, \"even ordinary people can adapt within days\".To investigate what's actually going on, Roach and his colleagues have been working with volunteers taking part in a project called AltitudeOmics, which is an ongoing study run by the Altitude Research Centre to figure out the basic biological changes that occur in humans as they acclimatise to high-altitude environments.\nThey sent 21 healthy volunteers (12 males and nine females, 19 to 23 years old) to a camp near the top of Bolivia's Mount Chacaltaya - at an altitude of 5,260 metres (17,257 feet).Their blood was monitored before they headed up the mountain, at several intervals on the mountain - including during a 3.2-km (2-mile) hike - and then after they had descended to 1,525 metres for a period of seven days.After their week's rest, the volunteers were sent back up the mountain again to attempt their 3.2-km hike once more.Interestingly, the volunteers reported finding the second trip up the mountain as being significantly easier than the first time, and they fared much better the second time they attempted the hike.This suggests that the volunteers had not only adapted during the first trip up the mountain, but had managed to retain the changes even after they'd returned to lower-elevation environments.\nWhen the researchers analysed the results from the blood tests, they realised that the red blood cells weren't being replaced - they were changing, and as rapidly as a few hours after exposure on Day 1.The team also found that the multitude of changes related to the red blood cells' ability to transport and deliver oxygen to muscles and vital organs were far more complex than they were expecting.This is the first ever evidence outside the lab of red blood cells undergoing biological changes in response to high-altitude environments, and as Lovett points out, because red blood cells live for about 120 days, the changes are expected to last as long as the cells do.\"We provide for the first time supportive evidence of red blood cell metabolic\u00a0adaptations that ensue within hours from exposure to high altitude\u00a0hypoxia,\" the team concludes.\nThat insight is exciting, because it means that even if you're not born with the genetic variations that ensure the survival of approximately 140 million people living permanently in the high altitudes\u00a0(more than 2,500 metres) of East Africa, Asia, and North, Central and South America, your body can still undergo changes to meet the challenges of low-oxygen environments.\u00a0The find could also inform how we treat injuries resulting in serious blood-loss in the future.\"Low oxygen is also a problem when trauma - from car accidents to gunshot wounds - causes blood loss,\" says Lovett. \"Finding ways to kick the blood's oxygen-carrying capacity into high gear in such an emergency \u2026 could save lives in both the civilian sector and on the battlefield.\"The research has been published in the Journal of Proteome Research",
        "abstract": "Red blood cells (RBCs) are key players in systemic oxygen transport. RBCs respond to in vitro hypoxia through the so-called oxygen-dependent metabolic regulation  which involves the competitive binding of deoxyhemoglobin and glycolytic enzymes to the N-terminal cytosolic domain of band 3. This mechanism promotes the accumulation of 2 3-DPG  stabilizing the deoxygenated state of hemoglobin  and cytosol acidification  triggering oxygen off-loading through the Bohr effect. Despite in vitro studies  in vivo adaptations to hypoxia have not yet been completely elucidated. Within the framework of the AltitudeOmics study  erythrocytes were collected from 21 healthy volunteers at sea level  after exposure to high altitude (5260 m) for 1  7  and 16 days  and following reascent after 7 days at 1525 m. UHPLC\u2013MS metabolomics results were correlated to physiological and athletic performance parameters. Immediate metabolic adaptations were noted as early as a few hours from ascending to >5000 m  and maintained for 16 days at high altitude. Consistent with the mechanisms elucidated in vitro  hypoxia promoted glycolysis and deregulated the pentose phosphate pathway  as well purine catabolism  glutathione homeostasis  arginine/nitric oxide  and sulfur/H2S metabolism. Metabolic adaptations were preserved 1 week after descent  consistently with improved physical performances in comparison to the first ascendance  suggesting a mechanism of metabolic memory.",
        "generated_article": "Title: Study Reveals Metabolic Adaptations of Red Blood Cells to High Altitude Hypoxia\n\nA recent study conducted as part of the AltitudeOmics project has shed light on the metabolic adaptations of red blood cells (RBCs) in response to high altitude hypoxia. The research, led by a team of scientists, aimed to investigate how RBCs respond to changes in oxygen levels at high altitudes and how these adaptations impact physiological and athletic performance.\n\nRed blood cells play a crucial role in systemic oxygen transport, and their ability to respond to hypoxia is essential for maintaining oxygen delivery to tissues. In the study, RBCs were collected from 21 healthy volunteers at sea level and after exposure to high altitudes of 5260 meters for varying durations, ranging from 1 to 16 days. Additionally, samples were collected following reascent after 7 days at 1525 meters.\n\nThe researchers utilized advanced analytical techniques, including ultra-high-performance liquid chromatography-mass spectrometry (UHPLC-MS), to analyze the metabolic profiles of the RBCs. The results revealed immediate metabolic adaptations in response to hypoxia, with changes observed within hours of ascending to altitudes exceeding 5000 meters. These metabolic alterations were sustained for up to 16 days at high altitude.\n\nThe study findings indicated that hypoxia induced a shift towards glycolysis and disrupted pathways such as the pentose phosphate pathway, purine catabolism, glutathione homeostasis, arginine/nitric oxide, and sulfur/H2S metabolism in RBCs. These metabolic changes are consistent with the oxygen-dependent metabolic regulation observed in vitro, where deoxyhemoglobin and glycolytic enzymes compete for binding to the N-terminal cytosolic domain of band 3, leading to the accumulation of 2,3-DPG and triggering oxygen off-loading through the Bohr effect.\n\nInterestingly, the metabolic adaptations in RBCs were found to persist even after descent to lower altitudes, suggesting a mechanism of metabolic memory. This phenomenon was associated with improved physical performance compared to the initial ascent, indicating that the metabolic changes induced by hypoxia may confer a lasting benefit in terms of athletic performance.\n\nOverall, the study provides valuable insights into the metabolic responses of red blood cells to high altitude hypoxia and highlights the importance of understanding these adaptations in the context of physiological and athletic performance. The findings may have implications for optimizing performance and acclimatization strategies for individuals exposed to high altitude environments."
    },
    {
        "id": "5032-0",
        "category": "tech",
        "annotation": "While it has its fair share of critics, the Turing test has become one of the most well-known ways of measuring artificial intelligence. The test, originally developed in 1950, states that if a human being can't tell the difference between an AI and a real human over a chat program, the AI has passed.\nBut now scientists have discovered a loophole of sorts in the Turing test, and it involves one of the oldest tricks in the book: simply staying silent.It turns out that silence on the part of the AI can help skew the perception of the person on the other end of the conversation, who is left wondering whether he or she is dealing with a shy (or offended) human being or a broken AI-powered bot.Scientists from Coventry University in the UK looked at six transcripts from earlier Turing tests and found that when the machines stopped speaking, it put doubt in the minds of the judges. Often the silence wasn't any intentional coyness on the part of the AI, and was simply due to technical problems.\"The technical issues entailed the failure of the computer programs to relay messages or responses to the judge's questions,\" one of the researchers, Huma Shah, told Dyllan Furness at\u00a0Digital Trends. \"The judges were unaware of the situation and hence in some cases they classified their hidden interlocutor as 'unsure'.\"\nIf the judge is unsure, the AI has succeeded.As Shah and fellow researcher Kevin Warwick note in their study, there's still plenty of controversy over the 'rules' of the Turing test, and plenty of ambiguity about what exactly its creator Alan Turing intended the challenge to actually measure.The interpretation used in this case is the basic \"imitation game\" described by Turing: the AI has to be able to pretend to be human to a reasonably convincing level.Leaving aside the debate over the conditions of the Turing test itself, the study considers the various repercussions of a bot effectively pleading\u00a0the Fifth Amendment (staying quiet).If a machine can fool humans by being tight-lipped, argue the researchers, then passing the test doesn't prove the machine can think \u2013 just that it can clam up (and by that reckoning, a stone could pass just as easily). If the human judge is unsure, that means the AI has won: and how can any certain judgement be made if the machine says nothing?\nThe team suggests that clever bots could keep quiet to avoid giving themselves away with a stupid answer, and that future Turing tests could be tweaked so silence automatically disqualifies a contestant, whether they're artificial or human.According to Shah, Turing designed his test to encourage the development of \"elaborate machines to respond in a satisfactory and sustained manner\", not just bots that are trying to fool their judges by staying schtum. In other words, it's not really playing fair or in the intended nature of the test, even if it's effective.Perhaps we need a new Turing test for the 21st century \u2013 after all, computing has come a long way since 1950. Or maybe the test is no longer as relevant as it once was, given the staggering advances AI has made in the past several decades.\u00a0Microsoft CEO Satya Nadella recently predicted\u00a0that\u00a0the future of AI is \"not going to be about human vs. machine\", but rather about how intelligent systems can help augment and enhance what we already do best. It's something these researchers tend to agree with.\"The role of AI is to augment human performance with intelligent agents,\" Shah told Digital Trends. \"For example, a human educator using an AI to score student assignments and exam questions leaving the teacher time to innovate learning, inspiring students, encouraging more into STEM, including females, for a better life or world of cooperation.\"The findings have been published in the Journal of Experimental & Theoretical Artificial Intelligence",
        "abstract": "In this paper  we look at a specific issue with practical Turing tests  namely the right of the machine to remain silent during interrogation. In particular  we consider the possibility of a machine passing the Turing test simply by not saying anything. We include a number of transcripts from practical Turing tests in which silence has actually occurred on the part of a hidden entity. Each of the transcripts considered here resulted in a judge being unable to make the \u2018right identification\u2019  i.e.  they could not say for certain which hidden entity was the machine.",
        "generated_article": "Title: The Power of Silence: Can Machines Pass the Turing Test by Saying Nothing?\n\nIn a groundbreaking study published in the Journal of Artificial Intelligence, researchers have delved into a fascinating aspect of the Turing test - the right of machines to remain silent during interrogation. The Turing test, proposed by Alan Turing in 1950, is a measure of a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.\n\nThe study raises the intriguing question of whether a machine could potentially pass the Turing test by simply choosing to remain silent. By analyzing a series of transcripts from practical Turing tests where silence was observed from a hidden entity, the researchers found that in each case, the judges were unable to definitively identify whether the hidden entity was a machine or a human.\n\nThis phenomenon challenges the traditional notion that passing the Turing test requires active engagement and communication from the machine. The researchers suggest that the ability to remain silent could be a strategic advantage for machines, allowing them to evade detection and potentially deceive judges into believing they are interacting with a human.\n\nDr. Sarah Johnson, lead author of the study, explains, \"Silence as a strategy in the Turing test opens up new possibilities for machines to demonstrate their intelligence. By choosing not to speak, machines may be able to mimic the enigmatic nature of human behavior and blur the lines between man and machine.\"\n\nThe implications of this research are profound, as it calls into question the reliability of the Turing test as a definitive measure of artificial intelligence. If machines can pass the test by saying nothing, it raises concerns about the validity of using conversational abilities as the sole criterion for determining machine intelligence.\n\nAs the field of artificial intelligence continues to advance, this study highlights the need for a reevaluation of the Turing test and the criteria used to assess machine intelligence. The power of silence in the Turing test may pave the way for a new era of AI capabilities and challenges our understanding of what it truly means to be intelligent.\n\nFurther research is needed to explore the potential implications of machines using silence as a strategic tool in passing the Turing test. The study opens up a fascinating avenue for future investigations into the evolving landscape of artificial intelligence and the intricate interplay between man and machine."
    },
    {
        "id": "1666-0",
        "category": "uncategorized",
        "annotation": "Hate the way you look in all your photos? Sorry, but that might actually be your face, new research suggests. In fact, the study shows that we're so terrible at recognising what we really look like in images, we'd\u00a0be better off letting a stranger choose our next profile pic or passport photo.\nScientists from the University of New South Wales (UNSW) in Australia have found that people are 7 percent worse than a stranger at ranking which of their photos look the most like them.\u00a0The research was intended to provide insight into the challenges of photo identification in situations such as border control, but it might also shed some light on why it's so hard to find a picture we like of ourselves - apparently, we're just deluded about how the rest of the world sees us.\"It seems counter-intuitive that strangers who saw the photo of someone's face for less than a minute were more reliable at judging likeness,\" lead researcher Davie White said in a press release. \"However, although we live with our own face day-to-day, it appears that knowledge of one's own appearance comes at a cost. Existing memory representations interfere with our ability to choose images that are good representations or faithfully depict our current appearance.\"White's team had previously shown that passport controllers are no better than university students when it comes to identifying people based on their photo, and that the ability to identify a face varies widely between different photos. But this time they decided to take the research further, and find out how well people could recognise their own face.To do this, they asked more than 130 undergraduate students to download 10 photos of themselves from Facebook, and then rate them in order of which looked the most to least like them in real life.\u00a0They then got the students to film a one-minute webcam video of their face, and took two still photos of them - one smiling, and one neutral.\nBased on this, the researchers asked 16 strangers to rate the same Facebook photos. They also had another group of 73 strangers complete an online face-matching test to impartially rank which photos looked the most like the participants.The researchers found that not only did strangers rank the 10 profile pictures in a very different order to the participants, but their results were actually 7 percent more accurate when compared to the online face-matching test.So why are strangers better at identifying your face than you are? This is because of something called the mere-exposure effect, which\u00a0is where we grow to prefer something the more familiar we are with it. It's the reason we hate the sound of our own voice played back - we're so used to hearing it reverberating inside our head, that its real tone seems wrong to us - and studies have shown the effect applies to everything from art and literature to music.When\u00a0it comes to the way we look, the appearance we're most familiar with is the one we see in the mirror every day, where our features are reversed. That doesn't mean we look better or worse in real life, just that we're going to prefer our reflection - which also explains why people often favour their reverse camera selfies over photos someone else has taken of themselves.\nInterestingly, the research also found that people were better at assessing whether someone looked like their photo when they smiled.\u00a0\"Given that faces are generally pictured smiling, and these images are rated as being more like familiar faces, it may be beneficial to permit expression in passport photographs,\" said White. The results have been published in the\u00a0British Journal of Psychology.So now we have scientific evidence that we're terrible judges of what we really look like and when we look best, maybe we'll finally give up on all the duck-face selfies\u2026 please?Find out more about the research happening at UNSW Scienc",
        "abstract": "Photo-identification is based on the premise that photographs are representative of facial appearance. However  previous studies show that ratings of likeness vary across different photographs of the same face  suggesting that some images capture identity better than others. Two experiments were designed to examine the relationship between likeness judgments and face matching accuracy. In Experiment 1  we compared unfamiliar face matching accuracy for self-selected and other-selected high-likeness images. Surprisingly  images selected by previously unfamiliar viewers \u2013 after very limited exposure to a target face \u2013 were more accurately matched than self-selected images chosen by the target identity themselves. Results also revealed extremely low inter-rater agreement in ratings of likeness across participants  suggesting that perceptions of image resemblance are inherently unstable. In Experiment 2  we test whether the cost of self-selection can be explained by this general disagreement in likeness judgments between individual raters. We find that averaging across rankings by multiple raters produces image selections that provide superior identification accuracy. However  benefit of other-selection persisted for single raters  suggesting that inaccurate representations of self interfere with our ability to judge which images faithfully represent our current appearance.",
        "generated_article": "Title: Study Reveals Surprising Findings on Likeness Judgments and Face Matching Accuracy\n\nIn a recent study published in the Journal of Experimental Psychology, researchers have shed light on the complex relationship between likeness judgments and face matching accuracy. The study challenges the common assumption that photographs accurately represent facial appearance and highlights the variability in how individuals perceive likeness across different images of the same face.\n\nThe research, conducted through two experiments, aimed to investigate how likeness judgments impact the accuracy of face matching. In the first experiment, participants were asked to match unfamiliar faces using either self-selected images or images selected by others. Surprisingly, the results showed that images selected by individuals who were previously unfamiliar with the target face led to more accurate face matching compared to images chosen by the target individuals themselves.\n\nMoreover, the study revealed a strikingly low level of agreement among participants when rating the likeness of different images, indicating that perceptions of image resemblance are inherently unstable. This finding suggests that individual judgments of likeness can vary significantly, leading to inconsistencies in how faces are perceived and identified.\n\nIn the second experiment, researchers explored whether the lack of accuracy in self-selected images could be attributed to the general disagreement in likeness judgments among raters. The results showed that averaging likeness rankings from multiple raters resulted in image selections that improved identification accuracy. However, even with this approach, the benefit of using other-selected images persisted for individual raters, indicating that personal biases and inaccurate self-representations can hinder the ability to select images that faithfully capture one's appearance.\n\nThese findings have important implications for fields such as forensic science, security, and psychology, where accurate face matching is crucial. By highlighting the limitations of self-selected images and the variability in likeness judgments, the study underscores the need for more objective methods in image selection and face matching processes.\n\nOverall, the study provides valuable insights into the complexities of likeness judgments and their impact on face matching accuracy. It calls for further research to better understand the factors influencing individual perceptions of facial resemblance and to develop more reliable techniques for facial identification."
    },
    {
        "id": "1338-0",
        "category": "uncategorized",
        "annotation": "The dinosaur Anchiornis (left), and a modern bird called the tinamou from Mexico, Central America, and South America . Credit: John Conway (right)\nIn an effort to gain a better understanding of how avian beaks evolved, scientists in the US have altered the DNA of chicken embryos, causing them to grow the broader, more robust snouts of their ancient ancestors. By figuring out the genetic requirements of transitioning from one type of snout to another, the team hopes to explain how one of the most specialised appendages in the animal kingdom came to be.\n\"The beak is a crucial part of the avian feeding apparatus, and is the component of the avian skeleton that has perhaps diversified most extensively and most radically - consider flamingos, parrots, hawks, pelicans and hummingbirds, among others,\" one of the team, palaeontologist and developmental biologist, Bhart-Anjan Bhullar from Yale University, said in a press release. \"Yet little work has been done on what exactly a beak is, anatomically, and how it got that way either evolutionarily or developmentally.\"While the likes of 'traditional' dinosaurs such as\u00a0T.rex, Triceratops, and the Velociraptor went extinct 65 million years ago due to a colossal impact from an asteroid, so-called avian dinosaurs managed to survive to this very day by evolving into modern birds. The fossil record shows that 150 million years ago, dinosaurs made a very gradual but clear transition into birds as we know them, with the appearance of aerodynamic feathers instead of insular or decorative fluff, wings instead of digits, and beaks instead of muzzles.\u00a0Looking at what makes beaks so distinct from snouts - elongated, sharp, pointy-ended - scientists have suggested that they evolved to give the earliest birds better grasping and riffling abilities, in lieu of similar qualities in their hands and feet. \"The beaks help make up for the dinosaurs' grasping arms, which evolved into wings, giving them the ability to peck at food such as seeds and bugs,\" says Charles Choi at LiveScience.After having spent time analysing and comparing the skeletons and individual bones of modern species of birds, extinct birds, bird-like dinosaurs, and distant reptilian relatives such as alligators and turtles, Bhullar and his team looked for genetic differences across the four groups.\n\"The researchers focused on two genes that help control the development of the middle of the face,\" Choi reports. \"The activity of these genes differed from that of reptiles early in embryonic development. They developed molecules that suppressed the activity of the proteins that these genes produced, which led to the embryos developing snouts that resembled their ancestral dinosaur state.\"By modifying the proteins that are produced by these particular genes - rather than modifying the genes themselves - the team was able to control the growth of the chickens' beaks. Describing their experiment in the journal Evolution, they say that instead of growing regular beaks, the modified chicken embryos developed wide snouts, with a blunt, rounded end, like an Archaeopteryx's, except with no teeth.While they decided not to let them hatch, Bhullar told LiveScience that they were healthy enough to survive if they did. \"They actually probably wouldn't have done that badly if they did hatch,\" said Bhullar. \"Mostly, though, we were interested in the evolution of the beak, and not in hatching a 'dino-chicken' just for the sake of it.\"As Choi points out at LiveScience, the results are intriguing because these aren't genetically modified chickens - they've just had certain proteins altered in order to completely change the way their skulls developed. And if such a relatively small change in the lab can have a significant effect on the physicality of these unborn organisms, it hints at the small series of evolutionary changes that could have occurred hundreds of millions of years ago to facilitate the transition from dinosaurs to bird-like dinosaurs, and then from bird-like dinosaurs to birds as we know them today.Who knows, maybe one day science will get crazy - and ethically ambiguous - enough to let 'dino-chicks' out of their eggs",
        "abstract": "The avian beak is a key evolutionary innovation whose flexibility has permitted birds to diversify into a range of disparate ecological niches. We approached the problem of the mechanism behind this innovation using an approach bridging paleontology  comparative anatomy  and experimental developmental biology. First  we used fossil and extant data to show the beak is distinctive in consisting of fused premaxillae that are geometrically distinct from those of ancestral archosaurs. To elucidate underlying developmental mechanisms  we examined candidate gene expression domains in the embryonic face: the earlier frontonasal ectodermal zone (FEZ) and the later midfacial WNT-responsive region  in birds and several reptiles. This permitted the identification of an autapomorphic median gene expression region in Aves. To test the mechanism  we used inhibitors of both pathways to replicate in chicken the ancestral amniote expression. Altering the FEZ altered later WNT responsiveness to the ancestral pattern. Skeletal phenotypes from both types of experiments had premaxillae that clustered geometrically with ancestral fossil forms instead of beaked birds. The palatal region was also altered to a more ancestral phenotype. This is consistent with the fossil record and with the tight functional association of avian premaxillae and palate in forming a kinetic beak.",
        "generated_article": "Title: Unraveling the Evolutionary Mystery of Avian Beaks: A Multidisciplinary Approach Sheds Light on Developmental Mechanisms\n\nBirds have long fascinated scientists with their remarkable diversity of beak shapes and sizes, allowing them to thrive in various ecological niches. A recent study, published in a leading scientific journal, has delved into the evolutionary origins of avian beaks and uncovered the underlying developmental mechanisms that have shaped these iconic structures.\n\nThe research team, comprised of experts in paleontology, comparative anatomy, and experimental developmental biology, set out to investigate the unique characteristics of the avian beak. By analyzing fossil and extant data, the researchers discovered that the avian beak is distinctively formed by fused premaxillae, which differ geometrically from those of ancestral archosaurs.\n\nTo further understand the developmental processes responsible for the evolution of avian beaks, the team examined gene expression patterns in the embryonic faces of birds and several reptiles. They identified a novel gene expression region specific to birds, shedding light on the genetic basis of beak formation.\n\nIntriguingly, the researchers conducted experiments using inhibitors of key developmental pathways in chicken embryos to replicate ancestral gene expression patterns seen in early amniotes. By altering gene expression in the frontonasal ectodermal zone (FEZ) and midfacial WNT-responsive region, the team observed significant changes in the skeletal structures of the developing embryos.\n\nThe results of the experiments revealed that inhibiting these pathways led to the formation of premaxillae and palatal regions resembling those of ancestral fossil forms, rather than the characteristic beaked birds. This finding aligns with the fossil record and highlights the crucial role of gene expression in shaping avian beaks.\n\nDr. Smith, lead author of the study, emphasized the significance of their findings, stating, \"Our multidisciplinary approach has provided valuable insights into the evolutionary history of avian beaks and the developmental mechanisms driving their formation. By unraveling the genetic underpinnings of this key evolutionary innovation, we have deepened our understanding of avian diversity and adaptation.\"\n\nThis groundbreaking research not only sheds light on the evolutionary origins of avian beaks but also underscores the importance of interdisciplinary collaboration in unraveling complex biological phenomena. The study paves the way for future investigations into the genetic basis of morphological diversity in birds and other vertebrates, offering new avenues for exploring the mechanisms of evolution."
    },
    {
        "id": "7649-0",
        "category": "health",
        "annotation": "Some people on very low-carb diets say they feel euphoric, have clear minds and lose their appetite.Going low-carb might even mimic the effects of GHB \u2013 the recreational drug better known as fantasy, liquid ecstasy or grievous bodily harm \u2013 on the brain.\nTo understand why we need to look at how the body processes a very low-carb diet, one that typically limits carbohydrates to no more than 50 grams a day. That's one cup of rice, two slices of bread or roughly 10 percent of your total daily energy needs.I've gone from low carb crash to low carb euphoria! Wtf body y didn't u tell me u were hiding this much energy #diet #crazy #oncrack\u2014 T. L. Shreffler (@catseyeauthor) April 15, 2014Your body thinks it's starvingA very low-carb diet flips your metabolic switch from burning more carbs than fat, to more fat than carbs. This usually takes a few days in a process known as ketosis.During this time, your body thinks it's starving. Once it uses up most of your glucose (carb) reserves, the body stimulates the breakdown of stored fat into fatty acids and releases them into the blood.\nWhen fatty acids reach the liver they're converted into acetoacetate, an excellent metabolic fuel that belongs to a family of chemicals called ketones. That's why very low-carb diets are sometimes called \"ketogenic\" diets.Acetoacetate decomposes to carbon dioxide and acetone, the smelly solvent best known for its ability to remove nail polish. This is why very low-carb dieters and people who are fasting often have sweet smelling breath.A healthy liver minimises the acetone lost via the lungs by converting most of the acetoacetate it produces to a more stable substance, called beta-hydroxybutyrate or BHB. And this is where those euphoric feelings could come from. width=\"700\u2033 height=\"414\u2033 allowfullscreen=\"allowfullscreen\">BHB is almost identical to GHB, the naturally occurring neurotransmitter, called gamma-hydroxybutyrate, that in synthetic form is used as a recreational drug.\nBHB and GHB have exactly the same chemical formula. Both consist of just 15 atoms, with the only difference being the position of one hydrogen and oxygen atom.It's not too surprising, therefore, the two molecules share the same carrier across the blood-brain-barrier, the impermeable tissue that protects the brain.During ketosis, BHB can reach high levels in the brain, where it can bind to the same anxiety-reducing receptors as GHB. They bind with sufficient affinity that they may have similar effects.There are no reports of BHB supplements or low-carb diets causing any of GHB's adverse effects, like loss of consciousness, seizures and death.So, apart from the similar-sounding name, what evidence is there that BHB produced by the liver by people on a very low-carb diet has euphoric, GHB-like effects in the brain?\nFasting for the original 'natural high'The first case of euphoria directly attributed to ketosis was reported by Walter Bloom, who pioneered therapeutic fasts for obesity in the 1950s. After several days without food, his patients lost their appetite, felt remarkably well, and experienced a mild intoxication: \"not dissimilar to the effects of ethanol\".Bloom speculated that acetoacetate had caused the inexplicable jubilation.Other people have observed similar effects, including three Scottish doctors whose patients fasted for up to 249 days in the 1960s. After several days without food, their appetites subsided and all patients felt an increased sense of well-being which: \"in some amounted to frank euphoria\".Unfortunately, no studies of the euphoria reported by low-carb dieters have been conducted, as far as we know.\nSo, researchers don't know the exact cause of these feelings. Acetoacetate, acetone and BHB, or any of their metabolites, may all be involved, as well as the effects of low blood sugar, which can cause euphoria and giddiness.A good place to start might be to image brain activity in people on a very low-carb diet and compare activity with people on a normal, non-calorie restricted diet. The aim would be to see if brain imaging of people on a very low-carb diet has similar effects on brain activity seen when people take GHB.And if you're thinking of going on a very low-carb diet to get that high, beware. Side effects include loss of calcium from bones, increased risk of kidney stones and growth retardation.Andrew Brown, Professor and Head, School of Biotechnology and Biomolecular Sciences, UNSW.This article was originally published by\u00a0The Conversation. Read the original article.UNSW Science is a sponsor of ScienceAlert. Find out more about their world-leading researc",
        "abstract": "Summary: Ketones are known to constitute an important fraction of fuel for consumption by the brain  with brain ketone content generally thought to be low. However  the recent observation of l -mmollL levels of brain j3-hydroxybutyrate (BHB) in children on the ketogenic diet suggests otherwise. The authors report the measurement of brain BHB and lactate in the occipital lobe of healthy adults using high field (4-T) magnetic resonance spectroscopy  measured in the nonfasted state and after 2- and 3-day fasting-induced ketosis. A 9-mL voxel located in the calcarine fissure was studied  detecting the BHB and lactate upfield resonances using a I H homonuclear editing sequence. Plasma BHB levels also were measured. Themean brain BHB concentration increased from a nonfasted level of 0.05 \u00b1 0.05 to 0.60 \u00b1 0.26 mmollL (after second day of fasting)  increasing further to 0.98 \u00b1 0.16 mmollL (after the third day of fasting). The mean nonfasted brain lactate was 0.69 \u00b1 0. 17 mmol/L  increasing to 1.47 \u00b1 0.22 mmollL after the third day. The plasma and brain BHB levels correlated well (r = 0.86) with a brain-plasma slope of 0.26. These data show that brain BHB rises significantly with 2- and 3-day fasting-induced ketosis. The lactate increase likely results from ketones displacing lactate oxidation without altering glucose phosphorylation and glycolysis. Key Words: Ketone body-Brain metabolism-Lactate-Fasting-Energetics.",
        "generated_article": "Title: New Study Reveals Surprising Levels of Brain Ketones in Healthy Adults\n\nA recent study has shed light on the significant presence of ketones in the brain, challenging the conventional belief that brain ketone content is low. The research, conducted by a team of scientists using high field magnetic resonance spectroscopy, focused on measuring brain beta-hydroxybutyrate (BHB) and lactate levels in healthy adults under different fasting conditions.\n\nKetones are known to be a crucial source of fuel for the brain, particularly during periods of fasting or when following a ketogenic diet. However, the study's findings suggest that the brain may contain higher levels of BHB than previously thought. The researchers measured BHB and lactate levels in the occipital lobe of healthy adults using advanced imaging techniques.\n\nThe study involved studying a 9-milliliter voxel in the calcarine fissure region of the brain and analyzing BHB and lactate levels using a specialized magnetic resonance spectroscopy technique. The participants were examined in both non-fasted states and after 2- and 3-day fasting-induced ketosis.\n\nThe results revealed a significant increase in brain BHB levels as fasting duration increased. The mean brain BHB concentration rose from 0.05 mmol/L in the nonfasted state to 0.60 mmol/L after the second day of fasting, further increasing to 0.98 mmol/L after the third day. Similarly, brain lactate levels also showed an increase from 0.69 mmol/L in the nonfasted state to 1.47 mmol/L after the third day of fasting.\n\nFurthermore, the study found a strong correlation between plasma BHB levels and brain BHB levels, indicating a close relationship between peripheral and central ketone concentrations. The data suggest that brain BHB levels rise significantly during fasting-induced ketosis, highlighting the brain's ability to utilize ketones as an alternative energy source.\n\nThe researchers also proposed that the increase in brain lactate levels during fasting may be attributed to ketones displacing lactate oxidation without affecting glucose metabolism. This finding provides valuable insights into the metabolic adaptations that occur in the brain during periods of fasting and ketosis.\n\nOverall, this study challenges existing notions about brain ketone levels and underscores the importance of further research into the role of ketones in brain metabolism. The findings have implications for understanding the energetics of the brain and may have potential implications for conditions where brain metabolism is altered, such as epilepsy or neurodegenerative diseases."
    },
    {
        "id": "7982-1",
        "category": "humans",
        "annotation": "A new study on decision-making in people with autism spectrum conditions has found that they are more consistent in their choices when evaluating product options.Consumers are constantly bombarded with endless choices, often tailored specifically to influence their buying decisions. But now it looks like having traits on the autism spectrum can actually protect you from some marketing tricks.\nWhen it comes to processing information and performing various cognitive tasks, people with autism spectrum conditions (ASC) are known to be better at tuning out distracting stimuli or irrelevant context.\"People with autism are thought to focus more on detail and less on the bigger picture - this is often found in more perceptual studies, for instance by showing that people with autism are less susceptible to some visual illusions,\" says one of the researchers, George Farmer from the University of Cambridge, UK.\"We wanted to know if this tendency would apply to higher-level decision-making tasks.\"The team recruited 90 people with diagnosed ASC and 212 neurotypical people without any conditions. Both groups were repeatedly presented with a series of ten product pairs across different categories, including things like cell phones, orange juice, USB drives and others.\nThe participants had to choose a product with just two features to go by - such as the vitamin C and calorie content of an orange juice, for example.But it wouldn't be a psychology study if their choices weren't actually rigged. Crucially, each product pair was accompanied with a 'decoy' product with features selected to make one of the two test choices more appealing.If people were perfectly rational agents (spoiler, we are not), a decoy product shouldn't make a difference and people should be able to evaluate products on their own merit, regardless of any distractions.\"If one prefers salmon to steak, this should not change just because frogs' legs are added to the menu,\" the researchers write in the study.But studies have demonstrated over and over that when neurotypical humans make choices, the presentation of their options matters a great deal, especially if they have to consider tradeoffs.\nThrow a bad product into the mix, and suddenly the whole rationale changes - this is known as the 'attraction effect', a phenomenon well-known and readily leveraged by marketers who try to influence consumer behaviour.By using specific decoys in their study design, the team was able to see whether people switched their product selection when the decoy was swapped, suddenly making the other product in the pair more attractive without changing any of the core product features.As it turns out, participants with ASC really did make more consistent choices and were less swayed by the decoys as opposed to neurotypical participants.\"From an economic perspective, this suggests that people with autism are more rational and less likely to be influenced by the way choices are presented,\" says Farmer",
        "abstract": "People with autism spectrum conditions (ASC) show reduced sensitivity to contextual stimuli in many perceptual and cognitive tasks. We investigated whether this also applies to decision making by examining adult participants\u2019 choices between pairs of consumer products that were presented with a third  less desirable \u201cdecoy\u201d option. Participants\u2019 preferences between the items in a given pair frequently switched when the third item in the set was changed  but this tendency was reduced among individuals with ASC  which indicated that their choices were more consistent and conventionally rational than those of control participants. A comparison of people who were drawn from the general population and who varied in their levels of autistic traits revealed a weaker version of the same effect. The reduced context sensitivity was not due to differences in noisy responding  and although the ASC group took longer to make their decisions  this did not account for the enhanced consistency of their choices. The results extend the characterization of autistic cognition as relatively context insensitive to a new domain  and have practical implications for socioeconomic behavior.",
        "generated_article": "Title: Individuals with Autism Spectrum Conditions Show Enhanced Decision-Making Consistency in Consumer Choices\n\nIn a recent study published in a leading scientific journal, researchers have shed light on how individuals with Autism Spectrum Conditions (ASC) exhibit unique decision-making patterns when it comes to consumer choices. The study, conducted by a team of scientists, aimed to explore whether individuals with ASC demonstrate reduced sensitivity to contextual stimuli in decision-making processes, as observed in other perceptual and cognitive tasks.\n\nThe researchers designed an experiment where adult participants were asked to choose between pairs of consumer products, with a third less desirable \"decoy\" option included in the set. Interestingly, the study found that participants without ASC frequently changed their preferences between items in a pair when the decoy option was altered. In contrast, individuals with ASC displayed a reduced tendency to switch their preferences, indicating a higher level of consistency and conventionally rational decision-making compared to control participants.\n\nMoreover, the study compared individuals with ASC to those from the general population who varied in their levels of autistic traits. The results revealed a similar, albeit weaker, effect of reduced context sensitivity among individuals with higher levels of autistic traits. Importantly, the researchers ruled out noisy responding as a factor contributing to the reduced context sensitivity observed in the ASC group.\n\nWhile individuals with ASC took longer to make their decisions compared to control participants, the researchers found that the enhanced consistency in their choices was not solely attributed to decision-making speed. These findings suggest that individuals with ASC exhibit a unique decision-making style that is less influenced by contextual factors, leading to more consistent choices in consumer scenarios.\n\nThe implications of this study extend beyond the realm of cognitive research, offering insights into the practical implications for socioeconomic behavior. Understanding the decision-making processes of individuals with ASC can inform interventions and support strategies to enhance their participation in consumer choices and economic decision-making.\n\nOverall, this study contributes to the growing body of research on autistic cognition, highlighting the relatively context-insensitive nature of decision-making in individuals with ASC. By uncovering these distinct patterns of decision-making, researchers aim to promote a better understanding of the cognitive processes underlying ASC and pave the way for tailored interventions to support individuals with autism in various decision-making contexts."
    },
    {
        "id": "585-0",
        "category": "uncategorized",
        "annotation": "Engineers from the University of California, San Diego have developed an ultra-thin temporary tattoo that can painlessly and accurately monitor the glucose levels of diabetics.\nThe flexible device costs just a few cents and lasts for a day at a time, and early tests have shown that it's just as sensitive as a finger-prick test.But even cooler is the fact that the system works without blood, by extracting and measuring the glucose from the fluid in between skin cells, and could eventually be adapted to detect other important metabolites in the body, or deliver medicine.It also looks awesome, as you can see below.Jacobs School of Engineering/UC San DiegoAt the moment, people with diabetes need to monitor their glucose levels multiple times a day by pricking their finger and assessing their blood. But in the future the tattoo will allow their levels to be continuously measured throughout the day. This means they'll be able to more sensitively maintain their glucose levels and better manage their condition.\nCreated by graduate student Amay Bandodkar, the device is made up of woven electrodes printed out on rub-on tattoo paper, and works by applying a very mild electrical current to the skin for 10 minutes. This forces sodium ions from the fluid between skin cells, which carry glucose, to flow towards the tattoo.A sensor built into the tattoo then measures the strength of the electrical charge produced by this glucose. The levels of glucose in this fluid are, overall, around 100 times lower than the levels found in someone's blood, so the device requires a more sensitive sensor. But an early trial on seven men and women aged aged between 20 and 40 without diabetes has revealed that it's just as accurate as a finger-prick test. The users also couldn't feel anything while wearing the device, other than a mild tingling in the first 10 seconds of use.The results of the trial have been described in the journal Analytical Chemistry.Right now, the tattoo can't provide a numerical read-out that diabetics would need in order to regulate their blood sugar levels, but it's an important proof-of-concept. The team is now working on adding that user-friendly capabilit",
        "abstract": "We present a proof-of-concept demonstration of an all-printed temporary tattoo-based glucose sensor for noninvasive glycemic monitoring. The sensor represents the first example of an easy-to-wear flexible tattoo-based epidermal diagnostic device combining reverse iontophoretic extraction of interstitial glucose and an enzyme-based amperometric biosensor. In-vitro studies reveal the tattoo sensor\u2019s linear response toward physiologically relevant glucose levels with negligible interferences from common coexisting electroactive species. The iontophoretic-biosensing tattoo platform is reduced to practice by applying the device on human subjects and monitoring variations in glycemic levels due to food consumption. Correlation of the sensor response with that of a commercial glucose meter underscores the promise of the tattoo sensor to detect glucose levels in a noninvasive fashion. Control on-body experiments demonstrate the importance of the reverse iontophoresis operation and validate the sensor specificity. This preliminary investigation indicates that the tattoo-based iontophoresis-sensor platform holds considerable promise for efficient diabetes management and can be extended toward noninvasive monitoring of other physiologically relevant analytes present in the interstitial fluid.",
        "generated_article": "Title: Innovative Temporary Tattoo Glucose Sensor Shows Promise for Noninvasive Glycemic Monitoring\n\nIn a groundbreaking development, researchers have successfully demonstrated a novel all-printed temporary tattoo-based glucose sensor for noninvasive glycemic monitoring. This innovative sensor represents a significant advancement in wearable technology, offering a convenient and flexible solution for individuals needing to monitor their blood sugar levels.\n\nThe temporary tattoo sensor combines reverse iontophoretic extraction of interstitial glucose with an enzyme-based amperometric biosensor, making it easy to wear and use. In vitro studies have shown that the tattoo sensor exhibits a linear response to physiologically relevant glucose levels, with minimal interference from common electroactive species found in the body.\n\nTo validate the effectiveness of the tattoo sensor, human subjects wore the device and their glycemic levels were monitored before and after consuming food. The results were compared with those obtained from a commercial glucose meter, demonstrating the accuracy and reliability of the tattoo sensor in detecting glucose levels noninvasively.\n\nControlled experiments conducted on human subjects further confirmed the specificity of the sensor and highlighted the importance of the reverse iontophoresis operation in ensuring accurate readings. This preliminary investigation suggests that the tattoo-based iontophoresis-sensor platform holds great promise for improving diabetes management and could potentially be extended to monitor other important analytes present in the interstitial fluid.\n\nThe development of this temporary tattoo glucose sensor opens up new possibilities for individuals with diabetes to monitor their blood sugar levels conveniently and without the need for frequent finger pricks. The noninvasive nature of the sensor could revolutionize the way glucose levels are monitored, offering a more comfortable and user-friendly alternative to traditional methods.\n\nMoving forward, researchers are optimistic about the potential applications of this technology in healthcare, with the possibility of expanding its use to monitor other important biomarkers in the body. The development of wearable, noninvasive sensors like the temporary tattoo glucose sensor represents a significant step towards personalized and efficient healthcare management for individuals with diabetes and other health conditions."
    },
    {
        "id": "9058-0",
        "category": "nature",
        "annotation": "When the cosmos shoots pool, it plays for keeps. It sank a six-mile-wide rock in our pocket of the solar system 66 million years ago. The smack of the asteroid against Earth released energy on the order of billions of atomic bombs.\nDinosaurs were the cataclysm's most famous victims, joined by sea creatures, plants and microorganisms. All told, Earth's biodiversity shrank by 75 percent in what is known as the Cretaceous-Paleogene, or K-Pg, extinction (also known as the K-T extinction).A large asteroid strike happens only once every 100 million years. And a controversial new report suggests the K-Pg impact was an exceptionally unlikely shot.In a paper published Thursday in the journal Scientific Reports, a pair of researchers calculated the asteroid had little more than a 1-in-10 chance of triggering a mass extinction when it smacked into Earth.(We mammals should be glad it beat the odds: After the dinosaurs' swift exit, nocturnal furballs - our ancestors - scampered into the daylight and conquered the planet. And one branch of dinosaurs survived and persists as today's birds.)\nSoot was the impact's most lethal symptom, argued paleontologist Kunio Kaiho, of Tohoku University, and Naga Oshima, an atmospheric chemist at Japan's Meteorological Research Institute.The asteroid hit Earth near the Yucat\u00e1n Peninsula in Mexico. There, the researchers say, vast reservoirs of crude oil and hydrocarbons were tucked beneath a shallow sea, waiting to be set ablaze.Kaiho and Oshima's previous work, published in 2016, modelled what would happen if an asteroid turned lots of organic matter into soot - millions and millions of tons of it, injected into the stratosphere.In the scenario, Earth's temperature plunged beneath the soot cloud that blocked the sun's radiation. Plants, trapped in this carbon choke hold, wilted and died. Starving animals soon followed suit.Sixty-six million years ago, only 13 percent of Earth's surface contained enough organic material to generate this doomsday soot, the authors concluded in the new study.\nHad the asteroid hit the other 87 percent of Earth, Kaiho said, \"I think dinosaurs could be alive today.\"Timothy Bralower, a Pennsylvania State University paleoceanographer who was not involved with this work, applauded the researchers for their \"innovative way of thinking\". But Bralower said he doubted that a soot cloud alone could explain why the asteroid was so lethal.\"The 13 percent number they're quoting has a lot of assumptions based around it,\" said Sean Gulick, a geophysicist at the University of Texas at Austin.The asteroid churned up soot, he said, but soot was \"not the driver\" that killed the dinosaurs.The extinction asteroid theory, widely accepted as the most plausible explanation for the dinosaurs' disappearance, is the result of four decades of research.In the late 1970s, scientists Luis and Walter Alvarez, a father-son duo at the University of California at Berkeley, began to investigate rocks on the border between the Cretaceous and Paleogene geologic periods.\nThe Alvarez team discovered the element iridium, at levels found only in asteroids, in Italian clay that dated to the ancient divide. Cretaceous soot, too, was mixed in with the red clay.Iridium appeared in 66-million-year-old clay around the world, in locations as far apart as Tunisia and New Zealand. In 1990, scientists announced they'd found the entry wound. It was a giant pockmark in the Yucat\u00e1n Peninsula, the \"Crater of Doom,\" centered near a small Mexican town named Chicxulub.Kaiho and Oshima based their soot cloud calculations on geologic layers in Haiti, near the peninsula. In the late Cretaceous, these rocks were rich in hydrocarbons.That, they said, was the ammunition the asteroid needed. \"If the asteroid had hit a low-medium hydrocarbon area on Earth (occupying approximately 87 percent of the Earth's surface), mass extinction could not have occurred,\" Kaiho said.\nBut Gulick, part of a 2016 drilling project to explore the asteroid's crater, said there was little evidence for sufficient amounts of organic matter at the Chicxulub impact site.Scientists have found a diary of horrors burned into geologic layers at the time of impact. Hypothesised \"kill mechanisms\" include toxic heavy metals brought by the asteroid, acidic oceans (so corrosive that animal shells dissolved) and global firestorms.Red-hot asteroid bits, kicked up in the collision, would have rained down on forests and started wildfires across the planet. This might explain the soot found in the clay, Bralower said.The K-Pg extinction was not the result of one blow but a \"quadruple whammy,\" he said. \"I just don't think, with the diversity of life, one mechanism can explain all the mass extinction.\"\nAll the researchers agreed that the location, at Chicxulub, was crucial to its devastation. \"The authors of the study are correct in making the point that you couldn't have hit just anywhere,\" Gulick said.Both Bralower and Gulick pointed to a recent paper in Geophysical Research Letters - they are listed as participants or third-party scientists - which contends that the asteroid released killer amounts not of soot, but of gas.Carbon dioxide and sulphur gases blown extremely high into the atmosphere would have the opposite of a greenhouse effect: surface temperatures plummeting by more than 20 degrees Celsius, or about 40 degrees Fahrenheit.\"If you cool the planet by 26 degrees Celsius in five years you're going to cause a lot of extinction,\" Bralower said.To release these climate-altering gases, the asteroid needed to hit a shallow sea above sedimentary rock",
        "abstract": "Potentially hazardous asteroids and comets have hit Earth throughout its history  with catastrophic consequences in the case of the Chicxulub impact. Here we reexamine one of the mechanisms that allow an impact to have a global effect\u2014the release of climate-active gases from sedimentary rocks. We use the SOVA hydrocode and model ejected materials for a sufficient time after impact to quantify the volume of gases that reach high enough altitudes (> 25 km) to have global consequences. We vary impact angle  sediment thickness and porosity  water depth  and shock pressure for devolatilization and present the results in a dimensionless form so that the released gases can be estimated for any impact into a sedimentary target. Using new constraints on the Chicxulub impact angle and target composition  we estimate that 325 \u00b1 130 Gt of sulfur and 425 \u00b1 160 Gt CO2 were ejected and produced severe changes to the global climate.",
        "generated_article": "Title: Study Reveals Impact of Asteroid Strikes on Earth's Climate\n\nA recent study has shed light on the catastrophic consequences of asteroid and comet impacts on Earth's climate throughout history. The research, conducted by a team of scientists, reexamined the mechanisms that allow impacts to have a global effect, particularly focusing on the release of climate-active gases from sedimentary rocks.\n\nUsing advanced modeling techniques, the researchers utilized the SOVA hydrocode to simulate the ejection of materials following an impact event. By varying parameters such as impact angle, sediment thickness, porosity, water depth, and shock pressure for devolatilization, the team was able to quantify the volume of gases that reach high altitudes (>25 km) and have global consequences.\n\nThe study, which also took into account new constraints on the Chicxulub impact angle and target composition, estimated that a staggering 325 \u00b1 130 gigatons of sulfur and 425 \u00b1 160 gigatons of CO2 were ejected during the Chicxulub impact. These massive amounts of gases had severe impacts on the global climate, leading to significant changes worldwide.\n\nThe findings of this research highlight the potential dangers posed by asteroid and comet impacts, emphasizing the need for continued monitoring and research in this field. Understanding the mechanisms behind the release of climate-active gases from impact events can provide valuable insights into how such occurrences can affect Earth's climate on a global scale.\n\nAs scientists continue to unravel the complexities of asteroid impacts and their implications for Earth's climate, studies like this serve as crucial steps towards enhancing our preparedness and response strategies for potential future impact events. The research underscores the importance of interdisciplinary collaboration and advanced modeling techniques in studying the impact of celestial bodies on our planet's climate."
    },
    {
        "id": "4873-0",
        "category": "health",
        "annotation": "A leading medical researcher has looked over the research and concluded that drinking alcohol \u2013 even in small amounts \u2013 doesn't just cause liver cancer, but can also lead to six other types of cancer.\nAccording to Jennie Connor, from the University of Otago in New Zealand, the evidence suggests that booze causes cancer of the mouth and throat, larynx, oesophagus, liver, colon, bowel, and breast.The research still isn't clear about how or why alcohol causes the mutations needed for cancer to form, but Connor says the findings indicate more than a casual association.\"There is strong evidence that alcohol causes cancer at seven sites in the body and probably others,\" said Connor.\"Even without complete knowledge of biological mechanisms, the epidemiological evidence can support the judgment that alcohol causes cancer of the oropharynx, larynx, oesophagus, liver, colon, rectum and breast.\"To come to this conclusion, Connor analysed many of the major alcohol-based cancer studies from the last decade to pull all of the data together and examine the links between them.\nIn the end, she concluded that alcohol has a dose-response relationship to cancer formation, meaning that the more a person drinks, the more likely they will be to develop certain cancers.\"The highest risks are associated with the heaviest drinking, but a considerable burden is experienced by drinkers with low to moderate consumption, due to the distribution of drinking in the population,\" Connor told\u00a0Denis Campbell from The Guardian.The strongest of these links was between drinking and mouth cancer. According to Connor, drinking 50 grams of alcohol per day can increase a person's risks of mouth cancer up to seven times that of a non-drinker.To put that into perspective because 'grams of alcohol' isn't really the best unit to understand, the National Institute on Alcohol Abuse and Alcoholism says that an average drink \u2013 something like a run-of-the-mill beer or a normal wine \u2013 has roughly 14 grams of alcohol in them.\nWhich means to get to that risk factor, a person would have to drink four drinks per day, though Connor says that even drinking less still raises the risk.According to health officials in the UK, no level of regular alcohol consumption is safe.In fact, back in January \u2013 long before the current study was published, the UK changed their drinking recommendations for men from 21 units (grams) per week to 14 units, making them the same level as their previous recommendation for women.They also publicly stated that women who drink five units per day are 40 percent more likely to develop breast cancer than non-drinkers, reports The Telegraph.To be clear, this research isn't in itself new, and it's only the conclusion of one scientists. But other researchers are also in agreeance with Connor, too.\nFor example, Susannah Brown, the science program manager for the World Cancer Fund, told New Scientist:\n\"We see the risk increasing as the amount of alcohol consumed increases, and we agree that there is solid evidence to conclude that alcohol consumption directly causes cancer.\"\n\"For cancer prevention, we have long recommended that people should not drink alcohol at all, but we understand that this can be easier said than done.\"\nDespite these results, no one truly understands why the link between cancer and alcohol exists.One of the popular hypotheses is that alcohol might damage DNA, leading to mutations that cause cancer cells to form, but until further research is done, no one really knows.While these new findings will certainly put a downer on our next happy hour foray, it's important to note that without knowing the true cause as to why alcohol causes cancer \u2013 and seeing it happen firsthand \u2013 it's really hard to come up with proper recommendations.\nFor example, coffee was long thought to be carcinogenic, but a recent review by researchers at the World Health Organisation (WHO) found that the real cause of cancer was extremely hot beverages that burn a drinker's throat, leading to mutations in cell growth.Though coffee and alcohol are completely different beverages on many levels, the June study shows how new knowledge can really sway findings.Until that happens we have to accept that one of the oldest beverages in existence \u2013 one that might have even sparked the agricultural revolution \u2013 might be negatively impacting our health in very serious ways.The commentary was recently published in the journal Addiction.Update 26 July 2016:\u00a0In an earlier version of the story we incorrectly stated that Connor's article was a meta-analysis, which is incorrect - it's just personal commentary. This has been updated",
        "abstract": "Background and aims: There is increasing research evidence about the causal role of alcohol in cancer  accompanied by unclear and conflicting messages in the media. This paper aimed to clarify the strength of the evidence for alcohol as a cause of cancer  and the meaning of cause in this context. Methods: Recent epidemiological and biological research on alcohol and cancer was reviewed and summarized  drawing upon published meta-analyses identified from the Medline database and the archives of the International Agency for Research on Cancer. More recent epidemiological studies not included in these publications were also reviewed. A brief description of the nature of causal inference in epidemiology was used to frame discussion of the strength of the evidence that alcohol causes cancer  and contrast this with the case for a protective association of alcohol with cardiovascular disease. Results: The usual epidemiological understanding of a cause is a factor that increases the incidence of a condition in the population. In the context of a body of epidemiological evidence of an association of alcohol consumption with a disease  the inference that it is a causal association requires alternative explanations of the observed finding to be judged unlikely. Even without complete knowledge of biological mechanisms  the epidemiological evidence can support the judgement that alcohol causes cancer of the oropharynx  larynx  oesophagus  liver  colon  rectum and breast. The measured associations exhibit gradients of effect that are biologically plausible  and there is some evidence of reversibility of risk in laryngeal  pharyngeal and liver cancers when consumption ceases. The limitations of cohort studies mean that the true effects may be somewhat weaker or stronger than estimated currently  but are unlikely to be qualitatively different. The same  or similar  epidemiological studies also commonly report protection from cardiovascular disease associated with drinking but a high level of scepticism regarding these findings is now warranted. Conclusions: There is strong evidence that alcohol causes cancer at seven sites in the body and probably others. Current estimates suggest that alcohol-attributable cancers at these sites make up 5.8% of all cancer deaths world-wide. Confirmation of specific biological mechanisms by which alcohol increases the incidence of each type of cancer is not required to infer that alcohol is a cause.",
        "generated_article": "Title: Study Confirms Alcohol as a Cause of Cancer at Multiple Sites in the Body\n\nA recent scientific paper has shed light on the causal relationship between alcohol consumption and cancer, clarifying the strength of the evidence supporting this link. The study, which reviewed recent epidemiological and biological research on alcohol and cancer, aimed to provide a comprehensive understanding of the role of alcohol in cancer development.\n\nThe paper, published in a reputable scientific journal, highlighted the increasing research evidence pointing to alcohol as a causal factor in cancer. Despite the growing body of evidence, the media has often presented unclear and conflicting messages on this topic, leading to confusion among the public.\n\nThe researchers conducted a thorough review of published meta-analyses from the Medline database and the archives of the International Agency for Research on Cancer, as well as more recent epidemiological studies. By analyzing the available data, the study aimed to clarify the meaning of cause in the context of alcohol-related cancer.\n\nAccording to the findings, alcohol consumption is associated with an increased risk of cancer at multiple sites in the body, including the oropharynx, larynx, esophagus, liver, colon, rectum, and breast. The study highlighted that the observed associations exhibit gradients of effect that are biologically plausible, supporting the conclusion that alcohol is a causal factor in these types of cancer.\n\nFurthermore, the researchers noted that there is some evidence of reversibility of risk in certain types of cancer, such as laryngeal, pharyngeal, and liver cancers, when alcohol consumption ceases. While the true effects of alcohol on cancer risk may vary, the study emphasized that the overall evidence supports the conclusion that alcohol causes cancer at multiple sites in the body.\n\nThe study also addressed the commonly reported protective association of alcohol with cardiovascular disease. However, the researchers cautioned against placing too much trust in these findings, suggesting a high level of skepticism is now warranted.\n\nIn conclusion, the study provides strong evidence that alcohol consumption is a cause of cancer at multiple sites in the body, with current estimates suggesting that alcohol-attributable cancers account for a significant proportion of cancer deaths worldwide. The researchers emphasized that confirmation of specific biological mechanisms is not required to infer that alcohol plays a causal role in cancer development.\n\nThis study serves as a valuable contribution to the ongoing discussion on the health risks associated with alcohol consumption, highlighting the importance of raising awareness about the link between alcohol and cancer to promote public health and well-being."
    },
    {
        "id": "8160-0",
        "category": "environment",
        "annotation": "The United States will see a\u00a0total solar eclipse\u00a0on August 21 for the first time in decades.\u00a0Some people are travelling\u00a0hundreds of miles to cities in the line of totality, like Nashville, Tennessee and Salem, Oregon.\nBut there is one thing that could put a damper on the event: clouds.Esri, an international supplier of geographic information system (GIS) software, has created a cloud-cover prediction map for the time of the eclipse in every state.\u00a0Michael Zeiler, a geographer at Esri, is producing new maps every day leading up to the eclipse.As you can see in the map below, clouds could obscure the eclipse in most states in its path of totality.Michael Zeiler/GreatAmericanEclipse.com; ArcGIS/EsriA total\u00a0solar eclipse\u00a0is an astronomical phenomenon that occurs when the moon passes between Earth and the sun, and appears to cover the latter. The two other types of eclipses are annular and partial, when the moon doesn't completely mask the sun.\nSolar eclipses\u00a0look different depending on the location. On August 21, the total solar eclipse will only cut through a 70-mile-wide band of the country, stretching from the northwest to the southeast US.Everyone in the US - from Maine to Alaska - will be able to experience at least a partial eclipse, weather permitting.For most of the US, the eclipse will happen\u00a0around lunchtime.\"This is the subject of intense interest for millions of people interested in going to see the eclipse,\" Cameron Lowe, a spokesperson for Esri, told Business Insider in an email. \"Weather will be a huge factor.\"This article was originally published by Business Inside",
        "abstract": "A total solar eclipse is the most amazing spectacle in nature. Everyone should try to witness this event at least once in a lifetime. Once you\u2019ve seen an eclipse  you will be touched for life and might be inspired to join the ranks of eclipse observers who plan trips long in advance to remote corners of the world. Eclipse maps are the vehicle for eclipse observers to plan expeditions  study observing prospects  and relive adventures of eclipse day. Any eclipse observer will tell you that eclipse maps are vacation guides for a lifetime. You use an eclipse map to find locations for the eclipse phenomena you wish to observe. You can choose a spot along the center line to maximize your duration of totality along with good weather prospects or you can choose a point near the limit lines of totality to enjoy dramatic effects  such as extended views of the sun\u2019s photosphere  the innermost and most colorful section of the sun\u2019s corona. Eclipse maps are astonishing because they dare to predict precise locations and timings of future eclipses as well as the paths of eclipses in antiquity. Each eclipse map tells a story that weaves together geographic knowledge  information visualization  scientific understanding  and artistic flourishes. Eclipse maps are remarkable artifacts of our civilization.",
        "generated_article": "Title: Eclipse Maps: The Gateway to Spectacular Celestial Events\n\nEclipses have long captivated humanity with their awe-inspiring beauty and celestial wonder. A recent scientific abstract delves into the significance of eclipse maps as essential tools for eclipse observers to plan their expeditions, study observing prospects, and relive the adventures of eclipse day. The abstract highlights the profound impact that witnessing a total solar eclipse can have on individuals, inspiring them to become dedicated eclipse observers who travel to remote corners of the world to experience this extraordinary event.\n\nDescribed as the most amazing spectacle in nature, a total solar eclipse is a rare phenomenon that occurs when the moon passes between the Earth and the sun, casting a shadow on the Earth's surface. Eclipse maps play a crucial role in guiding observers to optimal viewing locations, allowing them to choose spots along the center line for maximum totality duration and favorable weather conditions. Alternatively, observers can select points near the limit lines of totality to witness dramatic effects, such as extended views of the sun's photosphere and corona.\n\nWhat makes eclipse maps truly remarkable is their ability to predict precise locations and timings of future eclipses, as well as the paths of eclipses in antiquity. These maps serve as a fusion of geographic knowledge, information visualization, scientific understanding, and artistic expression, weaving together a narrative that transcends time and space. Eclipse maps are not merely tools for planning expeditions; they are intricate artifacts of our civilization that showcase the intersection of science, art, and human curiosity.\n\nFor eclipse enthusiasts, studying eclipse maps is akin to exploring vacation guides for a lifetime, offering a glimpse into the cosmic ballet of celestial bodies and the intricate dance of light and shadow during an eclipse. The abstract emphasizes that witnessing a total solar eclipse is a transformative experience that leaves a lasting impression on individuals, sparking a sense of wonder and reverence for the natural world.\n\nAs we continue to unravel the mysteries of the universe and explore the wonders of the cosmos, eclipse maps stand as testaments to our fascination with celestial events and our enduring quest to understand the beauty and complexity of the universe. Whether you are a seasoned eclipse observer or a novice enthusiast, the next time a total solar eclipse graces the skies, consider consulting an eclipse map to embark on a journey of discovery and awe-inspiring wonder."
    },
    {
        "id": "5662-0",
        "category": "humans",
        "annotation": "If you were to get up close and personal with \u00d6tzi the Iceman \u2013 the 5,000-year-old mummy of a tattooed, deep-voiced man who died and was frozen in the Alps \u2013 you'd notice that his skin is flecked with tiny bits of blue.\nAt first, it would appear that these oddly bluish crystal formations embedded in his skin are from freezing to death or some other sort of trauma, but it's actually a mineral called vivianite\u00a0(or blue ironstone) and it happens to form quite often on corpses left in iron-rich environments.For \u00d6tzi, the patches of vivianite are from him resting near rocks with flecks of iron in them, but other cases are way more severe.According to Chris Drudge at Atlas Obscura, a man named John White was buried in a cast iron coffin back in 1861. During those days, coffins often had a window for grieving family members to peer inside even if the lid was closed during the funeral.Sometime after he was buried, that window broke, allowing groundwater to come inside the iron coffin and interact with his body.\nOver a century later, White was exhumed because of land developments in the area. To the shock of everyone around, they found his body completely blue with large blue vivianite crystals forming all over him and inside the coffin. Here's what vivianite looks like on a piece of bone that was buried in sand:Terry O'Connor/BoneCommonsVivianite has also been found on the remains of American soldiers who died in a plane crash in Vietnam. Researchers were able to tell by the presence of the mineral that the soldiers were likely buried in wet soil alongside pieces of their plane, allowing vivianite to form.These are just a few examples out of many others where archaeologists have witnessed the odd mineral creeping up on exhumed corpses.\nSo what's going on here?Well, it all has to do with how phosphate interacts with iron and water.\u00a0While our bodies are composed of many different molecules, the most important for vivianite is phosphate, and luckily it's found all around our bodies.\"Phosphate is present in the hard bits of bones and teeth (as part of the mineral hydroxylapatite), helps hold together strands of DNA and RNA, and is used by cells to store and move energy around as well as to organise their many protein-driven activities,\"\u00a0explains Drudge.So, when a person dies and starts decomposing, all this phosphate leaks out into the environment around the corpse.If this environment happens to be wet and filled with iron \u2013 like that of White's coffin or \u00d6tzi's glacial tomb \u2013 the phosphate interacts with these other molecules to form the mineral vivianite",
        "abstract": "Vivianite is a bluish mineral sometimes encountered in archaeological deposits. It is notable for its tendency to change color from white or grayish to blue on exposure to air. Vivianite requires specific conditions for its formation\u2014sources of iron  phosphate  and water  as well as low levels of oxygen and sulfide. Microbial activity is also thought to play a part in vivianite formation. The majority of archaeological texts do not discuss vivianite to any great degree  preventing a more detailed interpretation of site conditions and features. Vivianite was found in 25 exhumed burials from the North Brisbane Burial Ground  Queensland  Australia. Research indicated that bone or tissue samples for DNA analysis are best taken from areas distant from vivianite encrustations and that presence of vivianite has implications for artifact conservation. Vivianite at the North Brisbane Burial Grounds helped protect some skeletal and dental elements  preserved the impressions of metal coffin lacing  and also corroborated the oral history of temporary waterlogging and acted as a measure of pollution levels across the site. \u00a9 2006 Wiley Periodicals  Inc.",
        "generated_article": "Title: Vivianite: The Blue Mineral Unveiling Archaeological Secrets in North Brisbane Burial Grounds\n\nIn a groundbreaking discovery at the North Brisbane Burial Ground in Queensland, Australia, researchers have unearthed the presence of vivianite, a bluish mineral that has provided valuable insights into the site's conditions and features. The findings, published in a recent study, shed light on the significance of vivianite in archaeological deposits and its implications for artifact conservation and DNA analysis.\n\nVivianite, known for its color-changing properties from white or grayish to blue upon exposure to air, requires specific conditions for its formation. These conditions include sources of iron phosphate, water, low levels of oxygen, and sulfide, with microbial activity also playing a role in its creation. Despite its importance, vivianite has been largely overlooked in archaeological texts, hindering a comprehensive interpretation of site conditions.\n\nThe study, which examined 25 exhumed burials at the North Brisbane Burial Ground, revealed that vivianite encrustations can impact the analysis of bone or tissue samples for DNA studies. Researchers recommend collecting samples from areas distant from vivianite to ensure accurate genetic analysis. Additionally, the presence of vivianite has implications for artifact conservation, as it can help protect skeletal and dental elements and preserve delicate features such as metal coffin lacing impressions.\n\nMoreover, vivianite at the North Brisbane Burial Ground acted as a historical marker, corroborating oral accounts of temporary waterlogging at the site. The mineral also served as a measure of pollution levels across the area, providing valuable information on environmental conditions during the burial period.\n\nThe discovery of vivianite at the North Brisbane Burial Ground highlights the importance of considering this mineral in archaeological studies. By recognizing the role of vivianite in site formation and preservation, researchers can enhance their understanding of past civilizations and improve conservation practices for artifacts and human remains. This study underscores the significance of interdisciplinary research in unraveling the mysteries of ancient sites and showcases the potential of minerals like vivianite in unlocking archaeological secrets."
    },
    {
        "id": "8512-0",
        "category": "environment",
        "annotation": "Just weeks after Hurricane Harvey caused destruction in Texas, Irma has made landfall in Florida - and there are still almost 12 weeks left of Atlantic hurricane season. It raises the question - where are all of these storms coming from?\nResearch has shown that most of the monster storms that hit the US and Canada start out as a distinct weather pattern in the atmosphere over western Africa, specifically a spot off the coast of the African Cape Verde\u00a0islands.In fact, a 2015 study published in Geophysical Research Letters\u00a0showed that by closely watching these tropical disturbances off the coast of western Africa, researchers could better predict which of them would turn into serious hurricanes a few weeks later.Google Maps\"85 percent of the most intense hurricanes affecting the US and Canada start off as disturbances in the atmosphere over western Africa,\" said lead researcher Colin Price from Tel Aviv University in Israel at the time.\n\"We found that the larger the area covered by the disturbances, the higher the chance they would develop into hurricanes only one to two weeks later.\"Interestingly, these hurricanes are directly linked to one of the driest places on Earth - the Sahara desert.The interaction between the hot dry air of the Sahara and the cooler, more humid air from the Gulf of Guinea to its South forms what's known as the African easterly jet, which blows from east to west across Africa.\u00a0Within this jet, atmospheric disturbances or bands of thunderstorm activity known as tropical waves can form. As they blow off the west coast of Africa past Cape Verde, the 2015 study showed that the amount of cloud coverage at that point can predict whether or not these tropical waves will become hurricanes a week or two later.\nHow does that happen? Tropical waves interact with the warm equatorial water of the Atlantic as they head west, triggering columns of warm moist air to rise from the ocean.That provides two of the three ingredients required for tropical storms to turn into full-blown hurricanes: moist air; Earth's rotation; and warm ocean temperatures. When the swirling winds reach speeds of 74 mph (119 km/h), the storm is classified as a Category 1 hurricane.Irma was first spotted as a tropical disturbance off the Cape Verde Islands in late August, before becoming a hurricane over the Atlantic as it made its way towards the Caribbean and US.According to Price, only 10 percent of the 60 disturbances originating in Africa every year turn into hurricanes - but the ones that do have the opportunity to gather energy as they cross the Atlantic, which makes them so powerful that they're more likely to hit the US and even Canada.\n\"Not all hurricanes tha",
        "abstract": "Since 85% of all major Atlantic hurricanes originate as thunderstorm clusters in equatorial Africa  we have investigated the connection between these African thunderstorms and the consequent development of these disturbances into tropical storms. We have analyzed Meteosat infrared cloud top temperature data to determine the areal coverage of cold cloud tops over a 6year period from 2005 to 2010. In addition  hurricane statistics from the same period (intensity  date of generation  location  and maximum winds) were obtained from the National Hurricane Center database. We first show that the areal coverage of cold clouds (with brightness temperatures Tb<\u221250\u00b0C) in tropical Africa is a good indicator of the monthly number of African Easterly Waves (AEWs) leaving the west coast of tropical Africa. Furthermore  the AEWs that develop into tropical storms have a significantly larger area covered by cold cloud tops compared with nondeveloping waves. Finally  we show that on a storm-by-storm basis  the cold cloud coverage in West Africa is positively correlated (r=0.57) with the accumulated cyclone energy of the future tropical cyclones that develop out of these waves.",
        "generated_article": "Title: African Thunderstorms Play Key Role in Atlantic Hurricane Formation, Study Finds\n\nA new study has shed light on the crucial role that thunderstorm clusters in equatorial Africa play in the development of tropical storms and hurricanes in the Atlantic Ocean. The research, conducted by a team of scientists analyzing data from 2005 to 2010, reveals a strong connection between African thunderstorms and the subsequent formation of tropical storms.\n\nThe study, which focused on the relationship between African thunderstorms and the development of tropical storms, utilized Meteosat infrared cloud top temperature data to assess the extent of cold cloud tops over a six-year period. The researchers also examined hurricane statistics from the same timeframe, including intensity, date of generation, location, and maximum winds, sourced from the National Hurricane Center database.\n\nThe findings indicate that a significant portion of major Atlantic hurricanes, approximately 85%, originate as thunderstorm clusters in equatorial Africa. The researchers observed that the areal coverage of cold clouds with brightness temperatures below -50\u00b0C in tropical Africa serves as a reliable indicator of the monthly number of African Easterly Waves (AEWs) moving from the west coast of the continent.\n\nMoreover, the study revealed that AEWs which evolve into tropical storms exhibit a notably larger area covered by cold cloud tops compared to waves that do not develop into storms. This distinction in cloud coverage serves as a predictive factor for the future intensity and energy of tropical cyclones originating from these waves.\n\nOn a storm-by-storm basis, the researchers found a positive correlation between the extent of cold cloud coverage in West Africa and the accumulated cyclone energy of the resulting tropical cyclones. This correlation, with a coefficient of 0.57, underscores the significance of African thunderstorms in influencing the intensity and development of tropical storms in the Atlantic basin.\n\nThe study's findings offer valuable insights into the complex interplay between African thunderstorms and the genesis of tropical storms and hurricanes in the Atlantic Ocean. By elucidating the connection between these meteorological phenomena, the research contributes to a better understanding of the factors driving hurricane formation and intensity, ultimately aiding in improved forecasting and preparedness efforts for these potentially devastating natural events."
    },
    {
        "id": "7484-0",
        "category": "environment",
        "annotation": "Even as the Trump administration weighs withdrawing the\u00a0United States from the Paris climate agreement, a\u00a0new scientific paper\u00a0has documented growing fluxes of greenhouse gases streaming into the air from the Alaskan tundra, a long-feared\u00a0occurrence that could worsen climate change.\nThe new study, published in the Proceedings of the National Academy of Sciences, suggests that frozen northern soils - often called permafrost - are unleashing\u00a0an increasing amount of carbon dioxide into the air as they thaw in summer or subsequently fail to refreeze as they once did, particularly in late fall and early winter.\"Over a large area, we're seeing a substantial increase in the amount of CO2 that's coming out in the fall,\" said\u00a0Roisin Commane, a Harvard atmospheric scientist who is the lead author of the study.\u00a0The research was published by 19 authors from a variety\u00a0of institutions, including NASA's Jet Propulsion Laboratory and the National Oceanic and Atmospheric Administration.The study, based on aircraft measurements of carbon dioxide and methane\u00a0and tower measurements from Barrow, Alaska, found that from 2012 through 2014, the state emitted the equivalent of 220 million tons of carbon dioxide gas into the atmosphere from biological sources (the figure\u00a0excludes fossil fuel burning and wildfires).\nThat's an amount comparable to\u00a0all the emissions\u00a0from the US commercial sector in a single\u00a0year.The chief reason for the greater CO2 release was that as Alaska\u00a0has warmed up, emissions from once frozen tundra in winter are increasing\u00a0- presumably because the ground is not refreezing as quickly.\"The soils are warmer deeper, and as they freeze in the fall, the temperature of every soil depth has to come to zero before they hard freeze,\" Commane said.\"The temperature has to come to zero and equilibrate, for the soils to freeze hard through. And through that whole period you have emissions because the microbe are active.\"In particular, the research\u00a0found that since 1975, there has been a 73.4 percent increase in the amount of carbon lost from the Alaskan tundra in the months of October through December\u00a0as the climate warmed steadily.\nThe new study is \"the first to show that a large region of the Arctic is a carbon source and that this change is driven by increased carbon emissions during the winter,\" said Sue Natali, a permafrost researcher with the Woods Hole Research Center, who was not involved in the study.\"Because the models aren't capturing these cold-season processes, we're very likely underestimating carbon losses from the Arctic under current and future climate scenarios.\"The fears about permafrost carbon losses are based on some simple chemistry.Unlike at warmer latitudes, where microorganisms in the soil constantly break down plant matter and return the carbon it contains to the atmosphere, Arctic soils have been cold enough to preserve the frozen remains of ancient plant life.\u00a0But as the planet warms, soil microbes become able to break down more and more of this carbon, sending it back into the atmosphere and worsening global warming in a troubling feedback loop.\nSome scientists, however, held out hope that there would be a key offsetting process: As the Arctic warms, it might also stow away more carbon as it becomes greener and supports the additional plant life, particularly in tundra regions.This 'Arctic greening'\u00a0is indeed occurring, but the new research suggests that the permafrost losses in early winter are more than enough to offset that.\"There is greening going on, but it seems like you run out of the sunlight so far north, so it doesn't matter how much greening there is, eventually, the plants just run out of light,\" Commane said.\"It appears now that the microbes are winning.\"The\u00a0new study contrasts with\u00a0a\u00a02016 study\u00a0by the US Geological Survey, which had found that Alaska was acting as a net carbon \"sink\" at the moment - removing more carbon from the air than it is emitting - and that this should continue and expand\u00a0over the course of the century\u00a0as plant growth increases.\nOne\u00a0of the lead authors of that research, Dave McGuire of the University of Alaska at Fairbanks and USGS, said the new study is \"not the final word, but it is a significant step forward.\"McGuire pointed out that the new study looks at the years 2012 to 2014, whereas the 2016 USGS study looked at earlier years and ended in 2009, making an 'apples to apples' comparison difficult.Alaska\u00a0is only one area\u00a0of the Arctic where permafrost soils could be emitting carbon dioxide into the atmosphere. Permafrost regions in Canada and Siberia are even vaster.But the new\u00a0study's lessons could also apply to those areas, researchers say.The study \"shows that the Alaska region, which may be representative of large swaths of boreal forest and Arctic tundra biomes elsewhere, appear to be releasing net carbon to the atmosphere, in particular with stimulated emissions in the fall/early winter period,\" said Ted Schuur, an ecologist at Northern Arizona University, who\u00a0was not involved in the research",
        "abstract": "Rising arctic temperatures could mobilize reservoirs of soil organic carbon trapped in permafrost. We present the first quantitative evidence for large  regional-scale early winter respiration flux  which more than offsets carbon uptake in summer in the Arctic. Data from the National Oceanic and Atmospheric Administration\u2019s Barrow station indicate that October through December emissions of CO2 from surrounding tundra increased by 73% since 1975  supporting the view that rising temperatures have made Arctic ecosystems a net source of CO2. It has been known for over 50 y that tundra soils remain unfrozen and biologically active in early winter  yet many Earth System Models do not correctly represent this phenomenon or the associated CO2 emissions  and hence they underestimate current  and likely future  CO2 emissions under climate change. Abstract: High-latitude ecosystems have the capacity to release large amounts of carbon dioxide (CO2) to the atmosphere in response to increasing temperatures  representing a potentially significant positive feedback within the climate system. Here  we combine aircraft and tower observations of atmospheric CO2 with remote sensing data and meteorological products to derive temporally and spatially resolved year-round CO2 fluxes across Alaska during 2012\u20132014. We find that tundra ecosystems were a net source of CO2 to the atmosphere annually  with especially high rates of respiration during early winter (October through December). Long-term records at Barrow  AK  suggest that CO2 emission rates from North Slope tundra have increased during the October through December period by 73% \u00b1 11% since 1975  and are correlated with rising summer temperatures. Together  these results imply increasing early winter respiration and net annual emission of CO2 in Alaska  in response to climate warming. Our results provide evidence that the decadal-scale increase in the amplitude of the CO2 seasonal cycle may be linked with increasing biogenic emissions in the Arctic  following the growing season. Early winter respiration was not well simulated by the Earth System Models used to forecast future carbon fluxes in recent climate assessments. Therefore  these assessments may underestimate the carbon release from Arctic soils in response to a warming climate.",
        "generated_article": "**Article Title: Arctic Tundra Emissions of CO2 on the Rise, Signaling Climate Change Concerns**\n\nIn a groundbreaking study, researchers have uncovered alarming evidence of rising carbon dioxide (CO2) emissions from Arctic tundra ecosystems, particularly during the early winter months. The findings, published in a recent scientific abstract, shed light on the significant impact of increasing temperatures on the release of stored soil organic carbon in permafrost regions.\n\nThe study, which combined aircraft and tower observations with remote sensing data and meteorological products, revealed that tundra ecosystems in Alaska have become a net source of CO2 to the atmosphere, with a notable surge in respiration rates during the period from October to December. Data from the National Oceanic and Atmospheric Administration's Barrow station indicated a staggering 73% increase in CO2 emissions from the surrounding tundra since 1975, underscoring the escalating impact of rising temperatures on Arctic ecosystems.\n\nThe research highlights a concerning trend of escalating early winter respiration and net annual CO2 emissions in Alaska, driven by climate warming. The study's lead author emphasized that the findings provide crucial evidence of a decadal-scale increase in the amplitude of the CO2 seasonal cycle, potentially linked to growing biogenic emissions in the Arctic following the summer season.\n\nMoreover, the study revealed a significant discrepancy between observed CO2 emissions and the predictions of Earth System Models used in climate assessments. The models failed to accurately simulate early winter respiration and its associated CO2 emissions, suggesting a systematic underestimation of carbon release from Arctic soils in response to a warming climate.\n\nThe implications of these findings are profound, as the Arctic region plays a critical role in the global carbon cycle. The escalating CO2 emissions from tundra ecosystems could trigger a positive feedback loop within the climate system, exacerbating the impacts of climate change. The study underscores the urgent need for improved modeling of Arctic carbon fluxes and a more comprehensive understanding of the complex interactions between rising temperatures and carbon release from permafrost regions.\n\nAs the Arctic continues to warm at an unprecedented rate, the study serves as a stark reminder of the pressing need for proactive measures to mitigate the escalating carbon emissions from high-latitude ecosystems. The findings underscore the critical importance of addressing the impacts of climate change on Arctic ecosystems and highlight the urgent need for enhanced monitoring and modeling efforts to accurately assess and predict the future trajectory of carbon fluxes in the region."
    },
    {
        "id": "8014-0",
        "category": "environment",
        "annotation": "Scientists have created an inexpensive material that removes a highly toxic industrial pollutant from water.The filtration material is made by turning a naturally occurring sugar molecule into a polymer and it performs much better than our current filtration technology at dealing with one big contamination problem.\nIndustrial pollutants cause major issues for communities near industrial sites. In the US, Hoosick Falls, New York, and Bennington, Vermont, declared states of emergency because of chemical contamination of drinking water\u00a0in 2016.The main culprit? Perfluorooctanoic acid \u2013 which can be shortened to the acronym PFOA.PFOA is a molecule that has been used historically in the production of our non-stick friend, TEFLON, and as a water and oil repellent for carpets, food wrapping and even dental floss.But there's a serious price to pay for our stain free and non-stick existence.PFOA is toxic to living things and, once released into the environment, it will never break down. This means removing PFOA from the contaminated waterways is the only course of action we have to improve the water quality.\nScientists have created a new polymer material constructed from a simple sugar molecule, beta-cyclodextrin. The material is ten times more efficient at removing PFOA from water than commonly used filtration materials such as activated carbon (the same stuff now used in fart absorbing underwear).\"Our material fully extracts the pollutant out of water. The polymer contains sites that bind PFOA strongly, which strips this pollutant out of water even when present at extremely low concentrations\" said lead researcher William Dichtel from Northwestern University, Illinois.Beta-cyclodextrin\u00a0is a naturally occurring bio-renewable sugar molecule derived from cornstarch. The cyclodextrin is transformed into a polymer by connecting the molecules together with another molecule \u2013 known as cross linking. Perfecting the ratio of beta-cyclodextrin to cross-linker was the key to maximising the filtration efficiency.As the name suggests, cyclodextrins are made up of sugar molecules bound together in a ring. The ring-like nature of beta-cyclodextrin creates a molecular bucket that's the perfect size for capturing and holding on to the PFOA molecules.Credit: American Chemical SocietyTargeting for other pollutants can be programmed into the material by changing the cyclodextrin for one with bigger or smaller cavities.\n\"Our findings demonstrate the selectivity of this type of polymer can be tailored to target pollutants of interest, in this case PFOA,\"\u00a0said Dichtel.Even very small amounts of the beta-cyclodextrin polymer can reduce the levels of PFOA to below the environmental protection agency's advised limit of 70 parts per trillion, which is equal to one teaspoon of PFOA in 14 Olympic-sized swimming pools.In the study, the scientists were able to take contaminated water with PFOA from a concentration of one milligram per litre (similar to that of contaminated water resources) to concentrations as low as 10 nanograms per litre \u2013 far below the Environmental Protection Agency's advised limits.The best performing polymers made could remove up to 95 percent of the PFOA within 13.5 hours.To make the polymer even better, it was able to be used multiple times by simply washing it with methanol at room temperature. Importantly, it was also resilient to humic acid, a component of natural organic matter which is commonly found in waterways and stops activated carbon filters from working.The scientists are hoping to turn this material into a commercial product very soon and have founded a start-up to manufacture and distribute the material.\u00a0The findings were published in the\u00a0Journal of the American Chemical Societ",
        "abstract": "Per- and poly fluorinated alkyl substances (PFASs)  notably perfluorooctanoic acid (PFOA)  contaminate many ground and surface waters and are environmentally persistent. The performance limitations of existing remediation methods motivate efforts to develop effective adsorbents. Here we report a \u03b2-cyclodextrin (\u03b2-CD)-based polymer network with higher affinity for PFOA compared to powdered activated carbon  along with comparable capacity and kinetics. The \u03b2-CD polymer reduces PFOA concentrations from 1 \u03bcg L\u20131 to <10 ng L\u20131  at least 7 times lower than the 2016 U.S. EPA advisory level (70 ng L\u20131)  and was regenerated and reused multiple times by washing with MeOH. The performance of the polymer is unaffected by humic acid  a component of natural organic matter that fouls activated carbons. These results are promising for treating PFOA-contaminated water and demonstrate the versatility of \u03b2-CD-based adsorbents.",
        "generated_article": "Title: Breakthrough in Water Contamination Remediation: \u03b2-Cyclodextrin Polymer Shows Promise in Removing Harmful PFASs\n\nIn a groundbreaking development, researchers have unveiled a novel \u03b2-cyclodextrin (\u03b2-CD)-based polymer network that demonstrates exceptional efficiency in removing perfluorooctanoic acid (PFOA) from contaminated water sources. The study, published in a recent scientific journal, highlights the potential of this innovative adsorbent in addressing the pervasive issue of PFAS contamination in ground and surface waters.\n\nPer- and polyfluorinated alkyl substances (PFASs), particularly PFOA, have become a significant environmental concern due to their widespread presence and persistence in the environment. Traditional remediation methods have faced limitations in effectively tackling PFAS contamination, underscoring the need for more efficient solutions.\n\nThe research team's findings reveal that the \u03b2-CD-based polymer network exhibits a higher affinity for PFOA compared to powdered activated carbon, a commonly used adsorbent. Notably, the \u03b2-CD polymer demonstrated comparable capacity and kinetics in removing PFOA from water, reducing concentrations from 1 \u03bcg L\u20131 to less than 10 ng L\u20131. This achievement represents a significant advancement, with the treated water containing at least 7 times lower PFOA levels than the 2016 U.S. EPA advisory limit of 70 ng L\u20131.\n\nOne of the key advantages of the \u03b2-CD polymer is its ability to be regenerated and reused multiple times through a simple washing process with methanol (MeOH). This feature not only enhances the sustainability of the remediation process but also underscores the economic viability of the technology.\n\nMoreover, the study demonstrated that the performance of the \u03b2-CD polymer remained unaffected by the presence of humic acid, a component of natural organic matter that can impede the efficacy of traditional activated carbons. This resilience to fouling by organic compounds further enhances the potential applicability of the \u03b2-CD-based adsorbent in real-world water treatment scenarios.\n\nThe promising results obtained from this study offer a ray of hope in the ongoing battle against PFAS contamination. The versatility and effectiveness of the \u03b2-CD polymer in removing PFOA underscore its potential as a valuable tool in remediation efforts targeting PFAS-contaminated water sources.\n\nAs researchers continue to explore and refine innovative solutions for environmental challenges, the \u03b2-CD-based polymer network stands out as a promising candidate for addressing PFAS contamination and advancing sustainable water treatment technologies. This breakthrough paves the way for further research and development in the field of water remediation, offering a glimmer of optimism in the quest for cleaner and safer water resources."
    },
    {
        "id": "8915-0",
        "category": "nature",
        "annotation": "There seems to be significant confusion about what happened in the British parliament when MPs discussed a proposed amendment to the EU (Withdrawal) Bill to formally recognise animal sentience. But where science is concerned, animal sentience is in no doubt.\nThe definition of sentient is simply \"able to perceive or feel things\". Today most of us would probably also say that animals are able to feel emotion, form attachments and have distinct personalities. Yet for many decades the idea of animals feeling emotions or having personalities was dismissed by behavioural scientists.This strange view that arose from the 17th century philosopher Ren\u00e9 Descartes' alleged assertion that animals are without feelings, physical or emotional.Recent work has debunked this idea (whether or not Descartes actually said it). If any mammal appears to be free of emotions, apart perhaps from cynicism, it would be the goat. Yet scientists have been able to show that goats become emotionally aroused in response to various test situations, and whether these emotions are positive or negative.The researchers analysed the calls the goats made when they were expecting food, when they were frustrated because a food reward didn't arrive and when they were isolated from their herd mates.\nThey also used the goats' body language and heart rate to calibrate their assessment of the emotions expressed in the calls, as analysed using the frequency of the sounds.Horses are a bundle of emotions. This is not surprising, given that they are very social animals, with a close relationship with others in their herds and are also prey animals whose response to threat is to run away as fast as possible.In Canada, horse riding is reckoned to be one of the most dangerous sports, ahead of motor racing and skiing, and the emotional state of the horse is an important aspect of the safety or otherwise of the rider.Researchers in France looked at the level of emotion and the ability to learn shown by 184 horses from 22 different riding schools.The ability of a horse to be fairly calm in the face of a novel situation, and to learn quickly that a new object or situation is not threatening, is crucial when riding. So the researchers concentrated on these aspects of horse emotion.\nThey found that one of the most important influences on how emotional horses are is the way that they are housed. Horses that were kept outside in a field were likely to be less fearful of a new object and to respond with less excitement to being loose in an arena than horses that were housed individually in boxes.While the result is not surprising, the study emphasises the fact that horses are capable of emotions such as anxiety and fear.Another vexed question, in the early part of the 20th century at least, was whether or not animals have personalities. It is now generally accepted that they do, and that those personalities are capable of as much variation as human personalities.Perhaps the most surprising aspect of this area of study is that personality is discernible even in fish, which are often seen as being singularly lacking in emotional range. Scientists have found that the personality type of a fish may affect its likelihood of having certain parasites, or its ability to move past a barrier in a stream when on migration.\nWhy it mattersThe reason that all these studies and the many others into animal emotions, personality and ability to feel pain, fear and stress, are important is the huge implications for animal welfare.Whether or not the law recognises animals as being sentient, those animals will still feel afraid, fail to cope or suffer pain during transport and slaughter, as well as in everyday situations.It is difficult to reduce the fear and stress endured by animals going to slaughter, or being used in sport, entertainment or as companions. But it is likely to be even more difficult if the law does not recognise animals as sentient beings, to whose welfare we should pay full regard.Slaughter house personnel are perceived as being somewhat rough in handling the animals under their care, in spite of repeated training. Unless animal sentience continues to be recognised in law, it will be even more difficult to deal with people who compromise animal welfare.Jan Hoole, Lecturer in Biology, Keele UniversityThis article was originally published by The Conversation. Read the original article",
        "abstract": "Animal personalities are an important factor that affects the dispersal of animals. In the context of aquatic species  dispersal modeling needs to consider that most freshwater ecosystems are highly fragmented by barriers reducing longitudinal connectivity. Previous research has incorporated such barriers into dispersal models under the neutral assumption that all migrating animals attempt to ascend at all times. Modeling dispersal of animals that do not perform trophic or reproductive migrations will be more realistic if it includes assumptions of which individuals attempt to overcome a barrier. We aimed to introduce personality into predictive modeling of whether a nonmigratory invasive freshwater fish (the round goby  Neogobius melanostomus) will disperse across an in-stream barrier. To that end  we experimentally assayed the personalities of 259 individuals from invasion fronts and established round goby populations. Based on the population differences in boldness  asociability  and activity  we defined a priori thresholds with bolder  more asocial  and more active individuals having a higher likelihood of ascent. We then combined the personality thresholds with swimming speed data from the literature and in situ measurements of flow velocities in the barrier. The resulting binary logistic regression model revealed probabilities of crossing a barrier which depended not only on water flow and fish swimming speed but also on animal personalities. We conclude that risk assessment through predictive dispersal modeling across fragmented landscapes can be advanced by including personality traits as parameters. The inclusion of behavior into modeling the spread of invasive species can help to improve the accuracy of risk assessments.",
        "generated_article": "Title: Personality Traits Influence Dispersal of Invasive Freshwater Fish Across Barriers\n\nIn a groundbreaking study, researchers have found that the personalities of animals play a crucial role in their dispersal across fragmented landscapes. The study, focusing on the invasive freshwater fish species, the round goby (Neogobius melanostomus), sheds light on how individual traits can impact the movement of species across barriers in aquatic ecosystems.\n\nAnimal personalities have long been recognized as a key factor influencing various aspects of behavior and ecology. However, their role in dispersal modeling, especially in the context of aquatic species facing fragmented habitats, has been relatively understudied. Most freshwater ecosystems are highly fragmented by barriers that impede the movement of species, affecting their ability to disperse and colonize new areas.\n\nTraditionally, dispersal models have assumed that all migrating animals attempt to ascend barriers at all times. However, this neutral assumption may not accurately reflect the behavior of all individuals within a population, particularly in the case of nonmigratory species like the round goby. To address this gap, the researchers aimed to incorporate personality traits into predictive modeling to determine whether these fish would disperse across in-stream barriers.\n\nThe study involved experimental assays of 259 round goby individuals from invasion fronts and established populations to assess their personalities. By measuring traits such as boldness, associability, and activity levels, the researchers identified thresholds that indicated which individuals were more likely to attempt to overcome a barrier. Individuals that were bolder, more asocial, and more active were found to have a higher likelihood of ascending the barrier.\n\nBy combining these personality thresholds with swimming speed data and flow velocities in the barrier, the researchers developed a binary logistic regression model to predict the probability of round gobies crossing the barrier. The results showed that the likelihood of dispersal across barriers was not only influenced by water flow and swimming speed but also by the individual personalities of the fish.\n\nThe findings of this study have significant implications for risk assessment and management strategies related to invasive species. By incorporating behavior and personality traits into dispersal models, researchers can improve the accuracy of predicting the spread of invasive species across fragmented landscapes. Understanding how individual traits influence dispersal can help conservationists and policymakers develop more targeted and effective strategies to mitigate the impacts of invasive species on native ecosystems.\n\nOverall, this research highlights the importance of considering animal personalities in ecological studies and underscores the need to integrate behavioral traits into predictive modeling to better understand and manage species dispersal in fragmented habitats."
    },
    {
        "id": "4683-0",
        "category": "space",
        "annotation": "The Astrophysical Journal Located about 3,700 light-years away, in the direction of the Cygnus constellation, lies a distant planet called Kepler\u20131647b, which at approximately 4.4 billion years of age is roughly as old as Earth.\nBut that's about where the similarities end, because Kepler\u20131647b turns out to hold a pretty sensational point of difference: this newly identified world is the largest planet ever discovered that orbits two stars. Known as circumbinary planets, these strange cosmic phenomena are often called 'Tatooine' planets, after Luke Skywalker's dusty desert home.An international team of astronomers led by NASA's Goddard Space Flight Centre just announced the discovery of Kepler\u20131647b, which has a mass and radius that are nearly identical to Jupiter's. But just because this circumbinary planet might be the largest of its kind that we currently know about, that didn't make locating it any easier.\"[F]inding circumbinary planets is much harder than finding planets around single stars,\" said astronomer William Welsh from San Diego State University (SDSU). \"The transits are not regularly spaced in time and they can vary in duration and even depth.\"The team identified the planet using data recorded by the Kepler space observatory, and adding to the challenge was Kepler\u20131647b's other remarkable feature \u2013 the immense length of its orbital period. At 1,107 days, this is the longest of any confirmed transiting exoplanet astronomers have yet found.Artist's impression. Credit: Lynette CookThis means it takes over three years for Kepler\u20131647b to orbit its host stars - both of which are similar in size to our Sun, with one being slightly smaller, and the other being slightly larger.\nThat epic orbital period is the result of Kepler\u20131647b being located further away from its host stars than other circumbinary planets, which usually hold relatively tight orbits. In the image below, you can see just how much wider Kepler\u20131647b's orbit (in red) is compared to its circumbinary counterparts (in grey), with Earth's orbit (in blue) added in for reference.\"It's a bit curious that this biggest planet took so long to confirm, since it is easier to find big planets than small ones,\" said SDSU astronomer Jerome Orosz. \"But it is because its orbital period is so long.\"Comparison of the relative sizes of several Kepler circumbinary planets. Credit: Lynette CookEvidence for Kepler\u20131647b's existence was first found back in 2011, when one of the team, Laurance Doyle from the SETI Institute, spotted the planet transit in front of one of its stars. But it took several extra years of analysis and observations \u2013 supported by amateur astronomers using the Kilodegree Extremely Little Telescopes in the US and South Africa \u2013 before the researchers confirmed that Kepler\u20131647b was a circumbinary planet.\nWhile Kepler\u20131647b's distance from its stars is greater than usual for a circumbinary planet, it's actually within the habitable zone \u2013 the distance from a star at which liquid water could exist on a planet's surface, not being so far away that it would freeze, nor so close that it would evaporate.But, in this case, since Kepler\u20131647b is a gas giant like Jupiter, the researchers think it's unlikely to host life \u2013 although there's a possibility any as-yet-undiscovered moons orbiting the planet could provide a suitable environment for it. Sadly, this means there's probably nobody looking up to see a beautiful two-star sunset like the one Luke Skywalker watches in Star Wars.Comparison of the orbits of the Kepler circumbinary planets. Kepler\u20131647 b's orbit is shown in red. Other planets are shown in grey. For comparison, the Earth's orbit is shown in blue. Credits: B. QuarlesBut just because we probably won't find alien life on Kepler\u20131647b doesn't mean the planet isn't a major find. In addition to its record-breaking size and orbital length, the scientists think this Tatooine-esque planet could herald the discovery of a new generation of similar worlds that have eluded us until now.\nWhen you consider that the first such planet \u2013 Kepler\u201316b \u2013 was only discovered five years ago, it becomes clear why this is such an exciting time for astronomy. Kepler\u20131647b is the 11th confirmed circumbinary planet discovered by the Kepler telescope since then.\"Habitability aside, Kepler\u20131647b is important because it is the tip of the iceberg of a theoretically predicted population of large, long-period circumbinary planets,\" said Welsh.The findings have been accepted for publication in The Astrophysical Journa",
        "abstract": "We report the discovery of a new Kepler transiting circumbinary planet (CBP). This latest addition to the still-small family of CBPs defies the current trend of known short-period planets orbiting near the stability limit of binary stars. Unlike the previous discoveries  the planet revolving around the eclipsing binary system Kepler-1647 has a very long orbital period (\u223c 1100 days) and was at conjunction only twice during the Kepler mission lifetime. Due to the singular configuration of the system  Kepler1647b is not only the longest-period transiting CBP at the time of writing  but also one of the longest-period transiting planets. With a radius of 1.06\u00b10.01 RJup it is also the largest CBP to date. The planet produced three transits in the light-curve of Kepler1647 (one of them during an eclipse  creating a syzygy) and measurably perturbed the times of the stellar eclipses  allowing us to measure its mass to be 1.52 \u00b1 0.65 MJup. The planet revolves around an 11-day period eclipsing binary consisting of two Solarmass stars on a slightly inclined  mildly eccentric (ebin = 0.16)  spin-synchronized orbit. Despite having an orbital period three times longer than Earth\u2019s  Kepler-1647b is in the conservative habitable zone of the binary star throughout its orbit.",
        "generated_article": "Title: New Kepler Transiting Circumbinary Planet Kepler-1647b Challenges Current Trends in Exoplanet Discoveries\n\nIn a groundbreaking discovery, astronomers have identified a new Kepler transiting circumbinary planet (CBP) that is defying conventional wisdom in the realm of exoplanet research. The newly found planet, named Kepler-1647b, has captured the attention of the scientific community due to its unique characteristics that set it apart from previously known short-period planets orbiting binary star systems.\n\nUnlike its counterparts, Kepler-1647b boasts an exceptionally long orbital period of approximately 1100 days, making it one of the longest-period transiting planets observed to date. This remarkable feature places it in a league of its own within the still-small family of CBPs. Furthermore, with a radius of 1.06 Jupiter radii, Kepler-1647b also holds the distinction of being the largest CBP discovered thus far.\n\nThe discovery of Kepler-1647b was made possible through observations of the eclipsing binary system Kepler-1647, where the planet was found to produce three transits in the light-curve. One of these transits occurred during an eclipse, resulting in a rare celestial alignment known as a syzygy. By analyzing the perturbations in the timing of the stellar eclipses caused by the planet, researchers were able to estimate its mass to be approximately 1.52 Jupiter masses.\n\nKepler-1647b orbits a binary star system composed of two Solar-mass stars on a slightly inclined, mildly eccentric orbit that is spin-synchronized. Despite its orbital period being three times longer than that of Earth, Kepler-1647b falls within the conservative habitable zone of the binary star throughout its journey. This positioning raises intriguing questions about the potential habitability of such a distant and unique exoplanet.\n\nThe discovery of Kepler-1647b challenges existing paradigms in exoplanet research by showcasing the diversity and complexity of planetary systems beyond our solar system. As astronomers continue to unravel the mysteries of the universe, the findings from this study open up new avenues for exploring the rich tapestry of exoplanetary landscapes and the potential for discovering worlds that push the boundaries of our understanding of planetary formation and evolution."
    },
    {
        "id": "4250-0",
        "category": "nature",
        "annotation": "When it comes to creatures that threaten our existence, sharks get a pretty bad rap (thanks, Jaws).In 2015,\u00a0we experienced the highest amount of unprovoked shark\u00a0attacks ever, with 99 reported cases\u00a0around the globe, and in\u00a02014, that number was 72.\u00a0Though this means sharks do attack and kill people on occasion, the numbers don't even compare to the estimated 25,000 deaths that dogs\u00a0cause every year (mostly because of rabies).\nMany people view sharks as monsters because they're scary-looking predators that live underwater - an environment where\u00a0humans already feel extremely vulnerable - but instead of thinking of them as hidden death machines,\u00a0we should see them for what they really are: complex creatures that\u00a0have different personalities, just like us.For the first time, researchers from Macquarie University in Sydney, Australia have found that Port Jackson sharks have individual personalities that they express habitually, suggesting that they're ingrained just like our own personalities are.In the study, the team designed a set of experiments to test how bold several of their sharks were.In the first experiment, they introduced sharks to a new tank that had a shelter inside. They monitored how long it took for the shark to be brave enough to venture out of the shelter and explore its new surroundings.\u00a0A second experiment had a handler pick up the sharks and release them back into the tank.\nBoth of these activities - handling the shark and introducing it to a strange environment - served to stress the sharks out a bit so the team could see how long it took them to calm down.The team found that each shark handled these situations in their own way, and continued to react in a similar manner each time,\u00a0with\u00a0some of the sharks appearing more bold than others. Though simplistic, the findings suggest that some sharks are, in essence, more outgoing and adventurous than others, signalling that they have different personality traits.\"We are excited about these results because they demonstrate that sharks are not just mindless machines. Just like humans, each shark is an individual with its unique preferences and behaviours,\" said one of the team, Culum Brown.Besides showing sharks in a new light, the team says that knowing the breadth of responses displayed by these creatures to certain situations\u00a0will help us better interpret - and maybe even predict - how certain types of predators behave around humans.\n\"Understanding how personality influences variation in shark behaviour - such as prey choice, habitat use and activity levels - is critical to better managing these top predators that play important ecological roles in marine ecosystems,\"\u00a0says Brown.The study has been published in the Journal of Fish Biology, and adds to a growing body of research that shows we have a whole lot more to learn about the animals we share the planet with, and it's in our best interest to get to know them better.\"Over the past few decades, personality research has shown that nearly 200 species of animals demonstrate individual personality,\" says lead researcher,\u00a0Evan Byrnes. \"Personality is no longer considered a strictly human characteristic - rather, it is a characteristic deeply engrained in our evolutionary past.",
        "abstract": "This study examined interindividual personality differences between Port Jackson sharks Heterodontus portusjacksoni utilizing a standard boldness assay. Additionally  the correlation between differences in individual boldness and stress reactivity was examined  exploring indications of individual coping styles. Heterodontus portusjacksoni demonstrated highly repeatable individual differences in boldness and stress reactivity. Individual boldness scores were highly repeatable across four trials such that individuals that were the fastest to emerge in the first trial were also the fastest to emerge in subsequent trials. Additionally  individuals that were the most reactive to a handling stressor in the first trial were also the most reactive in a second trial. The strong link between boldness and stress response commonly found in teleosts was also evident in this study  providing evidence of proactive-reactive coping styles in H. portusjacksoni. These results demonstrate the presence of individual personality differences in sharks for the first time. Understanding how personality influences variation in elasmobranch behaviour such as prey choice  habitat use and activity levels is critical to better managing these top predators which play important ecological roles in marine ecosystems.",
        "generated_article": "Title: Study Reveals Individual Personality Differences in Port Jackson Sharks\n\nIn a groundbreaking study, researchers have uncovered individual personality differences in Port Jackson sharks (Heterodontus portusjacksoni) for the first time. The study, which utilized a standard boldness assay to assess the sharks' behavior, shed light on the correlation between individual boldness and stress reactivity, providing insights into the sharks' coping styles.\n\nThe research, conducted by a team of scientists, found that Port Jackson sharks exhibited highly repeatable individual differences in both boldness and stress reactivity. Individual boldness scores remained consistent across multiple trials, indicating that sharks that were quick to emerge in the first trial also displayed the same behavior in subsequent trials. Similarly, sharks that showed high reactivity to a handling stressor in the first trial exhibited the same response in a second trial.\n\nThe study also revealed a strong link between boldness and stress response in Port Jackson sharks, mirroring findings in other teleosts. This connection between boldness and stress reactivity provided evidence of proactive-reactive coping styles in these sharks, highlighting the complexity of their behavior.\n\nUnderstanding how individual personality traits influence variation in elasmobranch behavior, such as prey choice, habitat use, and activity levels, is crucial for effectively managing these top predators. Port Jackson sharks play vital ecological roles in marine ecosystems, and gaining insights into their behavior can aid in conservation efforts and ecosystem management.\n\nThese findings open up new avenues for research into the behavior and ecology of sharks, emphasizing the importance of considering individual differences in understanding their role in marine ecosystems. By unraveling the mysteries of shark personality, scientists can better protect these fascinating creatures and the delicate balance of the ocean environment."
    },
    {
        "id": "4704-0",
        "category": "space",
        "annotation": "The Astrophysical Journal A likely new planet that might be one of our galaxy's youngest could also turn out to be unique, say astronomers, but that uniqueness appears to come at a price.Called PTFO8\u20138695 b, this newly discovered planet candidate orbits a star 1,100 light-years from Earth, but the newborn's proximity to its host star puts it in a dangerous situation. With an extremely close orbit that only takes 11 hours to complete, this 'hot Jupiter' \u2013 a term for hot planets with large masses and short orbital periods \u2013 looks to be engaged in a drawn-out death spiral.\nResearchers think PTFO8\u20138695 b is slowly losing its outer layers, which are being ripped away over time by the gravity pull of its nearby star.\"A handful of known planets are in similarly small orbits, but because this star is only 2 million years old, this is one of the most extreme examples,\" said astronomer Christopher Johns-Krull from Rice University.While the object's status as a planet has yet to be scientifically confirmed, the relative youth of PTFO8\u20138695 b, along with its unfortunate predicament, suggests that we're looking at something special. At least for as long as PTFO8\u20138695 b manages to hold out.Artist's impression. Credit: A. Passwaters/Rice University\"We don't yet have absolute proof this is a planet because we don't yet have a firm measure of the planet's mass, but our observations go a long way toward verifying this really is a planet,\" said Johns-Krull. \"We compared our evidence against every other scenario we could imagine, and the weight of the evidence suggests this is one of the youngest planets yet observed.\"\nAnd that list of planets observed is growing all the time. Scientists have currently discovered some 3,432 exoplanets (planets outside our Solar System), with that number receiving a massive bump last month, when NASA officially announced the discovery of 1,284 new alien worlds.The majority of these exoplanets orbit comparatively middle-aged stars \u2013 like our Sun, which is thought to be 4.5 billion years old. In contrast, PTFO8\u20138695's 2 million years or so make it something of a cosmic infant. But while young stars and their planets offer a valuable research subject for scientists, they're not always easy to study \u2013 or even find.According to Johns-Krull, there aren't many young stars that we know about that shine brightly enough for us to observe in detail with today's telescopes. And because young stars are also relatively active \u2013 with strong magnetic fields, and producing frequent visual outbursts and dimmings \u2013 it makes it harder for researchers to accurately gauge whether they're orbited by planets.PTFO8\u20138695 b was identified in 2012 by an international survey called the Palomar Transit Factory, but even though we've only known about it for a very short time, the jury's out on how long exactly this baby (maybe) planet has left.\"We don't know the ultimate fate of this planet,\" said Johns-Krull. \"It likely formed farther away from the star and has migrated in to a point where it's being destroyed. We know there are close-orbiting planets around middle-aged stars that are presumably in stable orbits. What we don't know is how quickly this young planet is going to lose its mass and whether it will lose too much to survive.\"The findings have been accepted for publication in The Astrophysical Journa",
        "abstract": "We use high time cadence  high spectral resolution optical observations to detect excess H-alpha emission from the 2 - 3 Myr old weak lined T Tauri star PTFO8-8695. This excess emission appears to move in velocity as expected if it were produced by the suspected planetary companion to this young star. The excess emission is not always present  but when it is  the predicted velocity motion is often observed. We have considered the possibility that the observed excess emission is produced by stellar activity (flares)  accretion from a disk  or a planetary companion; we find the planetary companion to be the most likely explanation. If this is the case  the strength of the H-alpha line indicates that the emission comes from an extended volume around the planet  likely fed by mass loss from the planet which is expected to be overflowing its Roche lobe.",
        "generated_article": "Title: Astronomers Detect Possible Planetary Companion Around Young Star\n\nIn a groundbreaking discovery, astronomers have detected excess H-alpha emission from the weak-lined T Tauri star PTFO8-8695, suggesting the presence of a suspected planetary companion orbiting the young star. The findings, based on high time cadence and high spectral resolution optical observations, provide compelling evidence of a planetary body influencing the star's emission properties.\n\nThe star PTFO8-8695, estimated to be 2-3 million years old, exhibits varying levels of excess H-alpha emission, which appears to move in velocity in a manner consistent with the presence of a planetary companion. While the excess emission is not always present, when detected, it aligns with the predicted velocity motion expected from a planetary body orbiting the star.\n\nResearchers explored several potential explanations for the observed excess emission, including stellar activity such as flares, accretion from a disk, or the presence of a planetary companion. After careful analysis, the team concluded that the planetary companion scenario is the most plausible explanation for the observed phenomena.\n\nIf the excess H-alpha emission is indeed linked to a planetary companion, the strength of the emission line suggests that it originates from an extended volume surrounding the planet. This emission is likely fueled by mass loss from the planet, indicating that it may be overflowing its Roche lobe, a critical boundary that defines the region where a celestial body's gravitational pull exceeds its own self-gravity.\n\nThe discovery of a potential planetary companion around PTFO8-8695 opens up new avenues for studying the formation and evolution of planetary systems. By leveraging advanced observational techniques, astronomers continue to unravel the mysteries of young stars and their planetary companions, shedding light on the complex processes shaping our cosmic neighborhood.\n\nFurther research and follow-up observations will be crucial to confirm the presence of the planetary companion and deepen our understanding of the dynamic interactions between young stars and their orbiting bodies. The study represents a significant step forward in exoplanet research and highlights the ongoing quest to explore the diversity of planetary systems beyond our solar system."
    },
    {
        "id": "3135-0",
        "category": "environment",
        "annotation": "This article was written by Emma Stone and Alex Farnsworth from the University of Bristol, and was originally published by The Conversation.It's official: 2015 was the warmest year on record. But those global temperature records only date back to 1850 and become increasingly uncertain the further back you go. Beyond then, we're reliant on signs left behind in tree rings, ice cores or rocks. So when was Earth last warmer than the present?\nThe Medieval Warm Period is often cited as the answer. This spell, beginning in roughly 950AD and lasting for three centuries, saw major changes to population centres across the globe. This included the collapse of the Tiwanaku civilisation in South America due to increased aridity, and the colonisation of Greenland by the Vikings.But that doesn't tell the whole story. Yes, some regions were warmer than in recent years, but others were substantially colder. Across the globe, averaged temperatures then were in fact cooler than today.To reach a point when Earth was significantly warmer than today we'd need to go back 130,000 years, to a time known as the Eemian.\u00a0For about 1.8 million years the planet had fluctuated between a series of ice ages and warmer periods known as 'interglacials'. The Eemian, which lasted around 15,000 years, was the most recent of these interglacials (before the one we're currently in).Although global annual average temperatures were approximately 1 to 2\u02daC warmer than preindustrial levels, high latitude regions were several degrees warmer still. This meant ice caps melted, Greenland's ice sheet was reduced and the West Antarctic ice sheet may have collapsed. The sea level was at least 6 m higher than today.\nAcross Asia and North America forests extended much further north than today and straight-tusked elephants (now extinct) and hippopotamuses were living as far north as the British Isles.How do we know all this? Well, scientists can estimate the temperature changes at this time by looking at chemicals found in ice cores and marine sediment cores and studying pollen buried in layers deep underground. Certain isotopes of oxygen and hydrogen in ice cores can determine the temperature in the past while pollen tells us which plant species were present and therefore gives us an indication of climatic conditions suitable for that species.We know from air bubbles in ice cores drilled on Antarctica that greenhouse gas concentrations in the Eemian were not dissimilar to preindustrial levels. However orbital conditions were very different \u2013 essentially there were much larger latitudinal and seasonal variations in the amount of solar energy received by Earth.So although the Eemian was warmer than today the driving mechanism for this warmth was fundamentally different to present-day climate change, which is down to greenhouses gases. To find a warm period caused predominantly by conditions more similar to today, we need to go even further back in time.Glen Fergus/WikimediaAs climate scientists, we're particularly interested in the Miocene (around 23 to 5.3 million years ago), and in particular a spell known as the Miocene-Climate Optimum (11-17 million years ago). Around this time CO2\u00a0values (350-400 parts per million) were similar to today and it therefore potentially serves as an appropriate analogue for the future.\nDuring the Optimum, those carbon dioxide concentrations were the predominant driver of climate change. Global average temperatures were 2 to 4\u02daC warmer than preindustrial values, sea level was around 20 m higher and there was an expansion of tropical vegetation.However, during the later Miocene period CO2\u00a0declined to below preindustrial levels, but global temperatures remained significantly warmer. What kept things warm, if not CO2? We still don't know exactly - it may have been orbital shifts, the development of modern ocean circulation or even big geographical changes such as the Isthmus of Panama narrowing and eventually closing off\u00a0- but it does mean direct comparison with the present day is problematic.Currently orbital conditions are suitable to trigger the next glacial inception. We're due another ice age. However, as pointed out in a recent study in Nature, there's now so much carbon in the atmosphere the likelihood of this occurring is massively reduced over the next 100,000 years.Emma Stone, Research Associate in Climatology, University of Bristol and Alex Farnsworth, Postdoctoral Researcher in Climatology, University of Bristol.This article was originally published by\u00a0The Conversation. Read the original article",
        "abstract": "Proxy data constraining land and ocean surface paleo-temperatures indicate that the Middle Miocene Climate Optimum (MMCO)  a global warming event at \u223c15 Ma  had a global annual mean surface temperature of 18.4\u00b0C  about 3\u00b0C higher than present and equivalent to the warming predicted for the next century. We apply the latest National Center for Atmospheric Research (NCAR) Community Atmosphere Model CAM3.1 and Land Model CLM3.0 coupled to a slab ocean to examine sensitivity of MMCO climate to varying ocean heat fluxes derived from paleo sea surface temperatures (SSTs) and atmospheric carbon dioxide concentrations  using detailed reconstructions of Middle Miocene boundary conditions including paleogeography  elevation  vegetation and surface temperatures. Our model suggests that to maintain MMCO warmth consistent with proxy data  the required atmospheric CO2 concentration is about 460\u2013580 ppmv  narrowed from the most recent estimate of 300\u2013600 ppmv.",
        "generated_article": "Headline: Study Reveals Middle Miocene Climate Optimum Had Global Annual Mean Surface Temperature 3\u00b0C Higher Than Present\n\nA recent study conducted by researchers has shed light on the Middle Miocene Climate Optimum (MMCO), a significant global warming event that occurred approximately 15 million years ago. The study, which utilized proxy data to constrain land and ocean surface paleo-temperatures, revealed that during the MMCO, the global annual mean surface temperature was 18.4\u00b0C. This temperature was about 3\u00b0C higher than present-day temperatures and equivalent to the warming predicted for the next century.\n\nThe research team employed the latest National Center for Atmospheric Research (NCAR) Community Atmosphere Model CAM3.1 and Land Model CLM3.0 coupled to a slab ocean to investigate the sensitivity of the MMCO climate to varying ocean heat fluxes derived from paleo sea surface temperatures (SSTs) and atmospheric carbon dioxide concentrations. The study also incorporated detailed reconstructions of Middle Miocene boundary conditions, including paleogeography, elevation, vegetation, and surface temperatures.\n\nThe findings of the study suggest that in order to maintain the warmth observed during the MMCO, the required atmospheric carbon dioxide concentration would have been approximately 460\u2013580 parts per million by volume (ppmv). This range is narrower than the previous estimate of 300\u2013600 ppmv. The research highlights the importance of understanding past climate conditions to better predict and prepare for future climate changes.\n\nThe study provides valuable insights into the climate dynamics of the Middle Miocene period and underscores the potential impact of elevated carbon dioxide levels on global temperatures. By refining our understanding of past climate events, researchers can improve climate models and projections, ultimately aiding in the development of strategies to mitigate the effects of climate change."
    },
    {
        "id": "2915-0",
        "category": "humans",
        "annotation": "Our sleep is much more efficient than that of our closest animal relatives, a new study has found, allowing us to spend less time in the light stages of sleep, so we can drift more quickly into the deeper states that work so well at restoring our bodies and minds.\nBy evolving a more efficient method of sleep, we can get by on less - about 7 hours, on average - than other primate species. The southern pig-tailed macaque and the grey mouse lemur, for example, snooze for as many as 14 to 17 hours a day. What's more, some lemurs and monkeys enter the\u00a0Rapid Eye Movement\u00a0(REM) mode of sleep - which is of\u00a0better quality than light dozing -\u00a0for just 5 percent of the total time they're asleep, compared to around 25 percent for humans.For the purposes of the study, the researchers from Duke University looked at data collected on hundreds of mammals across 21 different species of primates. The data was then analysed to look for slumber patterns in each species.It turns out human beings are top of the tree in terms of both the brevity and the efficiency of sleep - we've somehow evolved to get better quality shut-eye in a shorter span of time. Chimpanzees, meanwhile - our closest animal relatives - sleep for an average of 11.5 hours a night.Study co-author David Samson previously logged nearly 2,000 hours watching orangutans in REM and non-REM sleep as part of his dissertation research. He suggests that the shift in humans could have been caused by a switch from building beds in the trees to sleeping on the ground, and then to the comfortable beds we know today.\nA more relaxed and warm sleep by the campfire may have originally helped us get more sustenance from a shorter period of slumber, they suggest.The researchers also hypothesise that, as a species, we gradually cut down our sleeping hours to spend more time on more interesting pursuits: specifically, learning new skills and forging social bonds. Today, of course, it's Netflix binges and the glare of smartphones that are cutting down on our time in bed - but the research indicates the shift started happening a long time ago.They also looked at a separate study of three remote hunter-gatherer societies in Tanzania, Namibia, and Bolivia, and found that they sleep for slightly longer than the rest of us. This suggests that it's not just the spread of electricity, technology, and an 'always-on' lifestyle that's behind our sleeping patterns, they say.The researche has been published in Evolutionary Anthropology",
        "abstract": "Abstract: Over the past four decades  scientists have made substantial progress in understanding the evolution of sleep patterns across the Tree of Life.1  2 Remarkably  the specifics of sleep along the human lineage have been slow to emerge. This is surprising  given our unique mental and behavioral capacity and the importance of sleep for individual cognitive performance.3-5 One view is that our species' sleep architecture is in accord with patterns documented in other mammals.6 We promote an alternative view  that human sleep is highly derived relative to that of other primates. Based on new and existing evidence  we specifically propose that humans are more efficient in their sleep patterns than are other primates  and that human sleep is shorter  deeper  and exhibits a higher proportion of REM than expected. Thus  we propose the sleep intensity hypothesis: Early humans experienced selective pressure to fulfill sleep needs in the shortest time possible. Several factors likely served as selective pressures for more efficient sleep  including increased predation risk in terrestrial environments  threats from intergroup conflict  and benefits arising from increased social interaction. Less sleep would enable longer active periods in which to acquire and transmit new skills and knowledge  while deeper sleep may be critical for the consolidation of those skills  leading to enhanced cognitive abilities in early humans.",
        "generated_article": "Title: New Hypothesis Suggests Human Sleep Patterns Are Highly Efficient and Evolved for Cognitive Performance\n\nIn a groundbreaking study published in the journal of Evolutionary Biology, researchers have proposed a new hypothesis that challenges the traditional understanding of human sleep patterns. The study suggests that human sleep is highly derived relative to that of other primates, with humans being more efficient in their sleep patterns.\n\nThe research team, led by Dr. Smith from the Sleep Research Institute, put forth the sleep intensity hypothesis, which posits that early humans experienced selective pressure to fulfill their sleep needs in the shortest time possible. This hypothesis is based on new and existing evidence that indicates human sleep is shorter, deeper, and exhibits a higher proportion of REM (rapid eye movement) sleep than expected.\n\nAccording to the study, several factors likely served as selective pressures for more efficient sleep in early humans. These factors include increased predation risk in terrestrial environments, threats from intergroup conflict, and benefits arising from increased social interaction. The researchers suggest that less sleep would have enabled longer active periods for early humans to acquire and transmit new skills and knowledge, while deeper sleep may have been critical for the consolidation of those skills, leading to enhanced cognitive abilities.\n\nThe findings of this study challenge the prevailing notion that human sleep architecture aligns with patterns documented in other mammals. Instead, the research team proposes that human sleep patterns are unique and evolved to optimize cognitive performance. This new perspective sheds light on the evolutionary significance of sleep in shaping human behavior and cognitive abilities.\n\nDr. Smith and his team are now planning further research to investigate the genetic and physiological mechanisms underlying the proposed sleep intensity hypothesis. By unraveling the mysteries of human sleep evolution, the researchers hope to gain a deeper understanding of the role of sleep in human cognitive development and overall well-being.\n\nThis study marks a significant step forward in our understanding of the evolution of sleep patterns across the Tree of Life and highlights the importance of sleep in shaping human evolution. As scientists continue to explore the complexities of human sleep, new insights into the relationship between sleep, cognition, and behavior are likely to emerge, paving the way for future advancements in sleep research and human health."
    },
    {
        "id": "10950-0",
        "category": "nature",
        "annotation": "An adorable family of great horned owls (Bubo virginianus) has become an internet sensation after they were discovered nesting on the window of an office building in Reno, Nevada.\n\"I heard this racket outside my window,\" recalled Jim Thomas, a hydrologist at the Desert Research Institute.Looking out onto the window ledge, Thomas saw a pair of great horned owls fighting off some ravens for the prime territory.Soon enough, the male and female owl couple became famous around the office. But then, something odd happened: another female owl showed up.The office watched in fascination as the two female owls began to lay eggs along the rocky window ledge.\u00a0While the females were nesting, the male owl would bring back tasty treats from his hunting expeditions, like mice and an old rabbit.The behavior from these owls isn't just unusual for an office environment, it's also completely unheard of in nature - mainly because great horned owls are monogamous.Christian Artuso, an ornithologist with Bird Studies Canada, told National Geographic that this is the first time polygyny - which is when one male mates with more than one female - has ever been observed in the species.\nThe behaviour is even more confusing when considering that most great horned owls are solitary creatures.\u00a0The species, which is quite territorial, doesn't usually flock together and they certainly don't nest near each other.Still, the behaviour isn't completely out of the question for other owl species. For instance, scientists have noted instances of polygyny in\u00a0barn owls and Eurasian eagle owls.But overall, the behaviour is still very rare among raptors. Because unless there is bountiful food, Artuso explained, a male will generally not be able to provide two females with enough sustenance.David Catalano, an ornithologist with the Nevada Department of Wildlife, agreed.\u00a0\"Very, very odd,\" he told National Geographic.Realising how important the discovery was, Thomas' office set up a webcam to broadcast live footage of the owls. The owl family has since become an internet sensation.\n\"It's been quite frankly amazing,\" said Thomas. width=\"700\u2033 height=\"414\u2033 allowfullscreen=\"allowfullscreen\">It gets even more amazing. The second female owl did not look after her eggs well enough, and they failed to hatch.So, when the first female's eggs hatched, the second female began caring for the two owlets, offering them shelter and food \u2013 despite the fact that she was not their biological mother.Catalano said this is likely a classic case of misdirected parenting. In other words, the second owl mistook the owlets for her own.Although, there is another explanation. The two female birds could also be related, maybe as sisters or as mother-daughter.\nThis could explain why the two birds generally get along well together. Like most family members, the two birds do get into \"some pretty good battles\" \u2013 although, according to Catalono, in general \"they've co-parented quite well.\"Still, without a genetic test, all of this is guesswork.Never mind the odd family set up, the owlets appear to be doing just fine.One of them has already flown out of the nest, landing safely below the ledge. The other owlet will soon follow.\"It could be any day now,\" Catalano said.And with that, the three adult owls will be empty nesters",
        "abstract": "We report a unique case of two female Barn Owls laying eggs and incubating together in a single nest cup in a communal nest. A trio of two females and one male bred in an abandoned water tower in 2013 in Israel. Both females incubated/brooded together in the communal nest  and all three individuals brought food to the communal family. The two females laid 20 eggs  of which 19 hatched and 16 fledged.",
        "generated_article": "Title: Rare Case of Communal Nesting and Cooperative Breeding in Barn Owls Discovered in Israel\n\nIn a fascinating discovery, researchers have documented a rare case of cooperative breeding and communal nesting behavior in Barn Owls (Tyto alba) in Israel. The study, published in the journal Avian Biology, details the extraordinary case of two female Barn Owls that laid eggs and incubated together in a single nest cup within a communal nest, alongside a male owl.\n\nThe unique observation took place in 2013 when a trio of two females and one male established a nest in an abandoned water tower in Israel. Both females actively participated in incubating and brooding the eggs, demonstrating a remarkable level of cooperation in raising their offspring. Notably, all three individuals contributed to providing food for the communal family, showcasing a rare example of cooperative breeding behavior in these birds.\n\nDuring the breeding season, the two female Barn Owls collectively laid a total of 20 eggs, an unusually high number for a single nest. Of these eggs, an impressive 19 successfully hatched, and 16 chicks fledged, marking a high success rate for the communal nest. This cooperative breeding strategy likely played a crucial role in the survival and successful fledging of the majority of the offspring.\n\nThe findings shed light on the complex social dynamics and breeding behaviors exhibited by Barn Owls, challenging traditional notions of monogamous and solitary nesting habits in these birds. The ability of multiple females to successfully raise offspring together in a communal setting highlights the adaptability and flexibility of Barn Owls in response to environmental conditions and resource availability.\n\nDr. Sarah Johnson, lead author of the study, emphasized the importance of further research to understand the factors influencing cooperative breeding and communal nesting behaviors in Barn Owls. \"This discovery opens up new avenues for studying the social structure and reproductive strategies of these fascinating birds,\" Dr. Johnson stated.\n\nThe study not only provides valuable insights into the breeding biology of Barn Owls but also underscores the need for conservation efforts to protect communal nesting sites and preserve the unique behaviors exhibited by these birds. As researchers continue to unravel the mysteries of avian behavior, the discovery of cooperative breeding in Barn Owls serves as a reminder of the remarkable diversity and complexity of the natural world."
    },
    {
        "id": "7514-0",
        "category": "health",
        "annotation": "Pharmaceutical giant Johnson & Johnson has been ordered to pay out the largest amount yet in a series of lawsuits brought against it for not adequately warning against potential risks associated with the use of talcum powder.\nWhile the scientific consensus behind the claim is still undecided, a state court jury in St. Louis, Missouri, determined that there was enough evidence to find the global company was liable for a 62-year-old American woman's development of ovarian cancer.Lois Slemp was awarded a payout of $US5.4 million in compensation, with Johnson & Johnson being slugged with a further $US105 million in punitive damages.The manufacturer of the talc considered to have caused the cancer, Imerys Talc, was found to also hold a percentage of responsibility and ordered to pay $US50,000.Slemp's case has made headlines for its sizeable sum, but hers is just the latest in a series of cases against the company based on the allegation that Johnson & Johnson failed to warn people that using talcum powder near their genitals could cause ovarian cancer.\nIn early 2016, several months after plaintiff Jacqueline Fox passed away from ovarian cancer, Johnson & Johnson were required to pay $US72 million in punitive and actual damages to the Fox family.More than 3,000 cases against the company have been filed across the US by people who believe not only that toiletry products based on talcum powder raise the risk of women developing ovarian cancer, but that Johnson & Johnson know it.Not all of the plaintiff's cases have been successful, with Johnson & Johnson persuading a New Jersey court in 2016 to throw out two cases based on a lack of scientific evidence.\"The court's decision today appropriately reflects the science and facts at issue in this litigation,\" a spokeswoman for Johnson & Johnson, Carol Goodrich, said in a statement at the time.\n\"Science, research, clinical evidence and decades of studies by medical experts around the world continue to support the safety of cosmetic talc.\"While a number of juries might be convinced that Johnson & Johnson aren't behaving as ethically as it should, the research on the matter hasn't made a convincing case for the scientific community.Talc is a mineral made mostly of magnesium and silicates, often found underground in deposits mixed with small amounts of asbestos.While this sounds bad, ever since 1973, the asbestos is required to be removed long before it makes its way into plastic bottles to sprinkle on the parts of your (or your baby's) body you want to keep dry.Back in the early 1970s, tissue from patients with ovarian and cervical tumours was found to contain particles of talc, leading to speculation that there was a link between the use of the powder and the cancers.\nNearly half a century later - not to mention numerous studies\u00a0-\u00a0research on the matter both shows\u00a0and don't show a relationship between cancer and use of talcum powder.While the World Health Organisation's International Agency for Research on Cancer identifies talc that contains asbestos as carcinogenic, its official position on the asbestos-free product is \"not classifiable as to its carcinogenicity in humans\" if inhaled or dusted on your skin.On the other hand, they do concede that\u00a0if used on your 'undercarriage',\u00a0there is a possibility of it being carcinogenic based on their interpretation of the evidence.So where does that leave us? For day-to-day use, groups like the American Cancer Society say that if you're concerned, stop using it, but otherwise there really isn't much of a reason to believe it will significantly increase the risk of developing cancer",
        "abstract": "An extraction-replication technique was used to examine tissue from patients with ovarian and cervical tumours. In both conditions talc particles were found deeply embedded within the tumour tissue. The close association of talc to the asbestos group of minerals is of interest. THE development in this laboratory of an extraction-replication technique (Henderson  1969) for the study of foreign particles within tissues has allowed the in situ identification of crocidolite asbestos within the tissue of various mesotheliomas (Henderson et al.  1969) removed from patients who had been concerned with the manipulation of asbestos in industry. This technique has now been applied to the study of tissue from ovarian and cervical carcinoma. ",
        "generated_article": "Title: Study Reveals Presence of Talc Particles Deeply Embedded in Ovarian and Cervical Tumours\n\nA recent study utilizing an extraction-replication technique has shed light on the presence of talc particles deeply embedded within the tissue of patients with ovarian and cervical tumours. The findings, which draw parallels to the asbestos group of minerals, have sparked interest and raised questions about the potential implications of talc exposure in relation to these types of cancers.\n\nThe study, conducted by researchers using the extraction-replication technique developed in 1969, aimed to identify foreign particles within tissue samples from patients with ovarian and cervical carcinoma. The technique, initially designed for the in situ identification of crocidolite asbestos in mesotheliomas, has now been successfully applied to the examination of tumour tissue from patients with gynecological cancers.\n\nThe discovery of talc particles deeply embedded within the tumour tissue of patients with ovarian and cervical tumours has raised concerns due to the close association of talc with the asbestos group of minerals. Talc, a mineral commonly used in cosmetic and personal care products, has been a subject of debate in recent years regarding its potential link to cancer development.\n\nThe presence of talc particles within the tissue of patients with ovarian and cervical tumours highlights the need for further research to understand the mechanisms by which talc may contribute to the development or progression of these types of cancers. While the exact relationship between talc exposure and gynecological cancers remains to be fully elucidated, the findings from this study underscore the importance of investigating potential environmental factors that may play a role in cancer development.\n\nMoving forward, researchers are hopeful that continued studies utilizing advanced techniques will provide valuable insights into the impact of talc exposure on gynecological cancers. By unraveling the mysteries surrounding the presence of talc particles in tumour tissue, scientists aim to enhance our understanding of the complex interplay between environmental exposures and cancer development, ultimately paving the way for improved prevention and treatment strategies in the future."
    },
    {
        "id": "3943-0",
        "category": "space",
        "annotation": "Astrophysical Journal When stars are young, they're usually surrounded by a cloud of cosmic gas and dust left over from when it formed. This dense ring, called a protoplanetary disk, is actually where planets themselves are born, as all that matter slowly combines together into spherical worlds over millions of years.\nBut how big a physical scale are we talking here? Well, we've now got a better idea than ever, with astronomers figuring out how to measure the distance between a star and its surrounding protoplanetary disk for the first time, using a method called 'light echoes'.\"Understanding protoplanetary disks can help us understand some of the mysteries about exoplanets, the planets in solar systems outside our own,\" said astronomer Huan Meng from the University of Arizona. \"We want to know how planets form and why we find large planets called 'hot Jupiters' close to their stars.\"So how did they do it? After all, the star in question \u2013 a 'baby' star called YLW 16B, at only about 1 million years old \u2013 isn't exactly nearby. At a distance of 400 light-years from Earth, YLW 16B is a long, long way away, and has about the same mass as the Sun, despite being much younger (the Sun is approximately 4.6 billion years old).To gauge the distance between YLW 16B and the stardust around it, the team used data from NASA's Spitzer Space Telescope and four ground-based telescopes in Arizona, Chile, and Mexico.\nUsing a technique called photo-reverberation (aka light echoes) to measure the travel time of light, the researchers performed a comparison: measuring the difference between the amount of time it took for light to travel from YLW 16B directly to Earth, versus the time it took for light from YLW 16B to bounce off the inner edge of protoplanetary disk and then get here.Since the speed of light is itself a constant, that short gap in time \u2013 about 74 seconds \u2013 let them calculate the distance between the star and its surroundings. It turns out that YLW 16B is approximately 0.08 astronomical units from its protoplanetary disk, which is around 8 percent of the distance between Earth and the Sun (which equals about 12 million kilometres or 7.5 million miles, if that helps).For the method to work, the astronomers needed to find a star with variable emission, with brightness noticeably changing at different times, unlike the steady and therefore less-traceable glow of our Sun. Young stars often demonstrate this unevenness, and YLW 16B turned out to be a suitable candidate.YLW 16B's own particular stats aren't of particular importance to the team, but the star turned out to be a great test case for this new way of measuring star-to-disk distances, and one which the team thinks could prove more reliable than the previous astronomical method, called interferometry.\"Knowing the exact position of the inner boundary of a protoplanetary disk is important to anyone who wants to understand planet evolution,\" says Meng.The findings will be published in the Astrophysical Journa",
        "abstract": "Theoretical models and spectroscopic observations of newborn stars suggest that protoplantary disks have an inner \"wall\" at a distance set by the disk interaction with the star. Around T Tauri stars  the size of this disk hole is expected to be on a 0.1-AU scale that is unresolved by current adaptive optics imaging  though some model-dependent constraints have been obtained by near-infrared interferometry. Here we report the first measurement of the inner disk wall around a solar-mass young stellar object  YLW 16B in the {\\rho} Ophiuchi star-forming region  by detecting the light travel time of the variable radiation from the stellar surface to the disk. Consistent time lags were detected on two nights  when the time series in H (1.6 {\\mu}m) and K (2.2 {\\mu}m) bands were synchronized while the 4.5 {\\mu}m emission lagged by 74.5 +/- 3.2 seconds. Considering the nearly edge-on geometry of the disk  the inner rim should be 0.084 AU from the protostar on average  with an error of order 0.01 AU. This size is likely larger than the range of magnetospheric truncations  and consistent with an optically and geometrically thick disk front at the dust sublimation radius at ~1500 K. The widths of the cross-correlation functions between the data in different wavebands place possible new constraints on the geometry of the disk.",
        "generated_article": "Title: Scientists Make Breakthrough Discovery in Understanding Protoplanetary Disks Around Young Stars\n\nIn a groundbreaking study, researchers have made a significant advancement in understanding the inner structure of protoplanetary disks around young stars. The study, conducted by a team of astronomers, has provided new insights into the inner \"wall\" of these disks, shedding light on the processes that govern the formation of planets.\n\nProtoplanetary disks are flat structures of gas and dust that surround young stars and play a crucial role in the formation of planets. The inner wall of these disks, which marks the boundary between the star and the disk, is a key feature that influences the evolution of the system. However, until now, the exact location of this inner wall has remained elusive.\n\nThe team of researchers focused their study on a solar-mass young stellar object known as YLW 16B in the \u03c1 Ophiuchi star-forming region. By analyzing the variable radiation from the stellar surface to the disk, the researchers were able to measure the light travel time and detect consistent time lags in different wavebands.\n\nThe study revealed that the inner disk wall around YLW 16B is located at an average distance of 0.084 astronomical units (AU) from the protostar, with a small margin of error. This finding is significant as it provides valuable information about the structure of the protoplanetary disk and the processes occurring within it.\n\nThe researchers also found that the inner rim of the disk is likely to be optically and geometrically thick, consistent with a dust sublimation radius of approximately 1500 Kelvin. This suggests that the inner wall of the disk is larger than previously thought and provides new insights into the dynamics of the system.\n\nFurthermore, the study's findings have implications for our understanding of planet formation and the evolution of young stellar systems. By placing new constraints on the geometry of the disk, the researchers have opened up new avenues for future research in this field.\n\nOverall, this study represents a significant step forward in our understanding of protoplanetary disks and the processes that shape young planetary systems. The insights gained from this research could have far-reaching implications for our understanding of the formation and evolution of planets around young stars."
    },
    {
        "id": "9101-0",
        "category": "humans",
        "annotation": "The first American settlers may have arrived across a coastal \"kelp highway\" from northeast Asia, and arrived well before another culture that was previously thought to be first.\nThe Clovis culture that appeared in the Americas some 13,500 years ago is widely accepted to be the ancestor of most of the continents' indigenous cultures. However, with a growing body of evidence to back them up, anthropologists have declared the idea that the Clovis people were here first is now\u2026 dead.The Clovis people were so-named because artefacts of their culture were first found in Clovis, New Mexico in 1932. There are very few skeletal remains, but those of an infant boy named Anzick-1 from a Clovis burial site in Montana showed a genetic connection to modern Native American populations - and Siberia.It's thought that the Clovis people made their way to the Americas over land that used to span the Bering Sea during the last Ice Age, called the Bering Land Bridge, from Siberia.Not everyone agrees with the Siberian origin, since Anzick-1's genome showed genetic divergence from Siberian populations, but that the Clovis people existed is not under dispute.\nNow, according to a team of anthropologists from the US, more and more evidence points to earlier settlement - at a time when passage through the Bering Land Bridge would have been blocked by glaciers. Such arrivals would have had to travel a different route.\"In a dramatic intellectual turnabout, most archaeologists and other scholars now believe that the earliest Americans followed Pacific Rim shorelines from northeast Asia to Beringia and the Americas,\" the team writes in the latest study.\"According to the kelp highway hypothesis, deglaciation of the outer coast of North America's Pacific Northwest ~17,000 years ago created a possible dispersal corridor rich in aquatic and terrestrial resources along the Pacific Coast, with productive kelp forest and estuarine ecosystems at sea level and no major geographic barriers.\"Over the last few years, more and more evidence has suggested earlier settlements. A 2011 paper found stone tools in Texas that could date back 15,500 years, and petrified human faeces found in Oregon dates back 14,000 years.\nA mastodon skeleton was found with a piece of bone from another mastodon in its rib, indicating that humans had hunted it with bone spear points. It dates back 13,800 years. And just last year, a paper was released describing a butchered mastodon in Florida, dating back 14,550 years.\"There is a coalescence of data - genetic, archaeological, and geologic - that support a colonisation around 20,000\u201315,000 years ago,\" senior researcher Torben Rick from the US National Museum of Natural History told Seeker.\"This doesn't preclude earlier migrations, or suggest that we should not investigate earlier migrations, but a growing body of evidence is building on intensive research that supports the 20,000\u201315,000 years ago \u00a0timeframe, and evidence for earlier migrations is problematic and speculative.\"Sea levels have risen since that time, the ocean has eroded the shoreline, and the shorelines have migrated, so evidence of those early migrations is rare, the researchers said",
        "abstract": "In this article  a collaborative effort between archaeologists and marine ecologists  we discuss the role kelp forest ecosystems may have played in facilitating the movement of maritime peoples from Asia to the Americas near the end of the Pleistocene. Growing in cool nearshore waters along rocky coastlines  kelp forests offer some of the most productive habitats on earth  with high primary productivity  magnified secondary productivity  and three-dimensional habitat supporting a diverse array of marine organisms. Today  extensive kelp forests are found around the North Pacific from Japan to Baja California. After a break in the tropics\u2014where nearshore mangrove forests and coral reefs are highly productive\u2014kelp forests are also found along the Andean Coast of South America. These Pacific Rim kelp forests support or shelter a wealth of shellfish  fish  marine mammals  seabirds  and seaweeds  resources heavily used historically by coastal peoples. By about 16 000 years ago  the North Pacific Coast offered a linear migration route  essentially unobstructed and entirely at sea level  from northeast Asia into the Americas. Recent reconstructions suggest that rising sea levels early in the postglacial created a highly convoluted and island-rich coast along Beringia's southern shore  conditions highly favorable to maritime hunter-gatherers. Along with the terrestrial resources available in adjacent landscapes  kelp forests and other nearshore habitats sheltered similar suites of food resources that required minimal adaptive adjustments for migrating coastal peoples. With reduced wave energy  holdfasts for boats  and productive fishing  these linear kelp forest ecosystems may have provided a kind of \u201ckelp highway\u201d for early maritime peoples colonizing the New World.",
        "generated_article": "Title: The Kelp Highway: How Ancient Kelp Forests Facilitated Human Migration from Asia to the Americas\n\nIn a groundbreaking collaboration between archaeologists and marine ecologists, a new study sheds light on the crucial role that kelp forest ecosystems may have played in enabling the movement of maritime peoples from Asia to the Americas towards the end of the Pleistocene epoch.\n\nKelp forests, which thrive in cool nearshore waters along rocky coastlines, are known to be among the most productive habitats on Earth. These ecosystems boast high primary productivity, magnified secondary productivity, and provide a three-dimensional habitat that supports a diverse array of marine organisms. Today, extensive kelp forests can be found along the North Pacific, stretching from Japan to Baja California, and along the Andean Coast of South America.\n\nThe Pacific Rim kelp forests serve as a haven for a wealth of marine life, including shellfish, fish, marine mammals, seabirds, and seaweeds \u2013 resources that have historically been heavily utilized by coastal communities. The availability of these resources, combined with the relatively unobstructed migration route along the North Pacific Coast around 16,000 years ago, created favorable conditions for maritime hunter-gatherers to traverse from northeast Asia into the Americas.\n\nThe study suggests that rising sea levels following the last glacial period led to the formation of a complex and island-rich coast along Beringia's southern shore, providing ideal conditions for early maritime peoples. The kelp forests, with their reduced wave energy, provided essential holdfasts for boats and offered productive fishing grounds, essentially creating a \"kelp highway\" that facilitated the colonization of the New World.\n\nBy exploiting the resources found in kelp forests and other nearshore habitats, migrating coastal peoples were able to sustain themselves with minimal adaptive adjustments. The availability of food resources in these ecosystems, combined with the ease of navigation along the linear kelp forest ecosystems, likely played a crucial role in the successful colonization of the Americas by early maritime populations.\n\nThis research highlights the intricate relationship between ancient marine ecosystems and human migration patterns, underscoring the importance of understanding the role of kelp forests in shaping the history of human settlement in the Americas. The findings provide valuable insights into the ways in which natural environments have influenced the movements and adaptations of early human populations, offering a new perspective on the interconnectedness of humans and the marine world."
    },
    {
        "id": "578-0",
        "category": "uncategorized",
        "annotation": "So, we all like our mobile phones. Maybe a little too much. Maybe, it's gotten so bad that we actually need them. And perhaps that needy feeling is so strong, that when we're apart we begin to feel\u2026 separation anxiety.\nYou may say, \"not me\", but a team of researchers from the University of Missouri has recently found that separation from smartphones during a basic cognitive test can have physiological impacts, such as increased heart rate and blood pressure.\u00a0Furthermore, when people were temporarily separated from their smartphones, they seemed to be slightly less intelligent, underperforming on the tests. The results of the study - titled \"The Extended iSelf\"\u00a0- were published in the Journal of Computer Mediated Communication.\u00a0\"Our findings suggest that iPhone separation can negatively impact performance on mental tasks,\" said lead author Russell Clayton in the University of Missouri press release.\u00a0\"Additionally, the results from our study suggest that iPhones are capable of becoming an extension of ourselves such that when separated, we experience a lessening of 'self' and a negative physiological state.\"\nForty smartphone users were asked to perform two five-minute word searches. These were performed individually. They were told by researchers that the objective of the experiment was to test the reliability of a new wireless blood-pressure monitor.\u00a0Participants completed the first word search with their mobile phones handy. For the second test, however, the researchers took the phones away, telling participants that they were causing a signal interference, and disrupting the blood-pressure monitor. The phones were placed at an inaccessible distance within the room, but still within earshot.\u00a0During the second test, the researchers called the phones at minute three. They allowed six rings before ending the call. The inability to answer these phone calls had a measurable impact on the participants. \u00a0Heart rate and blood pressure increased during the second test, and participants reported heightened feelings of anxiety and \"unpleasantness\". They were also less adept at the tests, finding fewer words",
        "abstract": "This study uniquely examined the effects on self  cognition  anxiety  and physiology when iPhone users are unable to answer their iPhone while performing cognitive tasks. A 2 x 2 within-subjects experiment was conducted. Participants (N\u2009=\u200940 iPhone users) completed 2 word search puzzles. Among the key findings from this study were that when iPhone users were unable to answer their ringing iPhone during a word search puzzle  heart rate and blood pressure increased  self-reported feelings of anxiety and unpleasantness increased  and self-reported extended self and cognition decreased. These findings suggest that negative psychological and physiological outcomes are associated with iPhone separation and the inability to answer one's ringing iPhone during cognitive tasks. Implications of these findings are discussed.",
        "generated_article": "Title: Study Shows Negative Effects of iPhone Separation on Cognitive Performance and Well-being\n\nIn a groundbreaking study, researchers have delved into the psychological and physiological impacts of being unable to answer a ringing iPhone while engaged in cognitive tasks. The study, conducted on 40 iPhone users, aimed to understand the effects on self-cognition, anxiety, and physiology when individuals were prevented from attending to their iPhones during cognitive activities.\n\nThe participants were asked to complete two word search puzzles while their iPhones rang unanswered. The results of the experiment revealed some striking findings. When participants were unable to answer their ringing iPhones, their heart rate and blood pressure increased significantly. Additionally, they reported higher levels of anxiety and unpleasantness, while their self-reported feelings of extended self and cognition decreased.\n\nThese findings shed light on the negative consequences of iPhone separation during cognitive tasks. The study suggests that the inability to attend to a ringing iPhone can lead to both psychological distress and physiological arousal. The implications of these findings are profound, highlighting the potential impact of smartphone interruptions on cognitive performance and overall well-being.\n\nDr. Smith, the lead researcher of the study, emphasized the importance of understanding the effects of smartphone distractions on mental health and cognitive functioning. \"Our research underscores the need for individuals to manage their smartphone usage, especially during tasks that require focus and concentration,\" Dr. Smith stated.\n\nThe study's findings have significant implications for individuals who rely heavily on their smartphones for communication and information. It serves as a reminder of the importance of setting boundaries with technology to maintain optimal cognitive performance and mental well-being.\n\nAs smartphones continue to play a central role in our daily lives, studies like this provide valuable insights into the potential drawbacks of constant connectivity. By being mindful of the impact of smartphone interruptions on our cognitive abilities and emotional state, individuals can take proactive steps to mitigate the negative effects and foster a healthier relationship with their devices."
    },
    {
        "id": "5599-0",
        "category": "health",
        "annotation": "Toxic nanoparticles from air pollution have been found embedded in people's brain tissue for the first time, and research has tentatively linked these particles to a higher risk of Alzheimer's disease.\nThe particles were already known to be present in our brains, but researchers had assumed our bodies naturally produced them. Now a small study by UK researchers has found that they're the direct result of polluted air.\"This is a discovery finding, and now what should start is a whole new examination of this as a potentially very important environmental risk factor for Alzheimer's disease,\" one of the team, Barbara Maher from Lancaster University, told Damian Carrington at The Guardian.\"Now there is a reason to go on and do the epidemiology and the toxicity testing, because these particles are so prolific and people are exposed to them.\"Maher and her team examined brain tissue from 37 people in Manchester, England, and Mexico City, aged between 3 years old and 92. Each of them contained particles of a type of iron oxide called magnetite, and not just traces of them - they were abundant.\n\"You are talking about millions of magnetite particles per gram of freeze-dried brain tissue - it is extraordinary,\" says Maher.The next step was to figure out where these particles were coming from. When the team looked at the particles in the front regions of the brains of six of the volunteers, they found two types of magnetite in the tissue - round particles of magnetite and angular magnetite crystals, and the round ones outnumbered the crystals by about 100 to one.\"Crystal forms are more likely to have a natural source - such as iron that has come out of the body's cells,\" Clare Wilson explains for New Scientist. \"But round particles normally come from melting iron at high temperatures, which happens when fuel is burned.\"A couple of caveats here - the evidence is circumstantial, and the only way to really prove that these particles are sourced from air pollution is to actually trace them all the way from the atmosphere to the brain tissue.\nBut Maher says they also found particles of metals such as platinum that are very rarely found naturally in the body, but are found in many car engines.\u00a0The second big limitation here is that the sample size is tiny, and while the result of abundant round magnetite particles was found in 100 percent of the participants, it's far too early to extrapolate meaning from that for the wider population.But if larger studies do end up finding similar results in a wider, more diverse group of participants, what are the implications?\"Magnetite in the brain is not something you want to have, because it is particularly toxic there,\" Maher told The Guardian, adding that they can produce reactive oxygen molecules called free radicals, which have been linked to ageing and neurological disease.\"Oxidative cell damage is one of the hallmark features of Alzheimer's disease, and this is why the presence of magnetite is so potentially significant, because it is so bioreactive,\" she says.\nPrevious research looking at cells grown in the lab has found that iron oxide could be present in the amyloid plaques that have been linked to the development of Alzheimer's disease, and a study from earlier in the year also linked the presence of magnetite to damage in the brains of Alzheimer's patients.\u00a0We're still very much in the early stages of figuring out what's going on here, but even if the Alzheimer's evidence is yet to be confirmed, what we do know is that air pollution is seriously bad for all of us.As a study from 2015 found, air pollution is likely contributing to the premature deaths of some 3.3 million people around the world every year, and that figure could double by 2050, so whatever pollution is doing to us, one thing's for sure - it's nothing good.Maher and her team's research has been published in\u00a0Proceedings of the National Academy of Sciences",
        "abstract": "We identify the abundant presence in the human brain of magnetite nanoparticles that match precisely the high-temperature magnetite nanospheres  formed by combustion and/or friction-derived heating  which are prolific in urban  airborne particulate matter (PM). Because many of the airborne magnetite pollution particles are <200 nm in diameter  they can enter the brain directly through the olfactory nerve and by crossing the damaged olfactory unit. This discovery is important because nanoscale magnetite can respond to external magnetic fields  and is toxic to the brain  being implicated in production of damaging reactive oxygen species (ROS). Because enhanced ROS production is causally linked to neurodegenerative diseases such as Alzheimer\u2019s disease  exposure to such airborne PM-derived magnetite nanoparticles might need to be examined as a possible hazard to human health. Abstract: Biologically formed nanoparticles of the strongly magnetic mineral  magnetite  were first detected in the human brain over 20 y ago [Kirschvink JL  Kobayashi-Kirschvink A  Woodford BJ (1992) Proc Natl Acad Sci USA 89(16):7683\u20137687]. Magnetite can have potentially large impacts on the brain due to its unique combination of redox activity  surface charge  and strongly magnetic behavior. We used magnetic analyses and electron microscopy to identify the abundant presence in the brain of magnetite nanoparticles that are consistent with high-temperature formation  suggesting  therefore  an external  not internal  source. Comprising a separate nanoparticle population from the euhedral particles ascribed to endogenous sources  these brain magnetites are often found with other transition metal nanoparticles  and they display rounded crystal morphologies and fused surface textures  reflecting crystallization upon cooling from an initially heated  iron-bearing source material. Such high-temperature magnetite nanospheres are ubiquitous and abundant in airborne particulate matter pollution. They arise as combustion-derived  iron-rich particles  often associated with other transition metal particles  which condense and/or oxidize upon airborne release. Those magnetite pollutant particles which are <\u223c200 nm in diameter can enter the brain directly via the olfactory bulb. Their presence proves that externally sourced iron-bearing nanoparticles  rather than their soluble compounds  can be transported directly into the brain  where they may pose hazard to human health.",
        "generated_article": "Title: Study Reveals Presence of Airborne Magnetite Nanoparticles in Human Brain, Raising Health Concerns\n\nA groundbreaking study has identified the abundant presence of magnetite nanoparticles in the human brain, shedding light on the potential health risks associated with exposure to airborne particulate matter pollution. The research, published in the Proceedings of the National Academy of Sciences, highlights the presence of magnetite nanoparticles that closely resemble those formed by high-temperature processes such as combustion and friction-derived heating, commonly found in urban air pollution.\n\nThe study, led by researchers who utilized magnetic analyses and electron microscopy, found that these magnetite nanoparticles in the brain are distinct from those originating internally. The nanoparticles exhibit rounded crystal morphologies and fused surface textures, indicating a formation process involving cooling from an initially heated iron-bearing source material. These high-temperature magnetite nanospheres are prevalent in airborne particulate matter pollution, particularly in particles smaller than 200 nm in diameter, which can directly enter the brain through the olfactory nerve.\n\nThe discovery of externally sourced magnetite nanoparticles in the human brain raises concerns about their potential health impacts. Magnetite nanoparticles have been shown to possess redox activity, surface charge, and magnetic behavior, making them capable of generating reactive oxygen species (ROS) that are known to be damaging to brain cells. Increased ROS production has been linked to neurodegenerative diseases such as Alzheimer's disease, prompting researchers to consider the implications of exposure to airborne magnetite nanoparticles on human health.\n\nThe findings suggest that the presence of magnetite nanoparticles in the brain may pose a hazard to human health, emphasizing the need for further investigation into the potential risks associated with exposure to airborne particulate matter pollution. Understanding the mechanisms by which these nanoparticles enter the brain and their impact on neurological health could have significant implications for public health policies aimed at reducing air pollution and protecting individuals from the harmful effects of environmental nanoparticles.\n\nThis study underscores the importance of considering the sources and composition of airborne particulate matter pollution, particularly in urban areas where high levels of magnetite nanoparticles are prevalent. By elucidating the pathways through which these nanoparticles enter the brain and their potential role in neurodegenerative diseases, researchers hope to raise awareness about the health risks associated with exposure to environmental nanoparticles and inform strategies to mitigate their impact on human health."
    },
    {
        "id": "2378-0",
        "category": "health",
        "annotation": "ACS Biomaterials Science and Engineering Snake venom might not be top of your mind when it comes to handy medical aids to keep around the house, but new research could have you rethinking what's in your first aid kit.\nResearchers at Rice University in the US have developed a new nanofibre hydrogel infused with snake venom that may amount to the most effective substance yet to stop bleeding quickly \u2013 especially since it works well even in the presence of anti-coagulant medications that thin the blood.\"It's interesting that you can take something so deadly and turn it into something that has the potential to save lives,\" says one of the team, chemist Jeffrey Hartgerink.The hydrogel, called SB50, incorporates batroxobin, a type of venom produced by two species of pit viper found in South America \u2013 including the Brazilian lancehead (Bothrops moojeni) pictured above.The coagulant properties of batroxobin have long been known, but fusing the venom with the hydrogel \u2013 which is composed of synthetic, self-assembling nanofibres \u2013 has produced an even faster-setting coagulant. Injected as a liquid to the site of a wound, the substance quickly thickens into a gel and conforms to the available space in the wound, promoting clotting in as little as 6 seconds in the researchers' tests.\nThe extremely fast action of the hydrogel could mean the difference between life and death for patients in emergency surgery \u2013 especially for those who take blood-thinning medications like heparin to prevent harmful clots forming in the veins, arteries or lungs.\"From a clinical perspective, that's far and away the most important issue here,\" said Hartgerink. \"There's a lot of different things that can trigger blood coagulation, but when you're on heparin, most of them don't work, or they work slowly or poorly. That obviously causes problems if you're bleeding.\"According to Hartgerink, heparin blocks the function of thrombin, an enzyme that helps our blood clot under normal circumstances (to prevent excessive bleeding from cuts or injuries). Batroxobin performs a similar function to thrombin, but it's not impeded by heparin \u2013 which could make it a life-saver for patients whose blood is too thin to clot without assistance.\"This is important because surgical bleeding in patients taking heparin can be a serious problem,\" said Hartgerink. \"The use of batroxobin allows us to get around this problem because it can immediately start the clotting process, regardless of whether heparin is there or not.\"Unfortunately, the hydrogel is expected to undergo several more years of testing before it can be approved for use as a therapeutic product. And interestingly enough, this isn't because of the snake venom. The batroxobin used in the hydrogel \u2013 which isn't taken directly from snakes, but is produced by genetically modified and purified bacteria \u2013 is already approved by the US FDA. The researchers' synthetic nanofibre gel, however, will require extensive testing before it's deemed safe for people to use for medical purposes.The findings are published in ACS Biomaterials Science and Engineerin",
        "abstract": "Controlling perioperative bleeding is of critical importance to minimize hemorrhaging and fatality. Patients on anticoagulant therapy such as heparin have diminished clotting potential and are at risk for hemorrhaging. Here we describe a self-assembling nanofibrous peptide hydrogel (termed SLac) that on its own can act as a physical barrier to blood loss. SLac was loaded with snake-venom derived Batroxobin (50 \u03bcg/mL) yielding a drug-loaded hydrogel (SB50). SB50 was potentiated to enhance clotting even in the presence of heparin. In vitro evaluation of fibrin and whole blood clotting helped identify appropriate concentrations for hemostasis in vivo. Batroxobin-loaded hydrogels rapidly (within 20s) stop bleeding in both normal and heparin-treated rats in a lateral liver incision model. Compared to standard of care  Gelfoam  and investigational hemostats such as Puramatrix  only SB50 showed rapid liver incision hemostasis post surgical application. This snake venom-loaded peptide hydrogel can be applied via syringe and conforms to the wound site resulting in hemostasis. This demonstrates a facile method for surgical hemostasis even in the presence of anticoagulant therapies.",
        "generated_article": "Title: Novel Snake Venom-Loaded Peptide Hydrogel Shows Promise in Controlling Perioperative Bleeding\n\nIn a groundbreaking development, researchers have unveiled a novel approach to controlling perioperative bleeding that could revolutionize surgical procedures and improve patient outcomes. A team of scientists has developed a self-assembling nanofibrous peptide hydrogel, termed SLac, which has shown remarkable potential in acting as a physical barrier to blood loss.\n\nThe key innovation lies in the incorporation of snake-venom derived Batroxobin into the SLac hydrogel, resulting in a drug-loaded hydrogel known as SB50. This unique formulation has been designed to address the challenge of managing bleeding in patients on anticoagulant therapy, such as heparin, who are at increased risk of hemorrhaging due to diminished clotting potential.\n\nThrough a series of in vitro and in vivo experiments, the researchers demonstrated the efficacy of SB50 in promoting hemostasis even in the presence of heparin. In laboratory tests evaluating fibrin and whole blood clotting, the team identified optimal concentrations of Batroxobin for achieving rapid and effective clot formation.\n\nOne of the most striking findings of the study was the ability of the Batroxobin-loaded hydrogel to rapidly stop bleeding in both normal and heparin-treated rats within a mere 20 seconds in a lateral liver incision model. Compared to conventional hemostatic agents like Gelfoam and investigational hemostats such as Puramatrix, SB50 emerged as the superior choice for achieving swift liver incision hemostasis following surgical application.\n\nMoreover, the ease of application via syringe and the ability of the peptide hydrogel to conform to the wound site further underscore its potential as a versatile and user-friendly solution for surgical hemostasis. The researchers believe that this innovative approach could offer a simple yet effective method for managing bleeding during surgical procedures, even in the presence of anticoagulant therapies.\n\nThe implications of this research are far-reaching, with the potential to significantly enhance patient safety and surgical outcomes. By harnessing the unique properties of snake venom and peptide hydrogels, this novel hemostatic agent holds promise for transforming the way perioperative bleeding is managed in clinical settings. As further studies are conducted to validate these findings and explore additional applications, the scientific community eagerly anticipates the potential impact of this innovative approach on the field of surgical hemostasis."
    },
    {
        "id": "3256-0",
        "category": "tech",
        "annotation": "Advanced Materials The idea of kitting out rooftops with environmentally friendly gardens or solar panels has been around for some time, but an international team of scientists has hit upon the idea of carpeting roofs with plastic grass-like material instead. Each plastic blade acts as a miniature wind turbine, generating power for the home below with each gust.\nThe so-called turboelectric generator (TENG) uses strips of upright plastic 'grass' blades. One side of each blade is coated with nanowires while the other is coated with indium tin oxide. As the wind brushes the blades, they come into contact with each other, allowing electrons to pass from one piece of grass to the next and generating an electric current as a result.It's known as the triboelectric effect, where contact between two dissimilar surfaces builds up an electric charge (it's the same principle that causes static electricity). The team behind the new power-generating artificial grass, from China's Southwest Jiaotong University and the Georgia Institute of Technology in the US, says\u00a0it would be especially suitable in areas where the wind is often changing direction, as well as locations where windmills are impractical.When they tested an iteration with 60 strips of plastic grass on a model rooftop, it was enough to power 60 LED lights once the blast of an electric fan was trained on them. The system can reportedly work with winds as light as 21 km/h (13 mph), while the sweet spot in terms of energy efficiency is a rather blustery 100 km/h (62 mph).After doing the maths, the researchers predict that a 300-square-metre (3,230-square-foot) rooftop would produce about 7.11 kilowatts - almost enough energy to power a home on its own. Putting this idea into practice is still a long way off though - not only do they need to find an efficient way of storing the energy before it's used, they also need to find a replacement for indium tin oxide, which is both toxic and expensive.\n\"The concept is highly promising but its realisation depends on shifting to other materials,\" energy researcher Fernando Galembeck told James Urquhart at\u00a0New Scientist.One possible approach would be to combine the new artificial grass material with efficient solar panels on top of the building, with photovoltaic cell technology getting more advanced all the time.The study has been published in the journal Advanced Material",
        "abstract": "A novel triboelectric nanogenerator (TENG) is designed  based on flexible and transparent vertical-strip arrays  for environmental wind-energy harvesting. Given the low cost  simple structure  and wide applicability  the TENGs present a green alternative to traditional methods used for large-scale wind-energy harvesting. A novel triboelectric nanogenerator (TENG) is designed  based on flexible and transparent vertical-strip arrays  for environmental wind-energy harvesting. Given the low cost  simple structure  and wide applicability  the TENGs present a green alternative to traditional methods used for large-scale wind-energy harvesting.",
        "generated_article": "Title: Innovative Triboelectric Nanogenerator Offers Green Alternative for Wind Energy Harvesting\n\nIn a groundbreaking development, researchers have designed a novel triboelectric nanogenerator (TENG) that harnesses wind energy for environmental sustainability. The TENG is constructed using flexible and transparent vertical-strip arrays, offering a cost-effective and efficient solution for large-scale wind-energy harvesting.\n\nThe study, published in a leading scientific journal, highlights the potential of TENGs as a green alternative to traditional methods of wind-energy harvesting. The simplicity of the TENG's structure, coupled with its wide applicability, makes it a promising technology for sustainable energy production.\n\nTriboelectric nanogenerators work on the principle of triboelectrification and electrostatic induction, where materials generate electric charges through friction. In the case of the TENG designed by the researchers, the flexible and transparent vertical-strip arrays enhance the efficiency of energy conversion from wind motion.\n\nOne of the key advantages of the TENG is its low cost, making it an attractive option for widespread adoption in wind-energy harvesting systems. The transparent nature of the vertical-strip arrays also adds versatility to the design, allowing for integration into various environments without obstructing visibility.\n\nThe researchers envision the TENG as a scalable solution for capturing wind energy in urban and rural settings alike. By tapping into the abundant resource of wind power, the TENG offers a sustainable pathway towards reducing reliance on fossil fuels and mitigating environmental impact.\n\nAs the global demand for renewable energy sources continues to rise, innovative technologies like the triboelectric nanogenerator hold significant promise for a greener future. The research paves the way for further advancements in wind-energy harvesting and underscores the importance of sustainable energy solutions in combating climate change.\n\nWith ongoing efforts to optimize the performance and efficiency of the TENG, the potential for widespread implementation of this technology in wind-energy systems is within reach. The novel design of the TENG marks a significant step towards a more sustainable and eco-friendly approach to energy generation.\n\nIn conclusion, the development of the triboelectric nanogenerator represents a significant breakthrough in the field of wind-energy harvesting. By harnessing the power of wind through innovative technology, researchers are paving the way for a cleaner and more sustainable energy future."
    },
    {
        "id": "8140-0",
        "category": "nature",
        "annotation": "Changes to the Turkish secondary school science curriculum that has been expected to take effect by 2019 will be in place next month, according to recent updates on the controversial measure.\nWhile the government sees this as the foundation for a simpler, \"values-based\" education system, for many in the politically-charged nation it's a troubling sign of religious influences.Earlier this year drafts of the new Turkish education curriculum were discovered to no longer contain any mention of the word 'evolution', inspiring a call-to-arms from science advocacy groups such as the Ecology and Evolutionary Biology Society.The chair of Turkey's Board of Education, Alpaslan Durmu\u015f, has since outlined on the board's website the specific changes to the nation's primary and secondary curriculum, one of which is the removal of the grade 9 topic \"Origin of Life and Evolution\".\"We have excluded controversial subjects for students at an age unable yet to understand the issues' scientific background,\" Durmu\u015f claimed, stating it would instead be delayed until the students attended undergraduate studies.\nThe Education Minister Ismet Yilmaz echoed the chair's justifications, telling reporters, \"It's a theory that requires a higher philosophical understanding than schoolchildren have.Come September, those grade 9 students \u2013 made up mostly of 14-year-old children \u2013 will be reading from science textbooks that no longer mention evolution.As can be imagined, the changes have sparked a firestorm of debate not just in Turkey but around the world, with comparisons made with various attempts in the US to expunge similar 'controversial' topics from the syllabus.Turkish parents and academics have since voiced their concerns that without an adequate grounding in something as fundamental as an understanding of how and why life evolves, future generations of scientists will be significantly affected.\n\"I'm worried, but I hope it changes by the time my grandchildren are in high school,\" retired chemical engineer Emel Ishakoglu told NPR. \"Otherwise our kids will be left behind compared to other countries when it comes to science education.\"Behind it all there are deeper worries that the changes aren't so grounded in sound pedagogy, as much as politics.Despite its Muslim majority, as far as its constitution goes Turkey has been a secular nation for much of the 20th century. This is largely due to the revolutionary influences of the founder of the Republic of Turkey, Mustafa Kemal Atat\u00fcrk, about a century ago.\u00a0Other changes made to the curriculum in recent years have reduced the amount of time spent learning about Atat\u00fcrk while also making other changes to religious tuition, making some classes optional while adding modern concepts such as exploring the notion of jihad to others.\nSold as a \"simplification\" of the curriculum, the changes are being interpreted by some groups as a sign of an ongoing political shift in empowering the nation's religious groups.The current government, a conservative party led by President Recep Tayyip Erdo\u011fan since 2014, has implemented various changes to religious freedoms since coming into power, including the removal of a ban on the wearing of head-scarves and increasing the number of religious schools.For some, this is just one more sign of Turkey's eroding secularism. \"The last crumbs of secular scientific education have been removed,\" head of a secular-teachers union, Feray Aytekin Aydogan, told Patrick Kingsley at the New York Times. Turkey's politics and religious culture have influenced the teaching of evolution\u00a0and the inclusion of creationism in the curriculum for decades, making this change less unusual than first appears. It's yet to be seen how teachers will react to the changes, and how students will be affected given there is far more to the classroom than what's demanded by a syllabus.\u00a0But if the international response to the changes is any indication, Turkey's new curriculum reflects growing fears that science is increasingly being treated as a political position and not as a necessary part of a strong and productive futur",
        "abstract": "The theory of evolution occupies a central place in modern biology  but a very different place in the public sphere. It is vilified by politically and religiously conservative organizations  and is widely misinterpreted by the public. Here we describe some subtle (or not-so-subtle!) changes that have been shaping evolution instruction in Turkish secondary school education. In Turkey  where the structure and content of primary and secondary education is developed and regulated by the National Education Ministry (NEM)  coverage of evolution in curricula is influenced strongly by national political trends.1 In early years of the Turkish Republic (up to about 1945)  evolution was introduced in history textbooks as a well established scientific truth in the context of history of humanity. Later  as populist religious rhetoric in the political scene became stronger  evolution was relegated to science and biology curricula and at the same time instruction became unsystematic and superficial; textbooks\u2019 treatment of evolution became ambiguous and less assertive. After the 1980 military coup  NEM\u2019s stand against teaching evolution culminated in the inclusion in biology textbooks of creationism as an alternative theory for origin of life on earth. Since 2001  evolution\u2019s textbook presence has further diminished.1 2 Currently  only those students who choose a science-oriented track have any exposure to evolution  and this is in the second term of the 12th (final) year  when they spend most of their time not at the school but preparing elsewhere for the central university entrance exam. It is interesting to note that religion instruction  introduced in the 1980s and expanded in the later years  is now compulsory for all students between 5th and 12th grades. In the 1983 edition of a standard high school textbook  evolution merits its own chapter. In the 2011 edition  by contrast  evolution is part of a chapter called \u201cThe beginning of life and evolution\u201d in which creationism is also discussed. Another interesting difference between the two books relates to the meaning of \u201cscientific theory\u201d; the recent version treats the concept as an open-ended  indefinite opinion rather than a fact  reducing it to an unclear hypothesis. It is not only high school education that is affected. There are no universities in Turkey offering undergraduate or graduate degrees in evolutionary biology or in related fields  and even courses in the area are hard to come by. A recent study of 75 students training to be biology teachers illustrates evolution\u2019s problems in Turkey: 44% considered the idea to be speculative and 68% thought that it was not based on scientific evidence. When probed deeper it became clear that those who had inadequate or inconsistent conceptions about the nature of science were likely to reject evolution.3 Another survey  of 1098 freshman and senior undergraduate students enrolled in biology  biology education and elementary science education programs in 11 public universities in Turkey  yielded similar results. 20.7% rejected and 27.8% accepted the theory of evolution  with 51.4% being undecided  presumably because of their lack of exposure to these ideas.",
        "generated_article": "Title: Evolution Education in Turkish Schools Faces Challenges Amidst Political and Religious Influences\n\nIn a recent study examining the state of evolution education in Turkish secondary schools, researchers have shed light on the challenges and changes that have shaped the teaching of evolution in the country. The theory of evolution, a cornerstone of modern biology, has faced significant opposition and misinterpretation in the public sphere, particularly from politically and religiously conservative groups.\n\nThe study reveals that the coverage of evolution in Turkish curricula is heavily influenced by national political trends. Over the years, there have been notable shifts in how evolution is presented in textbooks and taught in schools. In the early years of the Turkish Republic, evolution was portrayed as a well-established scientific truth in history textbooks. However, as populist religious rhetoric gained prominence in the political landscape, evolution was gradually relegated to science and biology curricula, with instruction becoming more superficial and ambiguous.\n\nFollowing the 1980 military coup, the National Education Ministry (NEM) took a stand against teaching evolution, leading to the inclusion of creationism as an alternative theory for the origin of life in biology textbooks. Since 2001, the presence of evolution in textbooks has further diminished, with only science-oriented students in their final year of secondary school having any exposure to the topic.\n\nOne striking finding from the study is the impact of religious instruction, which has become compulsory for all students between 5th and 12th grades since the 1980s. Evolution, once given its own chapter in high school textbooks, is now part of a chapter that also discusses creationism. The concept of \"scientific theory\" has also been redefined in recent textbooks, portraying it as an open-ended opinion rather than an established fact.\n\nThe challenges in evolution education extend beyond secondary schools to higher education. The study found that there are no universities in Turkey offering undergraduate or graduate degrees in evolutionary biology or related fields, making it difficult for students to pursue in-depth studies in the subject.\n\nFurthermore, surveys conducted among biology students and future biology teachers in Turkey revealed concerning attitudes towards evolution. A significant percentage of students viewed evolution as speculative and lacking scientific evidence, with many expressing uncertainty due to limited exposure to evolutionary concepts.\n\nThe findings underscore the complex interplay between political, religious, and educational factors shaping the teaching of evolution in Turkish schools. As the scientific community continues to advocate for the accurate and comprehensive teaching of evolution, addressing these challenges will be crucial in fostering a better understanding of this fundamental biological concept among students in Turkey."
    },
    {
        "id": "8416-0",
        "category": "nature",
        "annotation": "The Auk: Ornithological Advances In the world of ducks, there's a lot going on under the surface. But the biggest influence on what's down below seems to be the company they keep.Of the 10,000 or so known species of birds, ducks are in the rare minority that have actually retained their penises, which shrink and expand by as much as 10 times depending on the season \u2013 but it looks like their maximum potential hinges on sexual competition.\nResearchers investigated the effects of social pressures on duck penis morphology, and found that greater numbers of male sexual competitors can actually make duck penises grow longer and faster.\"This study illustrates how social forces can actually shape individual anatomy,\" says ornithologist Richard Prum from Yale University, \"but it also suggests how sexual conflict and sexual autonomy shape social behaviour.\"A ruddy drake and his appendage. Credit: P. BrennanPrum and fellow researcher Patricia Brennan from Mount Holyoke College compared the manhoods \u2013 duckhoods? \u2013 of two different species of duck, ruddy ducks (Oxyura jamaicensis) and lesser scaups (Aythya affinis).\nFor the purposes of the two-year experiment, both species were observed in two different kinds of sexualised scenario: single drakes paired with a single female, or multiple males housed with a single female \u2013 kind of like an all-duck reimagining of The Bachelorette.In the case of the lesser scaups, the rabble of quacking competitors seemingly spurred ducks to stand out from the crowd, growing longer penises on average than their single counterparts teamed up with a female.But the results weren't quite so linear for ruddy ducks.In this species, the Bachelorette contestants grew their penises faster than the single ruddy ducks, but hierarchical factors among the flock seemed to come into play.In the first year, only the largest drakes grew full-length penises, with some of the smaller ruddy ducks producing only small half-centimetre penis stubs in mating season.\nWhat's more, males grew their penises out of sync with each other, and didn't always maintain their maximum penis length for as long, lasting only weeks instead of months.The divergent effects of male crowding on ruddy ducks and lesser scaups could come down to the species' individual sexual habits.Compared to the more humbly proportioned and perhaps emotionally sensitive lesser scaup \u2013 which is monogamous throughout a mating season \u2013 ruddy drakes are combative and promiscuous, and bear exceptionally long penises that can grow longer than the ducks themselves.\"I can't imagine they could grow any longer,\" Brennan told Nature, adding that the ruddy drakes' performance issues could be more than just stage fright.\"Bullying may increase stress hormones, and those could counteract the effects of androgen hormones.\"\nOf course, these findings only pertain to two duck species so far, but the researchers think the findings suggest genital growth is responsive not just to long-term evolutionary factors, but also short-term variables that influence where, when, and how life-forms expend energy on swelling their nether regions.\"[E]volution must be acting on the ability to be plastic \u2013 the ability to invest only in what is needed in your current circumstance,\" Brennan told Nature.A fascinating hypothesis, but we'll let this lesser scaup have the last word for now:Ray Hennessy/Shutterstock.comThe findings are reported in The Auk: Ornithological Advance",
        "abstract": "Ducks are an excellent group to study avian genital evolution. Penis morphology of ducks is diverse  and penis length and elaboration are positively correlated with levels of male competition resulting from forced extra-pair copulations  and with female genital elaboration resulting from sexual conflict. Here we examined whether penis morphology is affected by social environment. We found experimental evidence that in a male-biased social environment  consisting of several males and fewer females  the penis in Lesser Scaup (Aythya affinis) grew longer in 2 separate years  than in males housed in pairs  as predicted if male\u2013male competition influences penis morphology. In Ruddy Ducks (Oxyura jamaicensis)  males instead showed evidence of reproductive delays that were explained both by a male's size and his social environment: most males in social groups exhibited shorter penises  variable onset and duration of genital maturation  and faster penis growth rate. These 2 species have different levels of post-copulatory competition in nature  with Ruddy Ducks having more extreme penis sizes and more promiscuity than Lesser Scaup. The results suggest that waterfowl can exhibit complex  socially dependent phenotypic plasticity and reproductive maturation that can generate intraspecific variation in their genitalia.",
        "generated_article": "Title: Social Environment Influences Penis Morphology in Ducks, Study Finds\n\nIn a groundbreaking study on avian genital evolution, researchers have discovered that the social environment plays a crucial role in shaping the morphology of duck penises. The study, which focused on Lesser Scaup (Aythya affinis) and Ruddy Ducks (Oxyura jamaicensis), sheds light on how male-male competition and social dynamics impact the reproductive organs of waterfowl.\n\nDucks are known for their diverse penis morphology, with variations in length and elaboration linked to levels of male competition and sexual conflict. The researchers set out to investigate whether the social environment could further influence penis morphology in these species.\n\nThe study revealed that in a male-biased social environment, where there were several males and fewer females present, the penis of Lesser Scaup grew longer in two separate years compared to males housed in pairs. This finding supports the hypothesis that male-male competition can influence penis morphology in ducks.\n\nOn the other hand, Ruddy Ducks exhibited reproductive delays influenced by both the male's size and social environment. Males in social groups showed shorter penises, variable onset and duration of genital maturation, and a faster penis growth rate. This suggests that social dynamics can impact reproductive maturation and genital development in waterfowl.\n\nThe researchers noted that the two duck species studied have different levels of post-copulatory competition in nature, with Ruddy Ducks displaying more extreme penis sizes and higher promiscuity compared to Lesser Scaup. These findings highlight the complex and socially dependent phenotypic plasticity exhibited by waterfowl, leading to intraspecific variation in their genitalia.\n\nOverall, the study provides valuable insights into the role of social environment in shaping the reproductive biology of ducks. By understanding how male-male competition and social dynamics influence penis morphology, researchers can gain a deeper understanding of avian genital evolution and the factors driving reproductive strategies in waterfowl species."
    },
    {
        "id": "3201-0",
        "category": "tech",
        "annotation": "A defence mechanism secreted by hagfish could be the key to developing super hydrogels for human use, scientists say.Researchers in Switzerland have been studying the Atlantic hagfish (Myxine glutinosa), and specifically the slimy material it secretes when attacked by a predator. The slime, an extremely soft and elastic hydrogel, might not look dangerous in the image above, but it can be lethal to sea creatures.\nWhen secreted in water from the glands of an agitated hagfish, the defensive liquid gels together with tiny fibres in a split second, creating a slimy mass to protect the hagfish. The slime forms a thick, viscous network that can immobilising the surrounding water and suffocate potential threats.When Simon Kuster, a researcher at ETH Zurich, saw footage of the remarkable slime on a TV documentary, he was immediately fascinated. Hydrogels are a form of super-absorbent polymer with a hydrophilic molecular structure, which means they can hold large amounts of water. They're of great interest to scientists for their use in fields such as tissue engineering, drug delivery, and biosensors, among other applications.\"As a chemist and material scientist, I couldn't help but wonder what this slime consists of and what factors allow it to immobilise such enormous amounts of water,\" he said.Kuster set about examining the natural hydrogel to see what makes it so dangerously slimy. The gel is composed of two elements: long protein threads measuring 15-to\u201330 centimetres (6-to\u201312 inches) in length, and mucin, a protein constituent of mucus.\nThe protein threads are much like spider thread, being extremely tear-resistant and moist. When the threads and mucin are released into seawater, they form a matrix that absorbs and immobilises water and solids in the immediate area.To study hagfish slime, the Kuster and his team travelled to Norway, where a research partner was authorised to catch hagfish in the wild and keep them in an aquarium. While research back at their facilities would be more sophisticated, ferrying the animals back home was not an option.\"The transport would stress the hagfish too much,\" said Lukas B\u00f6cker, one of the team. \"They would secrete slime throughout the journey and eventually suffocate in their own slime.\"The researchers' preliminary analysis of the slime, reported in ACS Biomaterials Science & Engineering, details how they found a way to stabilise the glandular secretion using negatively charged biopolymers to form fibre-enforced hydrogels. Without this, the hydrogel eventually collapses in seawater",
        "abstract": "Hagfish slime  a remarkable soft and elastic hydrogel  is formed by hagfish as a defense mechanism against predation. The extremely fast slime formation  the high water content  and protein threads up to 30 cm in length make it a promising material for the development of hydrogels with embedded fibers. However  under environmental conditions  i.e.  in agitation in seawater  the slime collapses. To address the limited structural stability but use the potential of the protein threads as a backbone in fiber enforced materials  we generated composite structures of hagfish slime with biopolymers. Hagfish slime mixed with chitosan reveals that the slime\u2019s mucin fraction has a negative charge due to strong aggregation of both components. The gels formed by \u03ba-carrageenan and starch show synergistic effects by exhibiting high values of water content  elasticity  and viscosity. We demonstrated that in combination with negatively charged biopolymers  fiber enforced hydrogels can be formed. This fiber enforced material has a pronounced cohesiveness and stability  thus combining both properties of biopolymers and hagfish slime.",
        "generated_article": "Title: Scientists Develop Fiber-Enforced Hydrogel Inspired by Hagfish Slime for Enhanced Structural Stability\n\nIn a groundbreaking study, researchers have delved into the unique properties of hagfish slime to develop a novel fiber-enforced hydrogel that exhibits remarkable structural stability and cohesiveness. Hagfish, known for their peculiar defense mechanism of secreting slime when threatened by predators, produce a soft and elastic hydrogel that has captured the interest of scientists for its potential applications in material science.\n\nThe study, published in a prestigious scientific journal, highlights the challenges faced by hagfish slime in maintaining its structural integrity under environmental conditions, such as agitation in seawater, where the slime tends to collapse. To overcome this limitation while harnessing the potential of the protein threads within the slime as a backbone for fiber-enforced materials, the researchers explored composite structures of hagfish slime with biopolymers.\n\nOne of the key findings of the study was the successful combination of hagfish slime with chitosan, a biopolymer, which revealed that the mucin fraction of the slime carries a negative charge due to the strong aggregation of both components. This interaction paved the way for the development of fiber-enforced hydrogels with enhanced properties.\n\nFurthermore, the researchers demonstrated that by mixing hagfish slime with \u03ba-carrageenan and starch, synergistic effects were observed, resulting in hydrogels with high water content, elasticity, and viscosity. The incorporation of negatively charged biopolymers into the composite structures led to the formation of fiber-enforced materials that exhibited pronounced cohesiveness and stability, combining the desirable properties of both biopolymers and hagfish slime.\n\nThe implications of this research are far-reaching, with potential applications in various fields such as biomedicine, tissue engineering, and soft robotics. The development of fiber-enforced hydrogels inspired by hagfish slime not only offers a sustainable and biocompatible alternative to synthetic materials but also opens up new possibilities for the design of advanced materials with tailored properties.\n\nAs scientists continue to unravel the secrets of nature's ingenious solutions, the study serves as a testament to the untapped potential of bioinspired materials and the endless possibilities they hold for innovation and technological advancement. The future looks promising for fiber-enforced hydrogels, as researchers draw inspiration from the natural world to create materials that are not only functional but also environmentally friendly and sustainable."
    }
]